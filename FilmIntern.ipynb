{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Filimize/Course_1/blob/main/FilmIntern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gDwAVv7t8es"
      },
      "source": [
        "#Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laHgUkiPt45-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc8f8f1-77a3-416e-de00-c1e2b6783471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiQRRobP2UC1"
      },
      "outputs": [],
      "source": [
        "#!kill 245"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "wvBQudtSy_Iz",
        "outputId": "630e1416-bdc5-41e3-b06e-13d10ed1a974"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Launching TensorBoard..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d46782f9619b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--logdir logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36m_start_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_start_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(args_string)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mparsed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mstart_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStartLaunched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/manager.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(arguments, timeout)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mend_time_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time_seconds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend_time_seconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_interval_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0msubprocess_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubprocess_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XxV-yUsI3cG"
      },
      "source": [
        "#F PART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18yIWyEEoGLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc3c37e-31cd-4b87-92e6-0e97c9776ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VojRNdYqyEFs",
        "outputId": "b0fcf947-19a7-4cac-92e8-0c7cb0e2d0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GMtUtygDg3BiUeBpKo9vHaFc0AUY5rXg\n",
            "To: /content/A_single_beam_weak_noise.h5\n",
            "100% 5.91G/5.91G [04:07<00:00, 23.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "#1 Dominant mode single beam (dominant -/+ 2) -/+ 0.8\n",
        "!gdown 1GMtUtygDg3BiUeBpKo9vHaFc0AUY5rXg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 beam with noise\n",
        "!gdown 1-2AA_5JXaP55EL19TBzbBIJz6BdsVK_B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0guo6a6QJK2",
        "outputId": "b1c286e6-24e5-4dda-9a76-bc14374d2dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-2AA_5JXaP55EL19TBzbBIJz6BdsVK_B\n",
            "To: /content/1_beam.h5\n",
            "100% 4.58G/4.58G [01:24<00:00, 54.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 beam\n",
        "!gdown 10fQA_RX0YFLKO3mGQAxxmFDrVeDCRYt_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hvcDZEWghdh",
        "outputId": "2a40c333-ac33-46ae-8131-13edbebf9867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10fQA_RX0YFLKO3mGQAxxmFDrVeDCRYt_\n",
            "To: /content/3_beam_mode_nor.h5\n",
            "100% 4.58G/4.58G [02:48<00:00, 27.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrd3mxCwEqS5"
      },
      "source": [
        "##MIX BEST MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do19q0oAEMMd"
      },
      "source": [
        "##H5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE9qi6XA78Pt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_data_h5(name, batch_size, n=None, remove=True):\n",
        "    with h5py.File(name, \"r\") as file:\n",
        "        labels = [file[f'labels_{i}'] for i in range(3)]\n",
        "        datasets = [file[f'dataset_{i}'] for i in range(3)]\n",
        "        tf_datasets = []\n",
        "        if n is not None:\n",
        "            labels = [labels[i][:ni] for i, ni in enumerate(n)]\n",
        "            datasets = [datasets[i][:ni] for i, ni in enumerate(n)]\n",
        "        for label, dataset in zip(labels, datasets):\n",
        "            label = tf.data.Dataset.from_tensor_slices(label)\n",
        "            dataset = tf.data.Dataset.from_tensor_slices(tf.expand_dims(dataset, -1))\n",
        "            tf_dataset = tf.data.Dataset.zip((dataset, label)).batch(batch_size)\n",
        "            tf_datasets.append(tf_dataset)\n",
        "    if remove:\n",
        "        os.remove(name)\n",
        "    return tf_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYdMsJAtEPGX"
      },
      "source": [
        "##MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtXULhTT1mQw",
        "outputId": "aae8360b-046e-407b-ff91-d5072f90c770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 96, 96, 128)       1280      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 96, 96, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 48, 48, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 48, 48, 64)        73792     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 48, 48, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 24, 24, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 24, 24, 32)        18464     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 24, 24, 16)        4624      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 12, 12, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2048)              4720640   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 12)                3084      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,540,188\n",
            "Trainable params: 5,540,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the CNN model\n",
        "def build_cnn_model(input_shape, num_modes):\n",
        "\n",
        "    # Convolutional layers\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='silu', padding='same', input_shape=input_shape))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='silu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='silu', padding='same'))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='silu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='silu', padding='same'))\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='silu', padding='same'))\n",
        "    model.add(layers.Conv2D(16, (3, 3), activation='silu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    # Fully connected layers\n",
        "    # Fully connected layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(2048, activation='silu'))\n",
        "    model.add(layers.Dense(256, activation='silu'))\n",
        "    model.add(layers.Dense(num_modes, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Specify input shape and number of output nodes (modes)\n",
        "input_shape = (96, 96, 1)  # Input image size: 96x96 with 1 channel (grayscale)\n",
        "num_modes = 12\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = build_cnn_model(input_shape, num_modes)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # You can choose other optimizers as well\n",
        "loss = 'mean_squared_error'  # Since it's a regression problem, we use Mean Squared Error (MSE)\n",
        "cnn_model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "\n",
        "# Display the model summary\n",
        "cnn_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "em1A0MSp2j8J",
        "outputId": "a1cfd448-7110-4030-efc6-f4d5ff673f1f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-87fdbe72536f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "cnn_model.fit(dataset[0], validation_data = dataset[1], epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMeNHT_U84ah"
      },
      "outputs": [],
      "source": [
        "def model_1(input_shape, num_modes):\n",
        "\n",
        "    # Convolutional layers\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='silu', padding='same', input_shape=input_shape))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='silu', padding='same',kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(layers.Conv2D(256, (3, 3), activation='silu', padding='same',kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
        "    model.add(layers.Conv2D(256, (3, 3), activation='silu', padding='same',kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='silu', padding='same',kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='silu', padding='same',kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='silu', padding='same',kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
        "    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "    # Fully connected layers\n",
        "    # Fully connected layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(2048, activation='silu',kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
        "    model.add(layers.Dense(2048, activation='silu',kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
        "    model.add(layers.Dense(num_modes, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the CNN model\n",
        "model_1 = model_1(input_shape, num_modes)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # You can choose other optimizers as well\n",
        "loss = 'mean_squared_error'  # Since it's a regression problem, we use Mean Squared Error (MSE)\n",
        "model_1.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "\n",
        "# Display the model summary\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BxAtpZ598qk"
      },
      "outputs": [],
      "source": [
        "model_1.fit(dataset[0], validation_data = dataset[1], epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hY0lqchp-RCv"
      },
      "outputs": [],
      "source": [
        "def model_2(input_shape, num_modes):\n",
        "\n",
        "    # Convolutional layers\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Block 1\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(num_modes, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the CNN model\n",
        "model_2 = model_2(input_shape, num_modes)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # You can choose other optimizers as well\n",
        "loss = 'mean_squared_error'  # Since it's a regression problem, we use Mean Squared Error (MSE)\n",
        "model_2.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "\n",
        "# Display the model summary\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJB9ZzCYAo2-"
      },
      "outputs": [],
      "source": [
        "model_2.fit(dataset[0], validation_data = dataset[1], epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p9yw0JkKNob"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the LSTM-based model\n",
        "def create_lstm_model(input_shape, num_zernike_coeffs):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # LSTM layers\n",
        "    model.add(layers.LSTM(64, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(layers.LSTM(64, return_sequences=True))\n",
        "\n",
        "    # Flatten LSTM output and add dense layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(num_zernike_coeffs))  # Output layer with num_zernike_coeffs neurons for the Zernike coefficients\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape (modify this according to your image size and sequence length)\n",
        "input_shape = (3, , image_width, num_channels)\n",
        "\n",
        "# Define the number of Zernike coefficients (12 modes)\n",
        "num_zernike_coeffs = 12\n",
        "\n",
        "# Create the LSTM-based model\n",
        "model = create_lstm_model(input_shape, num_zernike_coeffs)\n",
        "\n",
        "# Compile the model with an appropriate optimizer and loss function for regression task\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV6BJOfE0Cck"
      },
      "source": [
        "##Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQQHUegtK9Ay",
        "outputId": "f6af921e-9d24-463a-88de-bd82b0d9b189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 96, 96, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 96, 96, 64)   640         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 96, 96, 64)  256         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                   (None, 96, 96, 64)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 96, 96, 64)   36928       ['re_lu[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_1 (ReLU)                 (None, 96, 96, 64)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 96, 96, 64)   36928       ['re_lu_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 96, 96, 64)   4160        ['re_lu[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 96, 96, 64)   0           ['batch_normalization_2[0][0]',  \n",
            "                                                                  'batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_2 (ReLU)                 (None, 96, 96, 64)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 96, 96, 64)   36928       ['re_lu_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_3 (ReLU)                 (None, 96, 96, 64)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 96, 96, 64)   36928       ['re_lu_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 96, 96, 64)   4160        ['re_lu_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 96, 96, 64)   0           ['batch_normalization_5[0][0]',  \n",
            "                                                                  'batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_4 (ReLU)                 (None, 96, 96, 64)   0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 96, 96, 64)   36928       ['re_lu_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_5 (ReLU)                 (None, 96, 96, 64)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 96, 96, 64)   36928       ['re_lu_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 96, 96, 64)   4160        ['re_lu_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 96, 96, 64)  256         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 96, 96, 64)   0           ['batch_normalization_8[0][0]',  \n",
            "                                                                  'batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_6 (ReLU)                 (None, 96, 96, 64)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 96, 96, 64)   36928       ['re_lu_6[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 96, 96, 64)  256         ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_7 (ReLU)                 (None, 96, 96, 64)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 96, 96, 64)   36928       ['re_lu_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 96, 96, 64)   4160        ['re_lu_6[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 96, 96, 64)  256         ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 96, 96, 64)  256         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 96, 96, 64)   0           ['batch_normalization_11[0][0]', \n",
            "                                                                  'batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_8 (ReLU)                 (None, 96, 96, 64)   0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 96, 96, 64)   36928       ['re_lu_8[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 96, 96, 64)  256         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_9 (ReLU)                 (None, 96, 96, 64)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 96, 96, 64)   36928       ['re_lu_9[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 96, 96, 64)   4160        ['re_lu_8[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 96, 96, 64)  256         ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 96, 96, 64)  256         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 96, 96, 64)   0           ['batch_normalization_14[0][0]', \n",
            "                                                                  'batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_10 (ReLU)                (None, 96, 96, 64)   0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 64)          0           ['re_lu_10[0][0]']               \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          16640       ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          32896       ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 12)           1548        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 445,900\n",
            "Trainable params: 443,852\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_resnet_model(input_shape, num_modes, num_filters=64, num_blocks=5):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial Convolutional Block\n",
        "    x = layers.Conv2D(num_filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Residual Blocks\n",
        "    for _ in range(num_blocks):\n",
        "        x = residual_block(x, num_filters)\n",
        "\n",
        "    # Global Average Pooling\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = layers.Dense(256, activation='silu')(x)\n",
        "    x = layers.Dense(128, activation='silu')(x)\n",
        "\n",
        "    # Output Layer\n",
        "    output_layer = layers.Dense(num_modes, activation='linear')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=output_layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "def residual_block(input_tensor, num_filters):\n",
        "    x = layers.Conv2D(num_filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(input_tensor)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(num_filters, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Shortcut Connection\n",
        "    shortcut = layers.Conv2D(num_filters, kernel_size=(1, 1), strides=(1, 1), padding='same')(input_tensor)\n",
        "    shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Define input shape (modify this according to your image size and number of channels)\n",
        "input_shape = (96, 96, 1)\n",
        "\n",
        "# Define the number of Zernike coefficients (12 modes)\n",
        "num_modes = 12\n",
        "\n",
        "# Create the ResNet-based model\n",
        "model_3 = create_resnet_model(input_shape, num_modes)\n",
        "\n",
        "# Compile the model with an appropriate optimizer and loss function for regression task\n",
        "model_3.compile(optimizer='Nadam', loss='mean_squared_error',metrics=['mse'])\n",
        "\n",
        "# Print the model summary\n",
        "model_3.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "eUlp2sH6FxOE",
        "outputId": "168689fd-be1b-4122-d44f-182d75cf6096"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-98adca915747>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "model_3.fit(dataset[0], validation_data = dataset[1], epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbeuXQljtDai"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model_3.evaluate(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUv-r3EOyiq6"
      },
      "outputs": [],
      "source": [
        "test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsYz5xMUEAr0"
      },
      "source": [
        "##Resnet2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vja8UAivm-iG",
        "outputId": "bd866f60-bc89-471b-a783-955b81b3b924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 96, 96, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 96, 96, 64)           3200      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 48, 48, 64)           0         ['conv2d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 48, 48, 64)           36928     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 48, 48, 64)           256       ['conv2d_1[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 48, 48, 64)           0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 48, 48, 64)           36928     ['activation[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 48, 48, 64)           256       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 48, 48, 64)           0         ['batch_normalization_1[0][0]'\n",
            "                                                                    , 'max_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 48, 48, 64)           0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 48, 48, 64)           36928     ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 48, 48, 64)           256       ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 48, 48, 64)           0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 48, 48, 64)           36928     ['activation_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 48, 48, 64)           256       ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 48, 48, 64)           0         ['batch_normalization_3[0][0]'\n",
            "                                                                    , 'activation_1[0][0]']       \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 48, 48, 64)           0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 48, 48, 64)           36928     ['activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 48, 48, 64)           256       ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 48, 48, 64)           0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 48, 48, 64)           36928     ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 48, 48, 64)           256       ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 48, 48, 64)           0         ['batch_normalization_5[0][0]'\n",
            "                                                                    , 'activation_3[0][0]']       \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 48, 48, 64)           0         ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 48, 48, 64)           36928     ['activation_5[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 48, 48, 64)           256       ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 48, 48, 64)           0         ['batch_normalization_6[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 48, 48, 64)           36928     ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 48, 48, 64)           256       ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_3 (Add)                 (None, 48, 48, 64)           0         ['batch_normalization_7[0][0]'\n",
            "                                                                    , 'activation_5[0][0]']       \n",
            "                                                                                                  \n",
            " activation_7 (Activation)   (None, 48, 48, 64)           0         ['add_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 48, 48, 64)           36928     ['activation_7[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 48, 48, 64)           256       ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)   (None, 48, 48, 64)           0         ['batch_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 48, 48, 64)           36928     ['activation_8[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 48, 48, 64)           256       ['conv2d_10[0][0]']           \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_4 (Add)                 (None, 48, 48, 64)           0         ['batch_normalization_9[0][0]'\n",
            "                                                                    , 'activation_7[0][0]']       \n",
            "                                                                                                  \n",
            " activation_9 (Activation)   (None, 48, 48, 64)           0         ['add_4[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 24, 24, 64)           0         ['activation_9[0][0]']        \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 24, 24, 64)           36928     ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_10 (Ba  (None, 24, 24, 64)           256       ['conv2d_11[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_10 (Activation)  (None, 24, 24, 64)           0         ['batch_normalization_10[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_10[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_11 (Ba  (None, 24, 24, 64)           256       ['conv2d_12[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_5 (Add)                 (None, 24, 24, 64)           0         ['batch_normalization_11[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " activation_11 (Activation)  (None, 24, 24, 64)           0         ['add_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_12 (Ba  (None, 24, 24, 64)           256       ['conv2d_13[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_12 (Activation)  (None, 24, 24, 64)           0         ['batch_normalization_12[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_13 (Ba  (None, 24, 24, 64)           256       ['conv2d_14[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_6 (Add)                 (None, 24, 24, 64)           0         ['batch_normalization_13[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_11[0][0]']       \n",
            "                                                                                                  \n",
            " activation_13 (Activation)  (None, 24, 24, 64)           0         ['add_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (Ba  (None, 24, 24, 64)           256       ['conv2d_15[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_14 (Activation)  (None, 24, 24, 64)           0         ['batch_normalization_14[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_14[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_15 (Ba  (None, 24, 24, 64)           256       ['conv2d_16[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_7 (Add)                 (None, 24, 24, 64)           0         ['batch_normalization_15[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_13[0][0]']       \n",
            "                                                                                                  \n",
            " activation_15 (Activation)  (None, 24, 24, 64)           0         ['add_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_16 (Ba  (None, 24, 24, 64)           256       ['conv2d_17[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_16 (Activation)  (None, 24, 24, 64)           0         ['batch_normalization_16[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_16[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_17 (Ba  (None, 24, 24, 64)           256       ['conv2d_18[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_8 (Add)                 (None, 24, 24, 64)           0         ['batch_normalization_17[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_15[0][0]']       \n",
            "                                                                                                  \n",
            " activation_17 (Activation)  (None, 24, 24, 64)           0         ['add_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_17[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_18 (Ba  (None, 24, 24, 64)           256       ['conv2d_19[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_18 (Activation)  (None, 24, 24, 64)           0         ['batch_normalization_18[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 24, 24, 64)           36928     ['activation_18[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_19 (Ba  (None, 24, 24, 64)           256       ['conv2d_20[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_9 (Add)                 (None, 24, 24, 64)           0         ['batch_normalization_19[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_17[0][0]']       \n",
            "                                                                                                  \n",
            " activation_19 (Activation)  (None, 24, 24, 64)           0         ['add_9[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 12, 12, 64)           0         ['activation_19[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 12, 12, 64)           36928     ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_20 (Ba  (None, 12, 12, 64)           256       ['conv2d_21[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 12, 12, 64)           0         ['batch_normalization_20[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_20[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_21 (Ba  (None, 12, 12, 64)           256       ['conv2d_22[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_10 (Add)                (None, 12, 12, 64)           0         ['batch_normalization_21[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 12, 12, 64)           0         ['add_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_21[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_22 (Ba  (None, 12, 12, 64)           256       ['conv2d_23[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_22 (Activation)  (None, 12, 12, 64)           0         ['batch_normalization_22[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_22[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_23 (Ba  (None, 12, 12, 64)           256       ['conv2d_24[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_11 (Add)                (None, 12, 12, 64)           0         ['batch_normalization_23[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_21[0][0]']       \n",
            "                                                                                                  \n",
            " activation_23 (Activation)  (None, 12, 12, 64)           0         ['add_11[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_23[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_24 (Ba  (None, 12, 12, 64)           256       ['conv2d_25[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_24 (Activation)  (None, 12, 12, 64)           0         ['batch_normalization_24[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_24[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_25 (Ba  (None, 12, 12, 64)           256       ['conv2d_26[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_12 (Add)                (None, 12, 12, 64)           0         ['batch_normalization_25[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_23[0][0]']       \n",
            "                                                                                                  \n",
            " activation_25 (Activation)  (None, 12, 12, 64)           0         ['add_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_25[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_26 (Ba  (None, 12, 12, 64)           256       ['conv2d_27[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_26 (Activation)  (None, 12, 12, 64)           0         ['batch_normalization_26[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_26[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_27 (Ba  (None, 12, 12, 64)           256       ['conv2d_28[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_13 (Add)                (None, 12, 12, 64)           0         ['batch_normalization_27[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_25[0][0]']       \n",
            "                                                                                                  \n",
            " activation_27 (Activation)  (None, 12, 12, 64)           0         ['add_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_27[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_28 (Ba  (None, 12, 12, 64)           256       ['conv2d_29[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_28 (Activation)  (None, 12, 12, 64)           0         ['batch_normalization_28[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)          (None, 12, 12, 64)           36928     ['activation_28[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_29 (Ba  (None, 12, 12, 64)           256       ['conv2d_30[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_14 (Add)                (None, 12, 12, 64)           0         ['batch_normalization_29[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_27[0][0]']       \n",
            "                                                                                                  \n",
            " activation_29 (Activation)  (None, 12, 12, 64)           0         ['add_14[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 64)             0         ['activation_29[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)          (None, 6, 6, 64)             36928     ['max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_30 (Ba  (None, 6, 6, 64)             256       ['conv2d_31[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_30 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_30[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_30[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_31 (Ba  (None, 6, 6, 64)             256       ['conv2d_32[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_15 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_31[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'max_pooling2d_3[0][0]']     \n",
            "                                                                                                  \n",
            " activation_31 (Activation)  (None, 6, 6, 64)             0         ['add_15[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_31[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_32 (Ba  (None, 6, 6, 64)             256       ['conv2d_33[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_32 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_32[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_32[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_33 (Ba  (None, 6, 6, 64)             256       ['conv2d_34[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_16 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_33[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_31[0][0]']       \n",
            "                                                                                                  \n",
            " activation_33 (Activation)  (None, 6, 6, 64)             0         ['add_16[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_33[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_34 (Ba  (None, 6, 6, 64)             256       ['conv2d_35[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_34 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_34[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_34[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_35 (Ba  (None, 6, 6, 64)             256       ['conv2d_36[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_17 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_35[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_33[0][0]']       \n",
            "                                                                                                  \n",
            " activation_35 (Activation)  (None, 6, 6, 64)             0         ['add_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_35[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_36 (Ba  (None, 6, 6, 64)             256       ['conv2d_37[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_36 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_36[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_36[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_37 (Ba  (None, 6, 6, 64)             256       ['conv2d_38[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_18 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_37[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_35[0][0]']       \n",
            "                                                                                                  \n",
            " activation_37 (Activation)  (None, 6, 6, 64)             0         ['add_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_37[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_38 (Ba  (None, 6, 6, 64)             256       ['conv2d_39[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_38 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_38[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_38[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_39 (Ba  (None, 6, 6, 64)             256       ['conv2d_40[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_19 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_39[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_37[0][0]']       \n",
            "                                                                                                  \n",
            " activation_39 (Activation)  (None, 6, 6, 64)             0         ['add_19[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_39[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_40 (Ba  (None, 6, 6, 64)             256       ['conv2d_41[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_40 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_40[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_40[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_41 (Ba  (None, 6, 6, 64)             256       ['conv2d_42[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_20 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_41[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_39[0][0]']       \n",
            "                                                                                                  \n",
            " activation_41 (Activation)  (None, 6, 6, 64)             0         ['add_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_41[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_42 (Ba  (None, 6, 6, 64)             256       ['conv2d_43[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_42 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_42[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_42[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_43 (Ba  (None, 6, 6, 64)             256       ['conv2d_44[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_21 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_43[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_41[0][0]']       \n",
            "                                                                                                  \n",
            " activation_43 (Activation)  (None, 6, 6, 64)             0         ['add_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_43[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_44 (Ba  (None, 6, 6, 64)             256       ['conv2d_45[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_44 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_44[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)          (None, 6, 6, 64)             36928     ['activation_44[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_45 (Ba  (None, 6, 6, 64)             256       ['conv2d_46[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_22 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_45[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_43[0][0]']       \n",
            "                                                                                                  \n",
            " activation_45 (Activation)  (None, 6, 6, 64)             0         ['add_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)          (None, 6, 6, 64)             4160      ['activation_45[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_46 (Ba  (None, 6, 6, 64)             256       ['conv2d_47[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_46 (Activation)  (None, 6, 6, 64)             0         ['batch_normalization_46[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)          (None, 6, 6, 64)             4160      ['activation_46[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_47 (Ba  (None, 6, 6, 64)             256       ['conv2d_48[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_23 (Add)                (None, 6, 6, 64)             0         ['batch_normalization_47[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_45[0][0]']       \n",
            "                                                                                                  \n",
            " activation_47 (Activation)  (None, 6, 6, 64)             0         ['add_23[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 2304)                 0         ['activation_47[0][0]']       \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 4096)                 9441280   ['flatten[0][0]']             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 4096)                 1678131   ['dense[0][0]']               \n",
            "                                                          2                                       \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 2048)                 8390656   ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 1024)                 2098176   ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 512)                  524800    ['dense_3[0][0]']             \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 256)                  131328    ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 128)                  32896     ['dense_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 128)                  16512     ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 12)                   1548      ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 39141004 (149.31 MB)\n",
            "Trainable params: 39134860 (149.29 MB)\n",
            "Non-trainable params: 6144 (24.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Add, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.initializers import HeNormal, GlorotNormal\n",
        "def loss(mode=0):\n",
        "    @tf.function(input_signature=(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.float32), tf.TensorSpec(shape=[None, None], dtype=tf.float32)))\n",
        "    def loss(y_labels, y_predicted):\n",
        "        symmetric_labels = tf.concat([y_labels[:, 2:5], y_labels[:, 9:14]], 1)\n",
        "        symmetric_pred = tf.concat([y_predicted[:, 2:5], y_predicted[:, 9:14]], 1)\n",
        "        diffs_sym_1 = tf.reduce_sum(tf.abs(symmetric_labels - symmetric_pred), 1)\n",
        "        diffs_sym_2 = tf.reduce_sum(tf.abs(symmetric_labels + symmetric_pred), 1)\n",
        "        diffs_sym = tf.minimum(diffs_sym_1, diffs_sym_2)\n",
        "        diffs_asym = tf.reduce_sum(\n",
        "            tf.abs(tf.concat([y_labels[:, :2], y_labels[:, 5:9], y_labels[:, 14:20]], 1) - tf.concat(\n",
        "                [y_predicted[:, :2], y_predicted[:, 5:9], y_predicted[:, 14:20]], 1)), 1)\n",
        "\n",
        "        return tf.reduce_mean(diffs_sym + diffs_asym)\n",
        "\n",
        "    @tf.function(input_signature=(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.float32), tf.TensorSpec(shape=[None, None], dtype=tf.float32)))\n",
        "    def loss_mae(y_labels, y_predicted):\n",
        "        return tf.reduce_mean(tf.reduce_sum(tf.abs(y_labels - y_predicted), 1))\n",
        "\n",
        "    return [loss_mae, loss][mode]\n",
        "\n",
        "def resblock(x, filters, kernel_size, repetitions, pool=False):\n",
        "    initializer = HeNormal()\n",
        "    for _ in range(repetitions):\n",
        "        shortcut = x\n",
        "        x = Conv2D(filters, kernel_size,kernel_initializer=initializer, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(filters, kernel_size,kernel_initializer=initializer, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Add()([x, shortcut])\n",
        "        x = Activation('relu')(x)\n",
        "    if pool:\n",
        "        x = MaxPooling2D(2, 2)(x)\n",
        "    return x\n",
        "\n",
        "input_size = (96, 96, 1)  # Adjust the input size according to your data\n",
        "num_modes = 12\n",
        "def build_model(input_size,num_modes):\n",
        "    inp = Input(input_size)\n",
        "    initializer = HeNormal()\n",
        "    initializer_2 = GlorotNormal()\n",
        "    x = Conv2D(64, (7, 7), activation='relu',kernel_initializer=initializer, padding='same')(inp)\n",
        "    x = MaxPooling2D(2, 2)(x)\n",
        "    x = resblock(x, 64, (3, 3), 5, pool=True)\n",
        "    x = resblock(x, 64, (3, 3), 5, pool=True)\n",
        "    x = resblock(x, 64, (3, 3), 5, pool=True)\n",
        "    x = resblock(x, 64, (3, 3), 4)\n",
        "    x = resblock(x, 64, (3, 3), 4)\n",
        "    x = resblock(x, 64, (1, 1), 1)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(4096,kernel_initializer=initializer_2, activation='silu')(x)\n",
        "    x = Dense(4096,kernel_initializer=initializer_2, activation='silu')(x)\n",
        "    x = Dense(2048,kernel_initializer=initializer_2, activation='silu')(x)\n",
        "    x = Dense(1024,kernel_initializer=initializer_2, activation='silu')(x)\n",
        "    x = Dense(512,kernel_initializer=initializer_2, activation='silu')(x)\n",
        "    x = Dense(256,kernel_initializer=initializer_2, activation='silu')(x)\n",
        "    x = Dense(128,kernel_initializer=initializer_2, activation='silu')(x)\n",
        "    x = Dense(128,kernel_initializer=initializer_2, activation='silu')(x)\n",
        "    x = Dense(num_modes,kernel_initializer=initializer_2, activation='linear')(x)  # Output layer with 10 nodes for regression\n",
        "\n",
        "    model = Model(inp, x)\n",
        "    return model\n",
        "\n",
        "model_4 = build_model(input_size,num_modes)\n",
        "model_4.compile(optimizer='Nadam', loss=loss(mode=0),metrics=['mse'])\n",
        "model_4.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwVediwtG5aN"
      },
      "source": [
        "##Resnet(Adam/Relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvoRk2XSGCBB",
        "outputId": "d8fc9b10-1a37-431d-a4d9-d068dce5d7f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 96, 96, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 96, 96, 64)   3200        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 48, 48, 64)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 48, 48, 64)   36928       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 48, 48, 64)  256         ['conv2d_1[0][0]']               \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 48, 48, 64)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 48, 48, 64)   36928       ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 48, 48, 64)   0           ['batch_normalization_1[0][0]',  \n",
            "                                                                  'max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 48, 48, 64)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 48, 48, 64)   36928       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 48, 48, 64)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 48, 48, 64)   36928       ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 48, 48, 64)   0           ['batch_normalization_3[0][0]',  \n",
            "                                                                  'activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 48, 48, 64)   0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 48, 48, 64)   36928       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 48, 48, 64)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 48, 48, 64)   36928       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 48, 48, 64)   0           ['batch_normalization_5[0][0]',  \n",
            "                                                                  'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 48, 48, 64)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 48, 48, 64)   36928       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 48, 48, 64)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 48, 48, 64)   36928       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 48, 48, 64)   0           ['batch_normalization_7[0][0]',  \n",
            "                                                                  'activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 48, 48, 64)   0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 48, 48, 64)   36928       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 48, 48, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 48, 48, 64)   36928       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 48, 48, 64)  256         ['conv2d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 48, 48, 64)   0           ['batch_normalization_9[0][0]',  \n",
            "                                                                  'activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 48, 48, 64)   0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 64)  0           ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 24, 24, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 24, 24, 64)  256         ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 24, 24, 64)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 24, 24, 64)  256         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 24, 24, 64)   0           ['batch_normalization_11[0][0]', \n",
            "                                                                  'max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 24, 24, 64)   0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 24, 24, 64)  256         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 24, 24, 64)   0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 24, 24, 64)  256         ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 24, 24, 64)   0           ['batch_normalization_13[0][0]', \n",
            "                                                                  'activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 24, 24, 64)   0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 24, 24, 64)  256         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 24, 24, 64)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 24, 24, 64)  256         ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 24, 24, 64)   0           ['batch_normalization_15[0][0]', \n",
            "                                                                  'activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 24, 24, 64)   0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 24, 24, 64)  256         ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 24, 24, 64)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 24, 24, 64)  256         ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 24, 24, 64)   0           ['batch_normalization_17[0][0]', \n",
            "                                                                  'activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 24, 24, 64)   0           ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 24, 24, 64)  256         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 24, 24, 64)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 24, 24, 64)   36928       ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 24, 24, 64)  256         ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 24, 24, 64)   0           ['batch_normalization_19[0][0]', \n",
            "                                                                  'activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 24, 24, 64)   0           ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 64)  0           ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 12, 12, 64)   36928       ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 12, 12, 64)  256         ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 12, 12, 64)  256         ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 12, 12, 64)   0           ['batch_normalization_21[0][0]', \n",
            "                                                                  'max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 12, 12, 64)   0           ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 12, 12, 64)  256         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 12, 12, 64)  256         ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 12, 12, 64)   0           ['batch_normalization_23[0][0]', \n",
            "                                                                  'activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 12, 12, 64)   0           ['add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 12, 12, 64)  256         ['conv2d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 12, 12, 64)  256         ['conv2d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 12, 12, 64)   0           ['batch_normalization_25[0][0]', \n",
            "                                                                  'activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 12, 12, 64)   0           ['add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 12, 12, 64)  256         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 12, 12, 64)  256         ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 12, 12, 64)   0           ['batch_normalization_27[0][0]', \n",
            "                                                                  'activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 12, 12, 64)   0           ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 12, 12, 64)  256         ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 12, 12, 64)   0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 12, 12, 64)   36928       ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 12, 12, 64)  256         ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 12, 12, 64)   0           ['batch_normalization_29[0][0]', \n",
            "                                                                  'activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 12, 12, 64)   0           ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)    0           ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 6, 6, 64)     36928       ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 6, 6, 64)    256         ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 6, 6, 64)    256         ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_31[0][0]', \n",
            "                                                                  'max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 6, 6, 64)     0           ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 6, 6, 64)    256         ['conv2d_33[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 6, 6, 64)    256         ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_33[0][0]', \n",
            "                                                                  'activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 6, 6, 64)     0           ['add_16[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 6, 6, 64)    256         ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 6, 6, 64)    256         ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_35[0][0]', \n",
            "                                                                  'activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 6, 6, 64)     0           ['add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 6, 6, 64)    256         ['conv2d_37[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 6, 6, 64)    256         ['conv2d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_37[0][0]', \n",
            "                                                                  'activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 6, 6, 64)     0           ['add_18[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 6, 6, 64)    256         ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_38[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 6, 6, 64)    256         ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_39[0][0]', \n",
            "                                                                  'activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 6, 6, 64)     0           ['add_19[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 6, 6, 64)    256         ['conv2d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 6, 6, 64)    256         ['conv2d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_41[0][0]', \n",
            "                                                                  'activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 6, 6, 64)     0           ['add_20[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 6, 6, 64)    256         ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 6, 6, 64)    256         ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_43[0][0]', \n",
            "                                                                  'activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 6, 6, 64)     0           ['add_21[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 6, 6, 64)    256         ['conv2d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 6, 6, 64)     36928       ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 6, 6, 64)    256         ['conv2d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_45[0][0]', \n",
            "                                                                  'activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 6, 6, 64)     0           ['add_22[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 6, 6, 64)     4160        ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 6, 6, 64)    256         ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 6, 6, 64)     0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 6, 6, 64)     4160        ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 6, 6, 64)    256         ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 6, 6, 64)     0           ['batch_normalization_47[0][0]', \n",
            "                                                                  'activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 6, 6, 64)     0           ['add_23[0][0]']                 \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 2304)         0           ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 4096)         9441280     ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 4096)         16781312    ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 2048)         8390656     ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1024)         2098176     ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 512)          524800      ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 256)          131328      ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          32896       ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 128)          16512       ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 12)           1548        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 39,141,004\n",
            "Trainable params: 39,134,860\n",
            "Non-trainable params: 6,144\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Add, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def resblock(x, filters, kernel_size, repetitions, pool=False):\n",
        "    for _ in range(repetitions):\n",
        "        shortcut = x\n",
        "        x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(filters, kernel_size, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Add()([x, shortcut])\n",
        "        x = Activation('relu')(x)\n",
        "    if pool:\n",
        "        x = MaxPooling2D(2, 2)(x)\n",
        "    return x\n",
        "\n",
        "input_size = (96, 96, 1)  # Adjust the input size according to your data\n",
        "num_modes = 12\n",
        "def build_model(input_size,num_modes):\n",
        "    inp = Input(input_size)\n",
        "    x = Conv2D(64, (7, 7), activation='relu', padding='same')(inp)\n",
        "    x = MaxPooling2D(2, 2)(x)\n",
        "    x = resblock(x, 64, (3, 3), 5, pool=True)\n",
        "    x = resblock(x, 64, (3, 3), 5, pool=True)\n",
        "    x = resblock(x, 64, (3, 3), 5, pool=True)\n",
        "    x = resblock(x, 64, (3, 3), 4)\n",
        "    x = resblock(x, 64, (3, 3), 4)\n",
        "    x = resblock(x, 64, (1, 1), 1)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(4096, activation='relu')(x)\n",
        "    x = Dense(4096, activation='relu')(x)\n",
        "    x = Dense(2048, activation='relu')(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(num_modes, activation='linear')(x)  # Output layer with 10 nodes for regression\n",
        "\n",
        "    model = Model(inp, x)\n",
        "    return model\n",
        "\n",
        "model_5 = build_model(input_size,num_modes)\n",
        "model_5.compile(optimizer='adam', loss='mean_squared_error',metrics=['mse'])\n",
        "model_5.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP6BuEZU1I1X"
      },
      "source": [
        "##SHUFFLENET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BgemKgD1HGu",
        "outputId": "c7e324c2-b41c-49e5-c405-bf4400dc0588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 96, 96, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 96, 96, 24)   240         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 48, 48, 24)  0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 48, 48, 60)   1500        ['max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 48, 48, 60)  240         ['conv2d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.reshape (TFOpLambda)        (None, 48, 48, 2, 3  0           ['batch_normalization_48[0][0]'] \n",
            "                                0)                                                                \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose (TFOpLa  (None, 48, 48, 30,   0          ['tf.reshape[0][0]']             \n",
            " mbda)                          2)                                                                \n",
            "                                                                                                  \n",
            " tf.reshape_1 (TFOpLambda)      (None, 48, 48, 60)   0           ['tf.compat.v1.transpose[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 48, 48, 216)  13176       ['tf.reshape_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 48, 48, 216)  864        ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 48, 48, 240)  0           ['max_pooling2d_4[0][0]',        \n",
            "                                                                  'batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 24, 24, 240)  0          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 24, 24, 120)  28920       ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 24, 24, 120)  480        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.reshape_2 (TFOpLambda)      (None, 24, 24, 2, 6  0           ['batch_normalization_50[0][0]'] \n",
            "                                0)                                                                \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_1 (TFOp  (None, 24, 24, 60,   0          ['tf.reshape_2[0][0]']           \n",
            " Lambda)                        2)                                                                \n",
            "                                                                                                  \n",
            " tf.reshape_3 (TFOpLambda)      (None, 24, 24, 120)  0           ['tf.compat.v1.transpose_1[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 24, 24, 240)  29040       ['tf.reshape_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 24, 24, 240)  960        ['conv2d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 24, 24, 480)  0           ['max_pooling2d_5[0][0]',        \n",
            "                                                                  'batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPooling2D)  (None, 12, 12, 480)  0          ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 12, 12, 120)  57720       ['max_pooling2d_6[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 12, 12, 120)  480        ['conv2d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.reshape_4 (TFOpLambda)      (None, 12, 12, 2, 6  0           ['batch_normalization_52[0][0]'] \n",
            "                                0)                                                                \n",
            "                                                                                                  \n",
            " tf.compat.v1.transpose_2 (TFOp  (None, 12, 12, 60,   0          ['tf.reshape_4[0][0]']           \n",
            " Lambda)                        2)                                                                \n",
            "                                                                                                  \n",
            " tf.reshape_5 (TFOpLambda)      (None, 12, 12, 120)  0           ['tf.compat.v1.transpose_2[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 12, 12, 1)    121         ['tf.reshape_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 12, 12, 1)   4           ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 12, 12, 481)  0           ['max_pooling2d_6[0][0]',        \n",
            "                                                                  'batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 481)   0           ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 481)         0           ['max_pooling2d_7[0][0]']        \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 4096)         1974272     ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 4096)         16781312    ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 2048)         8390656     ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 1024)         2098176     ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 512)          524800      ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 256)          131328      ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 128)          32896       ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 128)          16512       ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 12)           1548        ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 30,085,245\n",
            "Trainable params: 30,083,731\n",
            "Non-trainable params: 1,514\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dense, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import numpy as np\n",
        "\n",
        "def channel_shuffle(x, groups):\n",
        "    _, w, h, c = x.shape\n",
        "    group_ch = c // groups\n",
        "\n",
        "    x = tf.reshape(x, (-1, w, h, groups, group_ch))\n",
        "    x = tf.transpose(x, (0, 1, 2, 4, 3))  # Reshaping and shuffling\n",
        "    x = tf.reshape(x, (-1, w, h, c))\n",
        "\n",
        "    return x\n",
        "\n",
        "def shuffle_unit(inputs, out_channels, groups=2):\n",
        "    initializer = HeNormal()\n",
        "    bottleneck_channels = out_channels // 4\n",
        "    x = Conv2D(bottleneck_channels, (1, 1),kernel_initializer=initializer, activation='relu')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = channel_shuffle(x, groups)\n",
        "\n",
        "    # Calculate the number of filters based on the value of bottleneck_channels\n",
        "    num_filters = out_channels - inputs.shape[-1]\n",
        "\n",
        "    # Ensure that the number of filters is at least 1\n",
        "    num_filters = max(num_filters, 1)\n",
        "\n",
        "    x = Conv2D(num_filters, (1, 1),kernel_initializer=initializer, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    return Concatenate()([inputs, x])\n",
        "\n",
        "\n",
        "\n",
        "def build_shufflenet(input_size, num_modes):\n",
        "    initializer = HeNormal()\n",
        "    initializer_2 = GlorotNormal()\n",
        "    inp = Input(input_size)\n",
        "\n",
        "    x = Conv2D(24, (3, 3), activation='relu',kernel_initializer=initializer, padding='same')(inp)\n",
        "    x = MaxPooling2D(2, strides=2)(x)\n",
        "\n",
        "    x = shuffle_unit(x, out_channels=240)\n",
        "    x = MaxPooling2D(2, strides=2)(x)\n",
        "\n",
        "    x = shuffle_unit(x, out_channels=480)\n",
        "    x = MaxPooling2D(2, strides=2)(x)\n",
        "\n",
        "    x = shuffle_unit(x, out_channels=480)\n",
        "    x = MaxPooling2D(2, strides=2)(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(4096,kernel_initializer=initializer_2, kernel_regularizer=l2(0.01), activation='silu')(x)\n",
        "    x = Dense(4096,kernel_initializer=initializer_2, kernel_regularizer=l2(0.01),activation='silu')(x)\n",
        "    x = Dense(2048,kernel_initializer=initializer_2, kernel_regularizer=l2(0.01),activation='silu')(x)\n",
        "    x = Dense(1024,kernel_initializer=initializer_2, kernel_regularizer=l2(0.01),activation='silu')(x)\n",
        "    x = Dense(512,kernel_initializer=initializer_2, kernel_regularizer=l2(0.01),activation='silu')(x)\n",
        "    x = Dense(256,kernel_initializer=initializer_2, kernel_regularizer=l2(0.01),activation='silu')(x)\n",
        "    x = Dense(128,kernel_initializer=initializer_2, kernel_regularizer=l2(0.01),activation='silu')(x)\n",
        "    x = Dense(128,kernel_initializer=initializer_2, kernel_regularizer=l2(0.01),activation='silu')(x)\n",
        "    x = Dense(num_modes,kernel_initializer=initializer_2, activation='linear')(x)  # Output layer with num_modes nodes for regression\n",
        "\n",
        "    model = Model(inp, x)\n",
        "    return model\n",
        "\n",
        "input_size = (96, 96, 1)  # Adjust the input size according to your data\n",
        "num_modes = 12\n",
        "\n",
        "model_shufflenet = build_shufflenet(input_size, num_modes)\n",
        "model_shufflenet.compile(optimizer='Nadam', loss=loss(mode=0), metrics=['mse'])\n",
        "model_shufflenet.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-avIebJCZqi"
      },
      "source": [
        "##Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rpkaCuNCfac",
        "outputId": "563d599f-c298-46f6-9947-9ccb65894ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 96, 96, 1)]       0         \n",
            "                                                                 \n",
            " separable_conv2d (Separable  (None, 96, 96, 32)       73        \n",
            " Conv2D)                                                         \n",
            "                                                                 \n",
            " batch_normalization_54 (Bat  (None, 96, 96, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " separable_conv2d_1 (Separab  (None, 96, 96, 64)       2400      \n",
            " leConv2D)                                                       \n",
            "                                                                 \n",
            " batch_normalization_55 (Bat  (None, 96, 96, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 48, 48, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " separable_conv2d_2 (Separab  (None, 48, 48, 128)      8896      \n",
            " leConv2D)                                                       \n",
            "                                                                 \n",
            " batch_normalization_56 (Bat  (None, 48, 48, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " separable_conv2d_3 (Separab  (None, 48, 48, 256)      34176     \n",
            " leConv2D)                                                       \n",
            "                                                                 \n",
            " batch_normalization_57 (Bat  (None, 48, 48, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 24, 24, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " separable_conv2d_4 (Separab  (None, 24, 24, 256)      68096     \n",
            " leConv2D)                                                       \n",
            "                                                                 \n",
            " batch_normalization_58 (Bat  (None, 24, 24, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " separable_conv2d_5 (Separab  (None, 24, 24, 512)      133888    \n",
            " leConv2D)                                                       \n",
            "                                                                 \n",
            " batch_normalization_59 (Bat  (None, 24, 24, 512)      2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 12, 12, 512)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 512)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 4096)              2101248   \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 2048)              8390656   \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 1024)              2098176   \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 12)                1548      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,330,997\n",
            "Trainable params: 30,328,501\n",
            "Non-trainable params: 2,496\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, SeparableConv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dense, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_xception(input_size, num_modes):\n",
        "    initializer = HeNormal()\n",
        "    initializer_2 = GlorotNormal()\n",
        "    inp = Input(input_size)\n",
        "\n",
        "    x = SeparableConv2D(32, (3, 3), activation='relu',kernel_initializer=initializer, padding='same')(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = SeparableConv2D(64, (3, 3), activation='relu',kernel_initializer=initializer, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    x = SeparableConv2D(128, (3, 3), activation='relu',kernel_initializer=initializer, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = SeparableConv2D(256, (3, 3), activation='relu',kernel_initializer=initializer, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    x = SeparableConv2D(256, (3, 3), activation='relu',kernel_initializer=initializer, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = SeparableConv2D(512, (3, 3), activation='relu',kernel_initializer=initializer, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    # Continue with more SeparableConv2D layers and skip connections\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(4096, activation='silu',kernel_regularizer=l2(0.01),kernel_initializer=initializer_2)(x)\n",
        "    x = Dense(4096, activation='silu',kernel_regularizer=l2(0.01),kernel_initializer=initializer_2)(x)\n",
        "    x = Dense(2048, activation='silu',kernel_regularizer=l2(0.01),kernel_initializer=initializer_2)(x)\n",
        "    x = Dense(1024, activation='silu',kernel_initializer=initializer_2)(x)\n",
        "    x = Dense(512, activation='silu',kernel_initializer=initializer_2)(x)\n",
        "    x = Dense(256, activation='silu',kernel_initializer=initializer_2)(x)\n",
        "    x = Dense(128, activation='silu',kernel_initializer=initializer_2)(x)\n",
        "    x = Dense(128, activation='silu',kernel_initializer=initializer_2)(x)\n",
        "    x = Dense(num_modes, activation='linear',kernel_initializer=initializer_2)(x)\n",
        "\n",
        "    model = Model(inp, x)\n",
        "    return model\n",
        "\n",
        "input_size = (96, 96, 1)  # Adjust the input size according to your data\n",
        "num_modes = 12\n",
        "\n",
        "model_xception = build_xception(input_size, num_modes)\n",
        "model_xception.compile(optimizer='Nadam', loss=loss(mode=0), metrics=['mse'])\n",
        "model_xception.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSLuuMS-4Srh"
      },
      "source": [
        "##MODEL GROUND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xu7dbijGZBd",
        "outputId": "c66d3f58-89b7-4768-c4b6-1493d5d4ce68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RuhkDpMgMFyMWlIaWNntt1pXqrrMn-7g\n",
            "To: /content/mix_dom_no_dom.h5\n",
            "100% 5.91G/5.91G [00:39<00:00, 150MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1RuhkDpMgMFyMWlIaWNntt1pXqrrMn-7g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_erFRdyb4p1F"
      },
      "outputs": [],
      "source": [
        "dataset= get_data_h5(\"/content/A_single_beam_weak_noise.h5\",batch_size= 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWxz3b6_vz85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e65e332-9bb1-4e1a-b000-ed669f43b901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 377s 201ms/step - loss: 0.4293 - mse: 0.0024 - val_loss: 6.1861 - val_mse: 0.4469 - lr: 1.0000e-04\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 376s 201ms/step - loss: 0.4083 - mse: 0.0022 - val_loss: 4.9855 - val_mse: 0.2819 - lr: 1.0000e-04\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 375s 200ms/step - loss: 0.3901 - mse: 0.0020 - val_loss: 4.8630 - val_mse: 0.3178 - lr: 1.0000e-04\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 375s 200ms/step - loss: 0.3736 - mse: 0.0019 - val_loss: 3.9762 - val_mse: 0.1872 - lr: 1.0000e-04\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 376s 200ms/step - loss: 0.3587 - mse: 0.0017 - val_loss: 4.2870 - val_mse: 0.2176 - lr: 1.0000e-04\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 375s 200ms/step - loss: 0.3454 - mse: 0.0016 - val_loss: 6.0103 - val_mse: 0.4214 - lr: 1.0000e-04\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 375s 200ms/step - loss: 0.3329 - mse: 0.0015 - val_loss: 5.9141 - val_mse: 0.3892 - lr: 1.0000e-04\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 375s 200ms/step - loss: 0.3219 - mse: 0.0014 - val_loss: 4.7983 - val_mse: 0.2767 - lr: 1.0000e-04\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 375s 200ms/step - loss: 0.3114 - mse: 0.0013 - val_loss: 3.2676 - val_mse: 0.1260 - lr: 1.0000e-04\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 376s 200ms/step - loss: 0.3022 - mse: 0.0012 - val_loss: 5.5924 - val_mse: 0.3659 - lr: 1.0000e-04\n"
          ]
        }
      ],
      "source": [
        "stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001)\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "history_shufflenet = model_4.fit(dataset[0], validation_data=dataset[1], epochs=10, shuffle=False, callbacks=[stop,reduce_lr,tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "model_4.save(\"1_noise_model.h5\")\n",
        "shutil.copy(\"1_noise_model.h5\", \"/content/drive/MyDrive/OpticsML/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KXNGno08qbMa",
        "outputId": "005a02f9-082d-41a5-86e4-7fed9f0a5af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/OpticsML/1_noise_model.h5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zdWv3U4KM7J",
        "outputId": "b9c2f516-916d-47a3-c6d5-1bd920304c4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.304370880126953,\n",
              " 7.359570503234863,\n",
              " 8.872404098510742,\n",
              " 9.326915740966797,\n",
              " 8.493438720703125,\n",
              " 10.002659797668457,\n",
              " 6.194578647613525,\n",
              " 5.122730255126953,\n",
              " 3.113776445388794,\n",
              " 4.046057224273682]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "history_shufflenet.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awigy9BoLv7H",
        "outputId": "d0474910-9883-4579-e5ce-9265419351e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4270794987678528,\n",
              " 0.579643726348877,\n",
              " 1.15090012550354,\n",
              " 1.072864294052124,\n",
              " 0.8060885071754456,\n",
              " 1.201610803604126,\n",
              " 0.44943535327911377,\n",
              " 0.3585020899772644,\n",
              " 0.1280478686094284,\n",
              " 0.19603563845157623]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "history_shufflenet.history['val_mse']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3PbUcOhd41W"
      },
      "outputs": [],
      "source": [
        "# Register the custom loss function when loading the model\n",
        "# Define the Huber loss function\n",
        "def huber_loss(y_true, y_pred, delta=1.0):\n",
        "    error = y_true - y_pred\n",
        "    quadratic_loss = 0.5 * tf.square(error)\n",
        "    linear_loss = delta * (tf.abs(error) - 0.5 * delta)\n",
        "    return tf.where(tf.abs(error) <= delta, quadratic_loss, linear_loss)\n",
        "\n",
        "with tf.keras.utils.custom_object_scope({'loss_mae': loss(mode=0)}):\n",
        "    model_4 = tf.keras.models.load_model('/content/drive/MyDrive/OpticsML/single_beam_res2_improved.h5')\n",
        "    # model_4.compile(optimizer='Nadam', loss=huber_loss,metrics=loss(mode=0))  # Compile the model with the custom loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gCoZdtG47X6"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import tensorflow as tf\n",
        "ma = []\n",
        "ms = []\n",
        "stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001)\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "less = False\n",
        "\n",
        "while not less:\n",
        "    history = model_4.fit(dataset[0], validation_data=dataset[1], epochs=10, shuffle=False, callbacks=[stop, reduce_lr,tensorboard_callback])\n",
        "    if min(history.history['val_loss']) > 0.08:\n",
        "        ma.append(history.history['val_loss'])\n",
        "        ms.append(history.history['val_mse'])\n",
        "        print(\"mae:\",history.history['val_loss'],'/n',\"mse:\",history.history['val_mse'])\n",
        "        model_4.save(\"single_beam_res2_improved.h5\")\n",
        "        shutil.copy(\"single_beam_res2_improved.h5\", \"/content/drive/MyDrive/OpticsML/\")\n",
        "    else:\n",
        "        model_4.save(\"single_beam_res2_improved.h5\")\n",
        "        shutil.copy(\"single_beam_res2_improved.h5\", \"/content/drive/MyDrive/OpticsML/\")\n",
        "        ma.append(history.history['val_loss'])\n",
        "        ms.append(history.history['val_mse'])\n",
        "        print(\"mae:\",history.history['val_loss'],'/n',\"mse:\",history.history['val_mse'])\n",
        "        less = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIezaOqIZtfR",
        "outputId": "2eb385ae-0be0-4890-d9eb-b0d6e6cc30d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.7930656671524048, 1.1803542375564575, 1.1821225881576538, 1.5787155628204346, 0.6319171190261841, 1.962738275527954, 0.6964985132217407, 2.0019049644470215, 1.7550748586654663, 0.5769467353820801]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[print(i) for i in ma]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r82sojFwZxbP",
        "outputId": "0f5b0ad2-4aba-40a8-deb5-a573d4774e1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.03714953735470772,\n",
              "  0.016429955139756203,\n",
              "  0.01647276245057583,\n",
              "  0.028713924810290337,\n",
              "  0.0050377496518194675,\n",
              "  0.0446791835129261,\n",
              "  0.005922794342041016,\n",
              "  0.045930806547403336,\n",
              "  0.035415828227996826,\n",
              "  0.00411661621183157]]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKjhbnIUHaj4"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001)\n",
        "less = False\n",
        "\n",
        "while not less:\n",
        "    history = model_shufflenet.fit(dataset[0], validation_data=dataset[1], epochs=10, shuffle=False, callbacks=[stop, reduce_lr])\n",
        "    if min(history.history['val_loss']) > 0.1:\n",
        "        model_shufflenet.save(\"single_beam_shufflenet_2.h5\")\n",
        "        shutil.copy(\"single_beam_shufflenet_2.h5\", \"/content/drive/MyDrive/OpticsML/\")\n",
        "    else:\n",
        "        model_shufflenet.save(\"single_beam_shufflenet_2.h5\")\n",
        "        shutil.copy(\"single_beam_shufflenet_2.h5\", \"/content/drive/MyDrive/OpticsML/\")\n",
        "        less = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb6Mma_UWIIA"
      },
      "outputs": [],
      "source": [
        "stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001)\n",
        "less = False\n",
        "\n",
        "while not less:\n",
        "    history = model_xception.fit(dataset[0], validation_data=dataset[1], epochs=10, shuffle=False, callbacks=[stop, reduce_lr])\n",
        "    if min(history.history['val_loss']) > 0.1:\n",
        "        model_xception.save(\"single_beam_xception_2.h5\")\n",
        "        shutil.copy(\"single_beam_xception_2.h5\", \"/content/drive/MyDrive/OpticsML/\")\n",
        "    else:\n",
        "        model_xception.save(\"single_beam_xception_2.h5\")\n",
        "        shutil.copy(\"single_beam_xception_2.h5\", \"/content/drive/MyDrive/OpticsML/\")\n",
        "        less = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "jOreZ5rLmtfQ",
        "outputId": "3782a8ff-9c60-4bcf-ca73-f3fa66152e39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-6d204ceeb938>:10: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"b\" (-> color=(0.0, 0.0, 1.0, 1)). The keyword argument will take precedence.\n",
            "  plt.plot(epochs, val_acc, 'b', label='Validation MAE', color = 'red')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAHHCAYAAABEEKc/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABv1UlEQVR4nO3dd1gUV9sG8HtpSxGwUwwqGhQ7VgQTK4pd8vpasRtNUaOSaNQo1kSj0aDRN8Z89kgsKWosKBJbFEvsxhpjiwpWBEERYb4/jruyFF1kd2d29/5d114Ms7MzzzKUh3Oec45KkiQJRERERKRlI3cARERERErDBImIiIgoByZIRERERDkwQSIiIiLKgQkSERERUQ5MkIiIiIhyYIJERERElAMTJCIiIqIcmCARERER5cAEiYhkceXKFahUKixbtky7b9KkSVCpVHq9XqVSYdKkSQaNqWnTpmjatKlBz0lE5okJEhG9UseOHeHs7IyUlJR8jwkPD4eDgwPu3btnwsgK7syZM5g0aRKuXLkidyhau3btgkqlgkqlwg8//JDnMY0aNYJKpUL16tV19j99+hRz585F7dq14ebmhqJFi6JatWoYPHgwzp07pz1u2bJl2mvk9Thw4IBR3yORubGTOwAiUr7w8HD89ttv+PXXX9GnT59cz6elpWHDhg1o3bo1SpQo8drXGT9+PMaMGVOYUF/pzJkzmDx5Mpo2bYry5cvrPLd9+3ajXvtVHB0dER0djV69eunsv3LlCvbv3w9HR8dcr+ncuTO2bt2KHj16YNCgQcjIyMC5c+ewadMmBAcHw9/fX+f4KVOmwNfXN9d53nzzTcO+GSIzxwSJiF6pY8eOcHV1RXR0dJ4J0oYNG5Camorw8PBCXcfOzg52dvL9WnJwcJDt2gDQtm1bbNy4EXfv3kXJkiW1+6Ojo+Hh4QE/Pz88ePBAu//w4cPYtGkTPv/8c4wbN07nXPPnz0dSUlKua7Rp0wb16tUz2nsgshTsYiOiV3JycsJ//vMfxMXF4fbt27mej46OhqurKzp27Ij79+/jk08+QY0aNVCkSBG4ubmhTZs2OHHixCuvk1cNUnp6OkaOHIlSpUppr/Hvv//meu3Vq1fx4YcfonLlynByckKJEiXQpUsXna60ZcuWoUuXLgCAZs2aabuXdu3aBSDvGqTbt29j4MCB8PDwgKOjI2rVqoXly5frHKOpp/rqq6+waNEiVKxYEWq1GvXr18fhw4df+b41OnXqBLVajXXr1unsj46ORteuXWFra6uz/9KlSwBE91tOtra2hWrNI7J2TJCISC/h4eF49uwZ1q5dq7P//v372LZtG9555x04OTnhn3/+wfr169G+fXvMmTMHo0aNwqlTp9CkSRPcvHmzwNd99913ERUVhVatWmHGjBmwt7dHu3btch13+PBh7N+/H927d8e8efPw/vvvIy4uDk2bNkVaWhoAoHHjxvjoo48AAOPGjcPKlSuxcuVKVKlSJc9rP378GE2bNsXKlSsRHh6OWbNmwd3dHf369cPcuXNzHR8dHY1Zs2bhvffew7Rp03DlyhX85z//QUZGhl7v1dnZGZ06dcKPP/6o3XfixAn89ddf6NmzZ67jy5UrBwBYtWoVnj17ptc1Hj58iLt37+o8lF43RiQLiYhID8+ePZO8vLykoKAgnf0LFy6UAEjbtm2TJEmSnjx5ImVmZuocc/nyZUmtVktTpkzR2QdAWrp0qXbfxIkTpey/lo4fPy4BkD788EOd8/Xs2VMCIE2cOFG7Ly0tLVfM8fHxEgBpxYoV2n3r1q2TAEg7d+7MdXyTJk2kJk2aaD+PioqSAEg//PCDdt/Tp0+loKAgqUiRIlJycrLOeylRooR0//597bEbNmyQAEi//fZbrmtlt3PnTgmAtG7dOmnTpk2SSqWSrl27JkmSJI0aNUqqUKGCNr5q1appX5eVlSU1adJEAiB5eHhIPXr0kBYsWCBdvXo11zWWLl0qAcjzoVarXxofkTViCxIR6cXW1hbdu3dHfHy8TreVpj6mRYsWAAC1Wg0bG/GrJTMzE/fu3UORIkVQuXJlHD16tEDX3LJlCwBoW300RowYketYJycn7XZGRgbu3buHN998E0WLFi3wdbNf39PTEz169NDus7e3x0cffYRHjx5h9+7dOsd369YNxYoV037+9ttvAwD++ecfva/ZqlUrFC9eHKtXr4YkSVi9erXO9bNTqVTYtm0bpk2bhmLFiuHHH3/EkCFDUK5cOXTr1i3PGqQFCxYgNjZW57F161a94yOyFkyQiEhvmiLs6OhoAMC///6LvXv3onv37tr6mKysLHz99dfw8/ODWq1GyZIlUapUKZw8eRIPHz4s0PWuXr0KGxsbVKxYUWd/5cqVcx37+PFjREZGwsfHR+e6SUlJBb5u9uv7+flpEz4NTZfc1atXdfaXLVtW53NNspS9sPpV7O3t0aVLF0RHR2PPnj24fv16nt1rGmq1Gp999hnOnj2Lmzdv4scff0TDhg2xdu1aDB06NNfxDRo0QEhIiM6jWbNmesdHZC2YIBGR3urWrQt/f39tjcyPP/4ISZJ0Rq998cUXiIiIQOPGjfHDDz9g27ZtiI2NRbVq1ZCVlWW02IYNG4bPP/8cXbt2xdq1a7F9+3bExsaiRIkSRr1udjmLqDUkSSrQeXr27Injx49j0qRJqFWrFqpWrarX67y8vNC9e3fs2bMHfn5+WLt2rd61SUSki8P8iahAwsPDMWHCBJw8eRLR0dHw8/ND/fr1tc//9NNPaNasGRYvXqzzuqSkJJ2h6/ooV64csrKycOnSJZ1Wo/Pnz+c69qeffkLfvn0xe/Zs7b4nT57k6mbSd6ZuzfVPnjyJrKwsnVYkzQSMmiJpQ3vrrbdQtmxZ7Nq1C19++WWBX29vb4+aNWvi4sWLuHv3Ljw9PY0QJZFlYwsSERWIprUoMjISx48fzzX3ka2tba4Wk3Xr1uHGjRsFvlabNm0AAPPmzdPZHxUVlevYvK77zTffIDMzU2efi4sLAORZn5NT27ZtkZCQgDVr1mj3PXv2DN988w2KFCmCJk2a6PM2CkylUmHevHmYOHEievfune9xFy9exLVr13LtT0pKQnx8PIoVK4ZSpUoZJUYiS8cWJCIqEF9fXwQHB2PDhg0AkCtBat++PaZMmYL+/fsjODgYp06dwqpVq1ChQoUCXysgIAA9evTA//73Pzx8+BDBwcGIi4vD33//nevY9u3bY+XKlXB3d0fVqlURHx+PHTt25JoLKCAgALa2tvjyyy/x8OFDqNVqNG/eHKVLl851zsGDB+O7775Dv379cOTIEZQvXx4//fQT9u3bh6ioKLi6uhb4PemrU6dO6NSp00uPOXHiBHr27Ik2bdrg7bffRvHixXHjxg0sX74cN2/eRFRUVK5uv61bt+osQaIRHBz8WveIyFIxQSKiAgsPD8f+/fvRoEGDXEtUjBs3DqmpqYiOjsaaNWtQp04dbN68+bWXEFmyZAlKlSqFVatWYf369WjevDk2b94MHx8fnePmzp0LW1tbrFq1Ck+ePEGjRo2wY8cOhIaG6hzn6emJhQsXYvr06Rg4cCAyMzOxc+fOPBMkJycn7Nq1C2PGjMHy5cuRnJyMypUrY+nSpejXr99rvR9Daty4MaZOnYqtW7dizpw5uHPnDlxdXVG7dm18+eWX6Ny5c67XREZG5nmupUuXMkEiykYlFbR6kIiIiMjCsQaJiIiIKAcmSEREREQ5MEEiIiIiyoEJEhEREVEOTJCIiIiIcmCCRERERJSD7PMgLViwALNmzUJCQgJq1aqFb775Bg0aNMj3+HXr1mHChAm4cuUK/Pz88OWXX6Jt27ba5yVJwsSJE/H9998jKSkJjRo1wrfffgs/Pz/tMRcuXMCoUaOwb98+PH36FDVr1sTUqVMLtGBjVlYWbt68CVdX1wItXUBERETykSQJKSkp8Pb2zrUQdc4DZbN69WrJwcFBWrJkifTXX39JgwYNkooWLSolJibmefy+ffskW1tbaebMmdKZM2ek8ePHS/b29tKpU6e0x8yYMUNyd3eX1q9fL504cULq2LGj5OvrKz1+/Fh7jJ+fn9S2bVvpxIkT0oULF6QPP/xQcnZ2lm7duqV37NevX5cA8MEHH3zwwQcfZvi4fv36S//OyzpRZGBgIOrXr4/58+cDEK0yPj4+GDZsWJ6z7nbr1g2pqanYtGmTdl/Dhg0REBCAhQsXQpIkeHt74+OPP8Ynn3wCAHj48CE8PDywbNkydO/eHXfv3kWpUqWwZ88evP322wCAlJQUuLm5ITY2FiEhIXrF/vDhQxQtWhTXr1+Hm5tbYb8UFicjIwPbt29Hq1atYG9vL3c4BN4TpeH9UBbeD2Ux5v1ITk6Gj48PkpKS4O7unu9xsnWxPX36FEeOHMHYsWO1+2xsbBASEoL4+Pg8XxMfH4+IiAidfaGhoVi/fj0A4PLly0hISNBJctzd3REYGIj4+Hh0794dJUqUQOXKlbFixQrUqVMHarUa3333HUqXLo26devmG296ejrS09O1n6ekpAAQSxE4OTkV+P1bOjs7Ozg7O8PJyYm/bBSC90RZeD+UhfdDWYx5PzIyMgDgleUxsiVId+/eRWZmJjw8PHT2e3h45LmQIgAkJCTkeXxCQoL2ec2+/I5RqVTYsWMHwsLC4OrqChsbG5QuXRoxMTEoVqxYvvFOnz4dkydPzrV/+/btcHZ2fsW7tV6xsbFyh0A58J4oC++HsvB+KIsx7kdaWppex8lepG1qkiRhyJAhKF26NPbu3QsnJyf83//9Hzp06IDDhw/Dy8srz9eNHTtWp/VK00TXqlUrdrHlISMjA7GxsWjZsiX/G1MI3hNl4f1QFt4PZTHm/UhOTtbrONkSpJIlS8LW1haJiYk6+xMTE+Hp6Znnazw9PV96vOZjYmKiTqKTmJiIgIAAAMDvv/+OTZs24cGDB9rE5n//+x9iY2OxfPnyfFccV6vVUKvVufbb29vzh+kl+PVRHt4TZeH9UBbeD2Uxxv3Q93yyJUgODg6oW7cu4uLiEBYWBkAUacfFxWHo0KF5viYoKAhxcXEYMWKEdl9sbCyCgoIAAL6+vvD09ERcXJw2IUpOTsbBgwfxwQcfAHjRtJZzaJ+NjQ2ysrIM+A6JiOhlMjMztfUgcsvIyICdnR2ePHmCzMxMucOxeoW5H/b29rC1tS10DLJ2sUVERKBv376oV68eGjRogKioKKSmpqJ///4AgD59+qBMmTKYPn06AGD48OFo0qQJZs+ejXbt2mH16tX4888/sWjRIgCivmjEiBGYNm0a/Pz84OvriwkTJsDb21ubhAUFBaFYsWLo27cvIiMj4eTkhO+//x6XL19Gu3btZPk6EBFZE0mSkJCQgKSkJLlD0ZIkCZ6enrh+/TrntlOAwt6PokWLwtPTs1D3UtYEqVu3brhz5w4iIyORkJCAgIAAxMTEaIusr127ptPSExwcjOjoaIwfPx7jxo2Dn58f1q9fj+rVq2uPGT16NFJTUzF48GAkJSXhrbfeQkxMDBwdHQGIrr2YmBh89tlnaN68OTIyMlCtWjVs2LABtWrVMu0XgIjICmmSo9KlS8PZ2VkRCUlWVhYePXqEIkWKvHzyQDKJ170fkiQhLS0Nt2/fBoB864r1Ies8SOYsOTkZ7u7uePjwIYu085CRkYEtW7agbdu27M9XCN4TZbHW+5GZmYkLFy6gdOnSKFGihNzhaGVlZSE5ORlubm5MkBSgsPfj3r17uH37NipVqpSru03fv9/8LiAiIpPR1BxxehQyJs33V2Fq3JggERGRySmhW40slyG+v5ggEREREeXABImIiMhEmjZtqjNVTfny5REVFfXS16hUKu2SWoVhqPNYCyZIREREr9ChQwe0bt06z+f27t0LlUqFkydPFvi8hw8fxuDBgwsbno5JkyZp5wLM7tatW2jTpo1Br5XTsmXLoFKpUKVKlVzPrVu3DiqVCuXLl8/13OPHj1G8eHGULFlSZ91TjfLly0OlUuV6zJgxwxhvAwATJDIimzy+yYmIzNHAgQMRGxuLf//9N9dzS5cuRb169VCzZs0Cn7dUqVImK1j39PTMc0UIQ3NxccHt27dzLTy/ePFilC1bNs/X/Pzzz6hWrRr8/f3zbeWaMmUKbt26pfMYNmyYocPXYoJERmEzbx7a9+gB1Zo1codCRFRo7du3R6lSpbBs2TKd/Y8ePcK6deswcOBA3Lt3Dz169ECZMmXg7OyMGjVq4Mcff3zpeXN2sV28eBGNGzeGo6MjqlatmudirZ9++ikqVaoEZ2dnVKhQARMmTNCO1lq2bBkmT56MEydOaFtZNDHn7GI7deoUmjdvDicnJ5QoUQKDBw/Go0ePtM/369cPYWFh+Oqrr+Dl5YUSJUpgyJAhrxwZZmdnh549e2LJkiXaff/++y927dqFnj175vmaxYsXo1evXujVqxcWL16c5zGurq7w9PTUebi4uLw0lsKwusVqyTRsli+HKisLth9/DHToALi7yx0SESmVJAF6rrBucM7OgB4jnuzs7NCnTx8sW7YMn332mXaU1Lp165CZmYkePXrg0aNHqFu3Lj799FO4ublh8+bN6N27NypWrIgGDRq88hpZWVn4z3/+Aw8PDxw8eBAPHz7UqVfScHV1xbJly+Dt7Y1Tp05h0KBBcHV1xejRo9GtWzecPn0aMTEx2LFjBwDAPY/fv6mpqQgNDUVQUBAOHz6M27dv491338XQoUN1ksCdO3fCy8sLO3fuxN9//41u3bohICAAgwYNeul7GTBgAJo2bYq5c+fC2dkZy5YtQ+vWrbUTQWd36dIlxMfH45dffoEkSRg5ciSuXr2KYsWKvfJrZkxsQSLDu3ULqlOnAACq27eBKVNkDoiIFC0tDShSRJ5HARKzAQMG4NKlS9i9e7d239KlS9G5c2e4u7ujTJky+OSTTxAQEIAKFSpg2LBhaN26NdauXavX+Xfs2IFz585hxYoVqFWrFho3bowvvvgi13Hjx49HcHAwypcvjw4dOuCTTz7RXsPJyQlFihSBnZ2dtpXFyckp1zmio6Px5MkTrFixAtWrV0fz5s0xf/58rFy5UmdR+GLFimH+/Pnw9/dH+/bt0a5dO8TFxb3yvdSuXRsVKlTATz/9BEmSsGzZMgwYMCDPY5csWYI2bdqgWLFiKF68OEJDQ3O11AGi5axIkSI6j717974yltfFBIkM7/l/LU+LFBGfz5sHnDsnY0BERIXn7++P4OBgbdfR33//jb1792LgwIEAxCzhU6dORY0aNVC8eHEUKVIE27Ztw7Vr1/Q6/9mzZ+Hj4wNvb2/tPs1i7NmtWbMGjRo1gqenJ4oUKYLx48frfY3s16pVq5ZOF1WjRo2QlZWF8+fPa/dVq1ZNZyZqLy8v7TIerzJgwAAsXboUu3fvRmpqKtq2bZvrmMzMTCxfvhy9evXS7uvVqxeWL1+eawH5UaNG4fjx4zqPevXq6f2eC4pdbGR427cDAK6EhuLNx49hs2ULMHIksGWLXk3ZRGRlnJ2BbLUvJr92AQwcOBDDhg3DggULsHTpUlSsWBFNmjQBAMyaNQtz585FVFQUatSoARcXF4wYMQJPnz41WLjx8fEIDw/H5MmTERoaCnd3d6xevRqzZ8822DWyy7kMjkqlypW45Cc8PByjR4/GpEmT0Lt3b9jZ5U45tm3bhhs3bqBbt246+zMzM7F792506tRJu69kyZJ48803X+NdvB62IJFhZWUBz4sKbwcEIHPWLMDeHoiJATZvljk4IlIklQpwcZHnUcB/2rp27QobGxtER0djxYoVGDBggLYead++fejUqRN69eqFWrVqoUKFCrhw4YLe565SpQquX7+OW7duafcdOHBA55j9+/ejXLly+Oyzz1CvXj34+fnh6tWrOsc4ODggMzPzldc6ceIEUlNTtfv27dsHGxsbVK5cWe+YX6Z48eLo2LEjdu/enW/32uLFi9G9e/dcLUPdunXDypUrDRLH62KCRIZ16hSQmAjJxQUP/P0BPz8gIkI8N2IEwKH/RGTGihQpgm7dumHs2LG4desW+vXrp33Oz88PsbGx2L9/P86ePYv33ntPp57nVUJCQlCpUiX07dsXJ06cwN69e/HZZ5/pHOPn54dr165h9erVuHTpEubNm4dff/1V55jy5cvj8uXLOH78OO7evZvnvELh4eFwdHRE3759cfr0aezcuRPDhg1D79698yykfl3Lli3D3bt34e/vn+u5O3fu4LfffkPfvn1RvXp1nUfv3r2xZcsW3L9/X3t8SkoKEhISdB7JyckGizUnJkhkWM+716QmTZClaZr97DPAywu4dAl4xYyxRERKN3DgQDx48AChoaE69ULjx49HnTp1EBoaiqZNm8LT0xNhYWF6n9fGxga//vorHj9+jAYNGuDdd9/F559/rnNMx44dMXLkSAwdOhQBAQHYv38/JkyYoHNM586d0bp1azRr1gylSpXKc6oBZ2dnbNu2Dffv30f9+vXx3//+Fy1atMD8+fML9sV4Bc0UAnlZsWIFXFxc0KJFi1zPtWjRAo6Ojli1apV2X2RkJLy8vHQeo0ePNmi82akkSZKMdnYLlpycDHd3dzx8+BBubm5yh6McLVsCO3Ygc84cbKpQAW3bthV92CtWAH37ilEjFy6IhIlMKiMjA1u2bHlxT0hW1no/njx5gsuXL8PX1xeOjo5yh6OVlZWF5ORkuLm5wcaGbQdyK+z9eNn3mb5/v/ldQIaTlgY8H3KZFRKi+1yvXkBgoCjEHDNGhuCIiIj0xwSJDGfvXlFj5OMD5Czys7EBvvlGbK9YAeQoPCQiIlISJkhkOM/rj9CqVd4jQ+rXB/r3F9sffSRGvBERESkQEyQynOwJUn6++AJwdQUOHwaWLzdNXERERAXEBIkM4+ZN4PRp0XKUx4gELU9PYOJEsT1mDPDwoWniIyJF4fggMiZDfH8xQSLD0Kw4Xa8ekM+QTq1hw0SN0u3bwNSpxo+NiBRDM2IvTa7FackqaL6/CjNClEuNkGHo072m4eAAfP010LYtMHcuMGhQ7qJuIrJItra2KFq0qHY9L2dnZ+1M1HLKysrC06dP8eTJEw7zV4DXvR+SJCEtLQ23b99G0aJFddaRKygmSFR42ZYX0StBAoA2bYB27cTyIyNGcJ02Iivi6ekJAHovemoKkiTh8ePHcHJyUkTCZu0Kez+KFi2q/T57XUyQqPBOnADu3BGTQDZsqP/rvv5atDxp1mlr3954MRKRYqhUKnh5eaF06dLIyMiQOxwAYuLOPXv2oHHjxlY1cadSFeZ+2NvbF6rlSIMJEhWepnutWTPRfaYvPz9g5Ehg5kzxsWVLQK02ToxEpDi2trYG+UNmCLa2tnj27BkcHR2ZICmAEu4HO1qp8ApSf5TT+PFiZNvff4t6JCIiIgVggkSFk5oK/PGH2H6dBMnVFZgxQ2xPnQrcumW42IiIiF4TEyQqnD17gKdPgXLlRJfZ6+jdm+u0ERGRojBBosJ51fIi+rCxAebNE9srVgAHDxomNiIiotfEBIkKpzD1R9k1aAD06ye2hw3jOm1ERCQrJkj0+v79FzhzRrQANW9e+PNNn8512oiISBGYINHr00wOWb8+ULx44c/n6QlERortsWOB5OTCn5OIiOg1MEGi12eo7rXsPvoIqFQJSEzkOm1ERCQbJkj0el5neRF9aNZpA8S8SOfPG+7cREREemKCRK/n2DHg3j1RMxQYaNhzt20r1mnLyBAzbBMREZkYEyR6PZrutebNAWNMA//11+K8W7eKddqIiIhMiAkSvR5j1B9l5+cHjBghtkeOBNLTjXMdIiKiPDBBooJ79AjYt09sGytBAl6s03bxItdpIyIik2KCRAW3e7eoD/L1BSpWNN513Ny4ThsREcmCCRIVnCGWF9FX795ilu1Hj8TcSERERCbABIkKztj1R9llX6dt+XKu00ZERCbBBIkK5to14Nw5wy0voo/AQK7TRkREJsUEiQpGMzlkYCBQtKjprpt9nbYVK0x3XSIiskpMkKhgTNm9lp2nJzBhgtgeM4brtBERkVExQSL9ZWYCO3aIbVMnSAAwfLiYHykxEZg2zfTXJyIiq8EEifR39Chw/74Yft+ggemv7+AAREWJ7ago4MIF08dARERWgQkS6U/TvdaiBWBnJ08MbduKB9dpIyIiI2KCRPqTq/4oJ806bVu2cJ02IiIyCiZIpJ+UFGD/frEtd4JUqZLuOm1Pn8oaDhERWR4mSKSfXbuAZ8/E0iIVKsgdjVinzcOD67QREZFRMEEi/Sile00j5zptCQnyxkNERBaFCRLpR2kJEgD06SNG06WkcJ02IiIyKCZI9GpXrogh9ba2QLNmckfzQvZ12pYt4zptRERkMIpIkBYsWIDy5cvD0dERgYGBOHTo0EuPX7duHfz9/eHo6IgaNWpgy5YtOs9LkoTIyEh4eXnByckJISEhuHjxovb5Xbt2QaVS5fk4fPiwUd6jWdMsL9KwIeDuLm8sOQUGAn37iu2PPuI6bUREZBCyJ0hr1qxBREQEJk6ciKNHj6JWrVoIDQ3F7du38zx+//796NGjBwYOHIhjx44hLCwMYWFhOH36tPaYmTNnYt68eVi4cCEOHjwIFxcXhIaG4smTJwCA4OBg3Lp1S+fx7rvvwtfXF/Xq1TPJ+zYrSuxey276dKBIEeDQIWDlSrmjISIiCyB7gjRnzhwMGjQI/fv3R9WqVbFw4UI4OztjyZIleR4/d+5ctG7dGqNGjUKVKlUwdepU1KlTB/PnzwcgWo+ioqIwfvx4dOrUCTVr1sSKFStw8+ZNrF+/HgDg4OAAT09P7aNEiRLYsGED+vfvD5VKZaq3bh7kXl5EH15eQGSk2P70U67TRkREhSbTdMjC06dPceTIEYzNVmBrY2ODkJAQxMfH5/ma+Ph4RERE6OwLDQ3VJj+XL19GQkICQkJCtM+7u7sjMDAQ8fHx6N69e65zbty4Effu3UP//v3zjTU9PR3p6enaz5Of/xHOyMhARkbGq9+smVIdOgS7pCRIRYviWa1aYgZrPWi+Jib72nz4IewWLYLq77+ROXkysjQj3EjL5PeEXor3Q1l4P5TFmPdD33PKmiDdvXsXmZmZ8PDw0Nnv4eGBc+fO5fmahISEPI9PeD7MW/PxZcfktHjxYoSGhuKNN97IN9bp06dj8uTJufZv374dzs7O+b7O3FVaswZVANyqUgWHNV1tBRCrqV8yAY/u3dFw2jSo5s3D7ooVkVqmjMmubU5MeU/o1Xg/lIX3Q1mMcT/S0tL0Ok7WBEkJ/v33X2zbtg1r16596XFjx47VablKTk6Gj48PWrVqBTc3N2OHKRvbWbMAAKV790bbtm31fl1GRgZiY2PRsmVL2NvbGys8XW3bIuvPP2ETE4Pmmzcj83mrIgmy3BPKF++HsvB+KIsx70eynmUYsiZIJUuWhK2tLRITE3X2JyYmwtPTM8/XeHp6vvR4zcfExER4eXnpHBMQEJDrfEuXLkWJEiXQsWPHl8aqVquhVqtz7be3t7fcH6bkZODAAQCAXevWYv2zAjL51ycqCqhRAzZbtsAmNlYsbEs6LPp71gzxfigL74eyGON+6Hs+WYu0HRwcULduXcTFxWn3ZWVlIS4uDkFBQXm+JigoSOd4QDTBaY739fWFp6enzjHJyck4ePBgrnNKkoSlS5eiT58+/IHIy86dokjbzw/w9ZU7Gv1UrgwMHy62uU4bERG9JtlHsUVEROD777/H8uXLcfbsWXzwwQdITU3VFkz36dNHp4h7+PDhiImJwezZs3Hu3DlMmjQJf/75J4YOHQoAUKlUGDFiBKZNm4aNGzfi1KlT6NOnD7y9vREWFqZz7d9//x2XL1/Gu+++a7L3a1aUPrw/PxMmiHXaLlx4MZEkERFRAcheg9StWzfcuXMHkZGRSEhIQEBAAGJiYrRF1teuXYONzYs8Ljg4GNHR0Rg/fjzGjRsHPz8/rF+/HtWrV9ceM3r0aKSmpmLw4MFISkrCW2+9hZiYGDg6Oupce/HixQgODoa/v79p3qy5MdcEyc1NzI00YAAwZQrQqxeQT5ctERFRXlSSJElyB2GOkpOT4e7ujocPH1pmkfY//wAVKwJ2dsC9eyLpKICMjAxs2bIFbdu2laf7MitLzPx9+DDQrx+wdKnpY1AY2e8J6eD9UBbeD2Ux5v3Q9++37F1spFCaoZVBQQVOjhTBxgb45huxvWyZmGWbiIhIT0yQKG/m2r2WXWAg0KeP2OY6bUREVABMkCi3Z88AzShAc06QAGDGDLFO28GDXKeNiIj0xgSJcjt8GHj4EChWDKhbV+5oCsfLS4xqA4AxY4CUFHnjISIis8AEiXLTdK+FhAC2tvLGYgjDhwNvvgkkJADTpskdDRERmQEmSJSbJdQfZadWA19/Lba//hq4eFHeeIiISPGYIJGupCRRrwMALVvKGopBtWsHtGkDZGSIGbaJiIheggkS6dIsL1K5MlCunNzRGI5KJVqP7OyAzZuBrVvljoiIiBSMCRLpsrTuteyyr9M2YgTXaSMionwxQSJdlpwgAUBk5It12jQTSRIREeXABIleuHRJLDFibw80bSp3NMahWacNACZPFiPbiIiIcmCCRC9oWo+Cg8Xkipaqb1+gfn0xJ9K4cXJHQ0RECsQEiV6w9O41DRsbYN48sb10KddpIyKiXJggkZCRAfz+u9i29AQJABo25DptRESULyZIJBw6BCQnAyVKALVryx2NaWRfp+2HH+SOhoiIFIQJEgmWtryIPry8gPHjxfann3KdNiIi0mKCRIK11B/lNGLEi3XaPv9c7miIiEghmCAR8ODBi0JlS1peRB/Z12mbM4frtBEREQAmSASI4uysLKBKFcDHR+5oTK9dO6B1a1GoHhEhdzRERKQATJDIervXNLKv07ZpExATI3dEREQkMyZI1k6SgG3bxLa1JkgA4O/PddqIiEiLCZK1+/tv4OpVsbxIkyZyRyOvCROA0qWB8+e5ThsRkZVjgmTtNN1rb70FuLjIG4vc3N1frNM2ZQqQmChvPEREJBsmSNbO2uuPcurXD6hXT0yayXXaiIisFhMka2Zty4voI/s6bUuWAIcPyxsPERHJggmSNTtwAHj0CChZEggIkDsa5QgKAnr3Fttcp42IyCoxQbJm2ZcXseG3go4ZM0RN1oEDwKpVckdDREQmxr+K1oz1R/nz9haj2gCu00ZEZIWYIFmr+/df1NdY2/Ii+tKs03brFjB/vtzREBGRCTFBslZxcWKSyKpVgTfekDsaZVKrgZEjxfbu3fLGQkREJsUEyVqxe00/9euLj0eOiISSiIisAhMkayRJTJD0VaOGWKPt7l3g2jW5oyEiIhNhgmSNLlwQf+wdHIDGjeWORtkcHYHq1cX2kSPyxkJERCbDBMkacXmRgqlXT3xkgkREZDWYIFkjdq8VTN264iMTJCIiq8EEydo8fQrs2iW2mSDpR5Mg/fknC7WJiKwEEyRro1lepFQpoFYtuaMxDzVrAvb2wL17LNQmIrISTJCsjaZ7rWVLLi+iL7WahdpERFaGfyGtDeuPXk/2bjYiIrJ4TJCsyb17L/7Ac3mRgmGhNhGRVWGCZE00y4tUry4WYyX9ZR/qz0JtIiKLxwTJmrB77fXVqMFCbSIiK8IEyVpweZHCyV6ozTokIiKLxwTJWpw/D1y/Lv7Qv/223NGYJ86oTURkNZggWQtN69HbbwPOzvLGYq5YqE1EZDWYIFkLdq8VHmfUJiKyGkyQrEF6OrBzp9hmgvT6NIXa9+8DV6/KHQ0RERkREyRrEB8PpKUBHh7ijzy9HrX6xdeP3WxERBaNCZI14PIihsM6JCIiq8C/ltaA9UeGwyVHiIisAhMkS3fnDnD0qNgOCZE3FkvAGbWJiKwCEyRLp1lepGZNwMtL7mjMX/XqLNQmIrICsidICxYsQPny5eHo6IjAwEAcOnTopcevW7cO/v7+cHR0RI0aNbBlyxad5yVJQmRkJLy8vODk5ISQkBBcvHgx13k2b96MwMBAODk5oVixYggLCzPk21IOdq8ZVvZCbXazERFZLFkTpDVr1iAiIgITJ07E0aNHUatWLYSGhuL27dt5Hr9//3706NEDAwcOxLFjxxAWFoawsDCcPn1ae8zMmTMxb948LFy4EAcPHoSLiwtCQ0Px5MkT7TE///wzevfujf79++PEiRPYt28fevbsafT3a3JcXsQ4WKhNRGTxZE2Q5syZg0GDBqF///6oWrUqFi5cCGdnZyxZsiTP4+fOnYvWrVtj1KhRqFKlCqZOnYo6depg/vz5AETrUVRUFMaPH49OnTqhZs2aWLFiBW7evIn169cDAJ49e4bhw4dj1qxZeP/991GpUiVUrVoVXbt2NdXbNp2zZ4EbNwBHR+Ctt+SOxnJwyREiIotnJ9eFnz59iiNHjmDs2LHafTY2NggJCUF8fHyer4mPj0dERITOvtDQUG3yc/nyZSQkJCAkWzGyu7s7AgMDER8fj+7du+Po0aO4ceMGbGxsULt2bSQkJCAgIACzZs1Cdc1ipHlIT09Henq69vPk5GQAQEZGBjIyMgr8/k3BZutW2ALIevttZNrZASaMU/M1UerXplBq1YI9AOnIETx7+hRQqeSOSC8WfU/MEO+HsvB+KIsx74e+55QtQbp79y4yMzPh4eGhs9/DwwPnzp3L8zUJCQl5Hp+QkKB9XrMvv2P++ecfAMCkSZMwZ84clC9fHrNnz0bTpk1x4cIFFC9ePM9rT58+HZMnT861f/v27XBW6NpmDaOj4QHgTJkyuJSjVstUYmNjZbmuMdlkZKCdnR1s7t/HrmXLkJbj+03pLPGemDPeD2Xh/VAWY9yPtLQ0vY6TLUGSS1ZWFgDgs88+Q+fOnQEAS5cuxRtvvIF169bhvffey/N1Y8eO1Wm9Sk5Oho+PD1q1agU3NzfjB15Q6emw69EDAFB52DBUNvEM2hkZGYiNjUXLli1hb29v0mubRM2awNGjaObmBqltW7mj0YvF3xMzw/uhLLwfymLM+6HpAXoV2RKkkiVLwtbWFomJiTr7ExMT4enpmedrPD09X3q85mNiYiK8sg1pT0xMREBAAABo91etWlX7vFqtRoUKFXDt2rV841Wr1VCr1bn229vbK/OHae9e4PFjwNMT9rVry9YNpNivT2HVqwccPQq7EyeA7t3ljqZALPaemCneD2Xh/VAWY9wPfc8nW5G2g4MD6tati7i4OO2+rKwsxMXFISgoKM/XBAUF6RwPiOY3zfG+vr7w9PTUOSY5ORkHDx7UHlO3bl2o1WqcP39ee0xGRgauXLmCcuXKGez9yS776DUzqZExK5xRm4jIosnaxRYREYG+ffuiXr16aNCgAaKiopCamor+/fsDAPr06YMyZcpg+vTpAIDhw4ejSZMmmD17Ntq1a4fVq1fjzz//xKJFiwAAKpUKI0aMwLRp0+Dn5wdfX19MmDAB3t7e2nmO3Nzc8P7772PixInw8fFBuXLlMGvWLABAly5dTP9FMBYO7zeu7EP9JYlJKBGRhZE1QerWrRvu3LmDyMhI7WiymJgYbZH1tWvXYJNtcdXg4GBER0dj/PjxGDduHPz8/LB+/Xqd0WejR49GamoqBg8ejKSkJLz11luIiYmBo6Oj9phZs2bBzs4OvXv3xuPHjxEYGIjff/8dxYoVM92bN6bbt4Fjx8Q2lxcxjurVAQcH4MED4MoVwNdX7oiIiMiAZC/SHjp0KIYOHZrnc7t27cq1r0uXLi9t6VGpVJgyZQqmTJmS7zH29vb46quv8NVXXxU4XrOwY4f4GBAAmNkIK7OhmVH7yBHxYIJERGRRZF9qhIyA3WumwTokIiKLxQTJ0nB5EdPhjNpERBaLCZKl+esv4NYtwMkJaNRI7mgsW85CbSIishhMkCyNpvWoSROxBhsZT/ZC7cuX5Y6GiIgMiAmSpWH3muk4OIhCbYDdbEREFoYJkiV58gTYvVtsM0EyDdYhERFZJCZIluSPP0SS5O0NZFtKhYwoex0SERFZDCZIloTLi5geC7WJiCwSEyRLwvoj02OhNhGRRWKCZCkSEoATJ8Q2lxcxHQcHoGZNsc1uNiIii8EEyVJolhepUwcoVUreWKwNZ9QmIrI4TJAsBbvX5MNCbSIii8MEyRJweRF5aYb6Hz3KQm0iIgvBBMkSnDoFJCYCzs5AcLDc0VifatVYqE1EZGGYIFkCTetR06aAWi1rKFYpe6E265CIiCwCEyRLwO41+XFGbSIii8IEydw9fgzs2SO2mSDJh4XaREQWhQmSudu7F0hPB954A/D3lzsa68UZtYmILAoTJHPH5UWUoVo1Uf+VlAT884/c0RARUSExQTJ3rD9SBs6oTWTe0tJg9/ix3FGQgjBBMme3bokh/ioV0KKF3NEQ65CIzFNGBuzq1UPzIUOA27fljoYUggmSOYuNFR/r1gVKlpQ3FuKSI0TmavduqP7+G07378M2MlLuaEghmCCZM3avKQtn1CYyT7/+qt1ULV3Kf3IIABMk85WV9aIFiQmSMrBQm8j8ZGUBGzYAAB55e0MlScBHH4n9ZNWYIJmrkydFX7mLCxAUJHc0BAD29pxRm8jc/PkncOMGpCJFEB8ZCcnFBYiPB1atkjsykhkTJHOl6V5r1kyMoCJl4IzaROblefea1Lo10jw9kTV2rNj/6adASoqMgZHcmCCZK9YfKRNHshGZl/XrAQBZnTqJj8OHAxUrilHCn38uY2AkNyZI5igtTcygDTBBUhrOqE1kPs6dEw97e0itW4t9ajUQFSW258wBLl6ULTySFxMkc7RnD/D0KVC2LFCpktzRUHaaQu2HD4FLl+SOhoheRjN6rUULwN39xf527YA2bYCMDGDkSHliI9kxQTJHXF5EueztgVq1xDa72YiUTZMgvfOO7n6VCvj6a/HzvHmzeJDVYYJkjlh/pGysQyJSvn//BQ4fFslQx465n69cGRg+XGyPHCkWBSerwgTJ3Ny4Afz1F5cXUTLOqE2kfM/nPkJQEODpmfcxEyYAHh6iDmnuXNPFRorABMncaCaHrF8fKF5c3lgob5xRm0j58utey87NDfjyS7E9dSpw86bx4yLFYIJkbti9pnxVq7JQm0jJ7t8Hdu0S22FhLz+2d28gMBB49AgYM8bYkZGCMEEyJ1xexDywUJtI2TZvBjIzgerVgTfffPmxNjbAN9+I7ZUrxSzbZBWYIJmT48eBu3eBIkWAhg3ljoZehnVIRMqlT/dadvXrAwMGiO1hw7hOm5UoUIJ06NAhZGZm5vt8eno61q5dW+igKB+a7rXmzUUrBSkXlxwhUqa0NCAmRmzrmyABwBdfiJqkI0eApUuNExspSoESpKCgINy7d0/7uZubG/7Jtmp5UlISevToYbjoSBfrj8yHpgXp6FH+t0mkJLGxwOPHQLlyQECA/q/z8AAmTRLbY8cCSUlGCI6UpEAJkpRjRE7Oz/PbRwaQmgr88YfYZoKkfCzUJlImTfdaWFjBJ9odOhSoUgW4cweYPNngoZGyGLwGScWZnY1j924x7X358q8uKiT52du/+O+U3WxEyvDsGfDbb2K7IN1rGvb2L+ZD+uYb4MwZw8VGisMibXPB5UXMD2fUJlKWPXvEEP+SJYG33nq9c7RsCXTqJEbBffQR5zqzYHYFfcGZM2eQkJAAQHSnnTt3Do8ePQIA3L1717DR0QusPzI/HMlGpCzr14uPHTsCtravf545c0Shd1ycOOfrtEaR4hU4QWrRooVOnVH79u0BiK41SZLYxWYM168DZ8+K+TiaN5c7GtJXzkJtGzbYEslGkl4kSK+aHPJVKlQAPvkE+PxzICICaN0acHIqbISkMAVKkC5fvmysOOhlNJNDNmgAFCsmbyykv6pVAUdHIDlZFGr7+ckdEZH1OnJE/LPp4iK6yQpr7Fhg2TLgyhXgq6/Eum1kUQqUIJUrV+6Vx5w+ffq1g6F8sHvNPGlm1D54UPxyZoJEJB/N6LU2bcQ/LoXl4iISox49gOnTgb59gbJlC39eUgyDtPmnpKRg0aJFaNCgAWppllggw8jM5PIi5ox1SETKYKjutey6dQPeflvMqzR6tOHOS4pQqARpz5496Nu3L7y8vPDVV1+hefPmOHDggKFiIwA4dkyMunBzE11sZF44ozaR/C5cEEPy7eyAdu0Md16VCpg3T9QXrlkjpmMhi1HgBCkhIQEzZsyAn58funTpAjc3N6Snp2P9+vWYMWMG6tevb4w4rReXFzFvnFGbSH6a7rXmzYGiRQ177oAA4L33xPZHH4m5lsgiFChB6tChAypXroyTJ08iKioKN2/exDeaVY7JOFh/ZN6yF2r//bfc0RBZJ033mrGG40+dKgbQnDwJLFpknGuQyRUoQdq6dSsGDhyIyZMno127drAtzDwS9GopKcD+/WKbCZJ5srMThdoAu9mI5HDzJqAp/ejY0TjXKFECmDZNbI8fD2Rbs5TMV4ESpD/++AMpKSmoW7cuAgMDMX/+fE4OaUya5UUqVAAqVpQ7GnpdrEMiks+GDeJjw4aAt7fxrjN4MFCjBvDgAYf8W4gCJUgNGzbE999/j1u3buG9997D6tWr4e3tjaysLMTGxiIlJcVYcVondq9ZBi45QiQfTf2RsWe7trMTBdsA8N13wPHjxr0eGd1rjWJzcXHBgAED8Mcff+DUqVP4+OOPMWPGDJQuXRodjdWEaY2YIFmG7AkSC7WJTCcpCdi5U2ybYjmQpk2Brl3FzznXaTN7hZ4HqXLlypg5cyb+/fdfrF69+rWWGlmwYAHKly8PR0dHBAYG4tChQy89ft26dfD394ejoyNq1KiBLVu26DwvSRIiIyPh5eUFJycnhISE4OLFizrHlC9fHiqVSucxY8aMAsduNFevAufPi/WCmjWTOxoqDE2hdkoKC7WJTGnzZjGqrGpV003UOmuWWHZk714x9J/MVoFm0h4wYMArjylRokSBAlizZg0iIiKwcOFCBAYGIioqCqGhoTh//jxKly6d6/j9+/ejR48emD59Otq3b4/o6GiEhYXh6NGjqF69OgBg5syZmDdvHpYvXw5fX19MmDABoaGhOHPmDByzzaA6ZcoUDBo0SPu5q6trgWI3Ks3kkIGBhh+WSqZlZyeGAh84IFqRKlWSOyIi62Cq7rXsypYVy5BERor12jp0ELNuk9kpUAvSsmXLsHPnTiQlJeHBgwd5PpKSkgoUwJw5czBo0CD0798fVatWxcKFC+Hs7IwlS5bkefzcuXPRunVrjBo1ClWqVMHUqVNRp04dzJ8/H4BoPYqKisL48ePRqVMn1KxZEytWrMDNmzexXjPU8zlXV1d4enpqHy5K+iZm95pl4YzaRKb1+DEQEyO2TZkgASIxKl8euHEDUFLPBBVIgVqQPvjgA/z444+4fPky+vfvj169eqF48eKvffGnT5/iyJEjGDt2rHafjY0NQkJCEB8fn+dr4uPjERERobMvNDRUm/xcvnwZCQkJCAkJ0T7v7u6OwMBAxMfHo3v37tr9M2bMwNSpU1G2bFn07NkTI0eOhJ1d3l+S9PR0pKenaz9PTk4GAGRkZCAjI6Ngb/xVMjNht2MHVACeNW8OydDnNwHN18TgXxszpQoIgB2ArD//RKZMXxPeE2Xh/TAuVUwM7FJTIfn44FmNGmJE8EsY9H7Y2UE1cybsunaFNGsWnvXqJUYjk96M+fOh7zkLlCAtWLAAc+bMwS+//IIlS5Zg7NixaNeuHQYOHIhWrVoVuP7o7t27yMzMhIeHh85+Dw8PnDt3Ls/XJCQk5Hl8QkKC9nnNvvyOAYCPPvoIderUQfHixbF//36MHTsWt27dwpw5c/K87vTp0zF58uRc+7dv3w5nZ+dXvNOCKXrhApo8eIAMZ2dsvXMHUo4aK3MSq+kqtHKuqaloDiDz8GFs2bRJLE0gE94TZeH9MI6ABQtQDsDlmjVxautWvV9nsPthb4+gWrVQ+sQJ3O3bF4eyNQSQ/ozx85GWlqbXcQVKkABArVajR48e6NGjB65evYply5bhww8/xLNnz/DXX3+hSJEiBQ5WDtlboWrWrAkHBwe89957mD59OtRqda7jx44dq/Oa5ORk+Pj4oFWrVnBzczNobDbPh4fatmqFNh06GPTcppKRkYHY2Fi0bNkS9lwiBXj2DNLYsbB//BhtK1WSpQ6J90RZeD+M6Nkz2A0cCAAoO3w4fJo2feVLjHI/fH0h1a0Lr4MH0c7eHlLLloY5rxUw5s+HpgfoVQqcIGVnY2MDlUoFSZKQmZlZ4NeXLFkStra2SExM1NmfmJgIT0/PPF/j6en50uM1HxMTE+Hl5aVzTEBAQL6xBAYG4tmzZ7hy5QoqV66c63m1Wp1n4mRvb2/4X25xcQAAm9atYWPmvziN8vUxR/b2YkbtAwdgf+IEUK2ajKHwnigJ74cR7N8vZrMuXhx2zZqJgRJ6Muj9qFULGDYMiIqC3ccfi6VIeK8LxBg/H/qer8Dt/Onp6fjxxx/RsmVLVKpUCadOncL8+fNx7dq1ArceOTg4oG7duoh7nhAAQFZWFuLi4hAUFJTna4KCgnSOB0QTnOZ4X19feHp66hyTnJyMgwcP5ntOADh+/DhsbGzyHDlnUsnJgKb+igXaloUzahOZhmb0WocOBUqOjGLiRKBUKeDcOeD5YCIyDwX6zvnwww+xevVq+Pj4YMCAAfjxxx9RsmTJQgUQERGBvn37ol69emjQoAGioqKQmpqK/v37AwD69OmDMmXKYPr06QCA4cOHo0mTJpg9ezbatWuH1atX488//8Si5wsEqlQqjBgxAtOmTYOfn592mL+3tzfCwsIAiELvgwcPolmzZnB1dUV8fDxGjhyJXr16oVixYoV6P4W2a5eYt+PNNwFfX3ljIcPijNpExidJ8gzvz0/RosAXXwCDBgGTJgE9ewI5amRJmQqUIC1cuBBly5ZFhQoVsHv3buzevTvP43755Re9z9mtWzfcuXMHkZGRSEhIQEBAAGJiYrRF1teuXYNNtoLW4OBgREdHY/z48Rg3bhz8/Pywfv167RxIADB69GikpqZi8ODBSEpKwltvvYWYmBjtHEhqtRqrV6/GpEmTkJ6eDl9fX4wcOTLX6DhZcHi/5dIkSEePipl2ZSzUJrJYx44B164Bzs7K+T3avz+wcKH452jcOGDxYrkjIj0UKEHq06fPa82U/SpDhw7F0KFD83xu165dufZ16dIFXbp0yfd8KpUKU6ZMwZQpU/J8vk6dOjigWd1ZaTTT4ivlB5sMp0oVMcNuSgpw8SKQR60bERWSZr670FDx86YEtrZinbZGjYAlS4D33wfq15c7KnqFAiVIy5YtM1IYpHXwILBnD/D223JHQoammVE7Pl78J8kEicjwlNS9ll1wMNC7N7BypVinbd8+tiIrHO+O0hQpArRtCyhp2RMyHNYhERnP338Dp0+Lf0bat5c7mtxmzBC/4w8cAH74Qe5o6BWYIBGZEpccITIeTfda06aA3ANu8uLtDUyYILY//VSMWibFYoJEZEqaof7HjolCbSIyHE332vMRy4o0fDjg5wckJADTpskdDb0EEyQiU/L31y3UJiLDSEh4MYeckhMktRqIihLbUVHA+fNyRkMvwQSJyJQ0hdoAu9mIDGnDBjEHUoMGQJkyckfzcm3bAu3aiQV0R4wQcZPiMEEiMjUWahMZnqb+SGmj1/Lz9ddi2ZGYGGDzZrmjoTwwQSIyNS45QmRYDx9q17BUdPdadn5+wMiRYnvECCA9XdZwKDcmSESmlnNGbSIqnC1bRHeVv794mIvx4wFPT+DSJdGiRIrCBInI1DSF2o8eARcuyB0NkflT6uSQr+LqCsycKbanTQNu3JA3HtLBBInI1OzsgNq1xTa72YgK58kTYOtWsW0u3WvZhYcDQUFAaiowZozc0VA2TJCI5MBCbSLDiIsTrbFlyryo7zMnNjZinTaVSsyuvW+f3BHRc0yQiOTAGbWJDCP75JDmurZZvXrAwIFi+6OPgMxMeeMhAEyQiOShSZA4ozbR68vMBDZuFNvmVn+U0+efA+7uYvDGkiVyR0NggkQkD39/wNmZhdpEhbF/P3Dnjlh3rXFjuaMpnNKlgcmTxfa4ccCDB/LGQ0yQiGSRfUZt1iEVTFwccOWK3FGQEmi619q3F5MumrsPPwSqVgXu3gUmTZI7GqvHBIlILqxDKrhffwVCQoAqVYCvvmKthjWTJPMd3p8fe3tg7lyxvWABcPq0vPFYOSZIRHLhjNoF9/334uOTJ8CoUUBwMPDXX/LGRPI4eVK0JDo5AaGhckdjOCEhIuHLzASGD+c6bTJigkQkl+yF2mwJebVbt4Bt28T2lCmioPXQITGn1NSpYiZlsh6a1qNWrUQ9nyWZPRtQq4Hffwd++UXuaKwWEyQiubBQu2BWrRIj/oKCgAkTRMtRhw4iMYqMBOrXFyOAyDpYWvdadr6+wOjRYvvjj4HHj+WNx0oxQSKSi60tC7X1JUnA8uViu08f8bFMGWDDBiA6GihRAjhxAmjQAPjsM9EFR5brn39EF5utrSjQtkRjxgA+PsDVq8CsWXJHY5WYIBHJiXVI+jl+XBSsqtVAt24v9qtUQI8ewJkzQNeuoqvyiy+AOnWAAwdkC5eMbP168bFxY5EcWyJnZzEQAQCmTxeJEpkUEyQiOXHJEf1oWo86dhRz3uRUujSwZg3w88+Ahwdw9qwo4P74YyAtzbSxkvFZcvdadl26AE2avBiUQCbFBIlITpoE6ehRFmrnJyNDdKMBQN++Lz/2P/8RrUl9+ohuuTlzgJo1gV27jB4mmUhi4ov1ysxxcdqCUKnEOm02NsC6dcDOnXJHZFWYIBHJSVOonZrKQu38bN0qZksuXVq/4dzFi4sWp82bgTfeAC5dApo1E5PwpaQYP14yrt9+E8lvvXqiRsfS1awJfPCB2P7oI+DZM3njsSJMkIjkZGsrhqkD7GbLz4oV4mN4uJiBXF9t24qRbu+9Jz7/9lugevUXUwWQecq+OK21mDJFJP6nTwMLF8odjdVggkQkN86onb/790WLAfDq7rW8uLmJPyg7doih09euAa1bAwMGcK0rc5ScLO4lYPn1R9kVLy4WswXEFBd378obj5VggkQkNxZq52/1auDpU6BWLfF4XS1aAKdOiZmJVSpg6VKgWjUxTQCZj61bxfdDpUpiuRlrMmiQ+BlISgLGj5c7GqvABIlIbpqh/pxROzfN6LXXaT3KycUFiIoC9u4Vf2Bv3RLdND16iBonUj7N8P533hGJrjWxtRUF2wCwaJH4fUFGxQSJSG6VK4s/3izU1nXunFhKxNYW6NnTcOdt1EjMq/Tpp2J00OrVYgX1NWu47pWSpaeLwnvAuuqPsmvcGOjeXXyfDhvG71cjY4JEJLfsM2qzDukFTXF269ZibiNDcnICZswADh4EatQQNR3du4tpAm7dMuy1yDB+/12MQvTyEjOmW6uZM8XI1337RHJPRsMEiUgJOKO2rsxMYOVKsW2I7rX81KsnktKJE8UIufXrRWvS8uX871xpNN1rYWGi5c9a+fgA48aJ7VGjxFqOZBRW/F1GpCAs1Na1cyfw779A0aJiQVpjcnAAJk0SX/u6dUURbL9+YpqAa9eMe23ST2bmi4J6a+1ey+7jj8WozBs3xDIkZBRMkIiUgDNq69IUZ3fvDjg6muaaNWuK9dtmzBBrvsXEiHmTvvsOyMoyTQyUtwMHxAza7u5A06ZyRyM/R0fg66/F9ldficlQyeCYIBEpgaZQOy0NOH9e7mjklZIC/PKL2DZm91pe7OxE8fbx40BQkIjl/feBkBD+EZKTZnLI9u1Fix+JdQlbtRLTHkREyB2NRWKCRKQEnFH7hZ9/Fominx8QGChPDP7+YjqAqChRELtzpyjmjopiC5+pSZLu8H4SVCrx/WhnB2zcKFo8yaCYIBEpBeuQhOxzH8k5142trZhY8tQpsZbb48fAyJHA22+LKQjINE6fFq13arV+a/FZkypVxPpsADBihGhNIoNhgkSkFFxyBLhyBdi1SyRGvXvLHY1QoYJY3mLhQsDVFYiPF9MyzJjBhUNNQdO91qoVUKSIvLEoUWSkWMj5/Hngm2/kjsaiMEEiUgrOqP1iaH+zZkDZsvLGkp2NjVj09q+/gDZtxKSFY8cCDRsCJ0/KHZ1l0yRI7F7Lm7v7i5FskycDCQnyxmNBmCARKUWlStZdqC1JLyaHNHVxtr58fMRszsuXiykINFMDTJzI7g1juHJFFMzb2Bh/ugdz1q+f+AcrJUUk7mQQTJCIlCJ7obY1drPFxwN//y2SxP/8R+5o8qdSAX36AGfOiFaNZ8+AKVNEomSN982YNMXZb78NlCwpayiKZmPzontt2TKxRA8VGhMkIiWx5kJtTXF2587mUWvi5SVG3K1ZA5QqJYqJAwPFNAGPH8sdnWVg95r+GjZ80fI6dChbNA2ACRKRkljrkiOPH4tEA1Bu91peVCqga1dRm9Sjh5hQcuZMUcS9b5/c0Zm3O3eAP/4Q25w9Wz8zZoiBBIcPiy7JlBS5IzJrTJCIlETTgmRthdobNwIPH4rCbHOcKblUKSA6WiyH4eUFXLgguoWGD+daWa/rt99Ewlm7NlCunNzRmAdPT2DdOtFNvX070KQJi7YLgQkSkZJkL9S2prl2NMXZvXub90KkHTuK2qQBA0TR+bx5YoLJuDi5IzM/7F57PaGhYmLTUqXEP1pBQSJhpwIz499ERBbI1haoU0dsW0s3W0ICsG2b2FbK3EeFUbQosHixeE9ly4qRWCEhYpqAhw/ljs48pKQAsbFimwlSwdWvLwY9VKwovv+Cg8V6dlQgTJCIlMbaCrVXrRLdiQ0bijXpLEWrVqJw+8MPxeeLFgHVqolpAujltm0Tc029+ab4mlHBVawI7N8v6hrv3QOaNxfdlqQ3JkhESmNNM2pLku7SIpbG1RVYsEDMDl6xInDjhlhwtU8f4P59uaNTLk33WliYvMvNmLvSpUV3W5s2YiBEWBjw/fdyR2U2mCARKY1mJNvx45ZfqH38uFjrTK0GunWTOxrjadJEzLgdESH+4K9cCbtatVDaWloJC+Lp0xetbOxeK7wiRcTggQEDRNH74MHApEninxN6KSZIREpTqZL4pWYNhdqa4uyOHYFixeSNxdicnYHZs0W3R5UqUCUmosGXXwInTsgdmbLs3ClqtTw9RbcrFZ69PfB//wdMmCA+nzwZGDSIawm+AhMkIqWxsbGOGbUzMkT9ESC6nKxFw4bAsWPIat0atk+fwq5HDyA5We6olEMze3anTuY9olFpVCox4/vCheLrunix6HJLTZU7MsXidx+REllDoXZMjJgMsHRpMTTZmqjVyFy6FGklS0L199/iv3l2eYguoA0bxDYnhzSO994DfvkFcHQUXZnNm4ufQ8pFEQnSggULUL58eTg6OiIwMBCHXrGOzLp16+Dv7w9HR0fUqFEDW7Zs0XlekiRERkbCy8sLTk5OCAkJwcWLF/M8V3p6OgICAqBSqXD8+HFDvSWiwrGGGbU1xdnh4aILwNqUKIE/P/kEkp0dsHYt8O23ckckv4MHgVu3ADc38YebjKNTJ+D334HixcW6bY0aAf/8I3dUiiN7grRmzRpERERg4sSJOHr0KGrVqoXQ0FDcvn07z+P379+PHj16YODAgTh27BjCwsIQFhaG06dPa4+ZOXMm5s2bh4ULF+LgwYNwcXFBaGgonjx5kut8o0ePhre3t9HeH9Fr0bQgHT9umXUC9++/GHJsiaPX9PTA3x9ZX3whPhk50rITYn1outfatQMcHGQNxeIFBYl6uHLlgIsXxVxJR4/KHZWiyJ4gzZkzB4MGDUL//v1RtWpVLFy4EM7OzliyZEmex8+dOxetW7fGqFGjUKVKFUydOhV16tTB/PnzAYjWo6ioKIwfPx6dOnVCzZo1sWLFCty8eRPrNT98z23duhXbt2/HV199Zey3SVQwll6ovWaNGK1Uq5Z4WLGs4cPFf/RPnwJdugBJSXKHJA9J0h3eT8ZXubKYUDIgAEhMFKMtt2+XOyrFkDVBevr0KY4cOYKQkBDtPhsbG4SEhCA+Pj7P18THx+scDwChoaHa4y9fvoyEhASdY9zd3REYGKhzzsTERAwaNAgrV66Es7OzId8WUeHZ2Fj2jNqWPPdRQalUwNKlQPnywOXLL5YpsTZnzoiWDLVazNtDpuHlBezeDbRoIdYNbNfuxehSK2cn58Xv3r2LzMxMeHh46Oz38PDAuXz+a05ISMjz+ITnC/JpPr7sGEmS0K9fP7z//vuoV68erly58spY09PTkZ6erv08+fmok4yMDGRkZLzy9dZG8zXh1+b12dSuDds9e5B5+DCyevYs9PkUc0/On4f9wYOQbG3xrEsXMZrNCuncjyJFoIqOhm2TJlD9+isy58xB1kcfyRyhadn89BNsAWS1aIFMR0eTf18o5udDDk5OwIYNsH33XdisXg307YvM69eRNWqUbBN1GvN+6HtOWRMkuXzzzTdISUnB2LFj9X7N9OnTMXny5Fz7t2/fzhaol4jVrKdEBfaGjQ3qAni4Ywf25hiIUBhy35MqK1eiEoDE2rVx0BJbxwoo+/3w7dcPNb//HqpPP0V8ZiYeWNLSK6/QZOVKFAVwwtcX1wz4/V5Qcv98yKprV1R98gR+69fDdvx4XN2/H6cGDhRrRMrEGPcjLS1Nr+NkTZBKliwJW1tbJCYm6uxPTEyEp6dnnq/x9PR86fGaj4mJifDy8tI5JiAgAADw+++/Iz4+Hmq1Wuc89erVQ3h4OJZrmv+zGTt2LCIiIrSfJycnw8fHB61atYKbm5ue79h6ZGRkIDY2Fi1btoS9NY5QMoQKFYCvv0axa9fQtlUrwK5wP66KuCeZmbAbOhQAUDIiAm3btpUnDgXI8360aYOs+/dh8/PPeHvBAjw7eBAoUULeQE3h2jXYX7oEycYG1ceORfXSpU0egiJ+PpSgfXtkfvMNbD75BBW2bEF5BwdkLl8uWplMyJj3I1nPecdkTZAcHBxQt25dxMXFIex5UV5WVhbi4uIw9Pkv0ZyCgoIQFxeHESNGaPfFxsYiKCgIAODr6wtPT0/ExcVpE6Lk5GQcPHgQH3zwAQBg3rx5mDZtmvb1N2/eRGhoKNasWYPAwMA8r6tWq3MlVABgb29v3T9Mr8CvTyFUqya6Xh49gv2lS0D16gY5raz3ZM8e4N9/gaJFYffOO9Y5vD+HXPdjyRLgxAmo/v4b9u++C2zcaPkTJj5fWkTVqBHsy5SRNRT+zoJYEueNN4DevWGzfj1s2rUT81MVL27yUIxxP/Q9n+w/dREREfj++++xfPlynD17Fh988AFSU1PRv39/AECfPn10usKGDx+OmJgYzJ49G+fOncOkSZPw559/ahMqlUqFESNGYNq0adi4cSNOnTqFPn36wNvbW5uElS1bFtWrV9c+KlWqBACoWLEi3njjDdN+AYjyY4mF2prW2e7dxUR1lJubG7BunShW3rwZsIZRtprRa1x7TTm6dhUj2tzdgT/+AN56C7h2Te6oTEr2BKlbt2746quvEBkZiYCAABw/fhwxMTHaIutr167h1q1b2uODg4MRHR2NRYsWoVatWvjpp5+wfv16VM/23/Xo0aMxbNgwDB48GPXr18ejR48QExMDR/5CJnOjmQ/JEpYcSUkBfv5ZbFvT0iKvIyAAmDdPbI8bB+zdK2s4RnXvnmhZBDi8X2maNBHJUZkywNmzYu6kkyfljspkFFGkPXTo0Hy71Hbt2pVrX5cuXdClS5d8z6dSqTBlyhRMmTJFr+uXL18ekjUOqyXls6QZtX/+Wczr5OfHRUj1MWiQSBxWrRItbseOiWVZLM1vv4klRmrVAnx95Y6GcqpeXcyV1KYN8NdfwNtviwk9mzWTOzKjk70FiYhewpJm1M4+95FMQ4fNikolFhb19wdu3gR69wYyM+WOyvDYvaZ8Pj6iFbNxY7GwcuvWYrJXC8cEiUjJ/PwAV1fg8WPRxG2url4Fdu0Sf/R795Y7GvNRpIioR3JyEvUgmmVJLEVq6ouZm5kgKVuxYsC2bcB//ytmfe/eHfj6a7mjMiomSERKZmMD1K4tts25m23lSvGxWTOgbFl5YzE31asD//uf2J40SSwyaim2bQOePBFdazVqyB0NvYqjI7B6NTBsmPg8IgL4+GPRRWqBmCARKZ251yFJ0oulC1ic/Xr69QP69xd/iHr2BJ6vCmD2snevsdvVPNjaAnPnAjNnis/nzAHCw4FsK01YCiZIREqnqUMy1wQpPl6sseXiAnTuLHc05mv+fNGalJgI9Ohh/vVIGRnApk1im91r5kWlAkaNEi3DdnaiValNG+DhQ7kjMygmSERKZ+6F2pri7M6dRU0NvR5nZ1GP5OIi6rkmTZI7osLZvRtIShIj855P9EtmplcvYMsW8XO9c6co4r55U+6oDIYJEpHSmXOh9pMnL0a79O0rbyyWwN8fWLRIbH/+uajhMVea7rWOHWVd64sKqWVLMR2Fp6eYIykoyPx+T+WDCRKR0pnzjNobN4pmdx8foGlTuaOxDD17Au+9J2q7evUSS7eYm6wsMZcOwO41S1C7NrB/P1Cpkphtu1EjMcGkmWOCRGQOzHVGbU33Wu/elr+emClFRYnZtu/eFfVIGRlyR1Qwf/4pumJcXYEWLeSOhgzB1xfYt09MAvvggWhZ0rQSmin+xiIyB+ZYqJ2Q8KILiKPXDMvRUdQjubqK/9THj5c7ooLR/OFs21asOUeWoWRJIC5OdJs+eSLqDjVTVJghJkhE5kAz1P/ECfMp1F61Soy0atgQqFxZ7mgsz5tvAkuWiO2ZM1+MCDMHmgSJa69ZHmdnsazQ4MGiG3jIELGeoBku58UEicgcvPmm+RVqa+Y+YnG28fz3vy8m7evTR8xYrnRnzwLnzwMODqIFiSyPnZ1YJkezHur06WIuLzPrCmaCRGQOshdqm0Md0vHjYkSLgwPQrZvc0Vi2WbOA+vVF3UfXrmIZCCXTFGe3aAG4uckaChmRSgVMmAAsXixGKa5YAXToAKSkyB2Z3pggEZkLc5pRW1Oc3bGjWMOJjEetBtauBYoWBQ4dAj79VO6IXo7da9ZlwAAxmtXZWdQkNmsmJjs1A0yQiMyFuRRqZ2SI+iOA3WumUr78i6Q0Kgr45Rc5o8nfv/8Chw+L1oVOneSOhkylbVsxkWTJkuL3V1CQmF1f4ZggEZkLc5lROyYGuHNHzJAcGip3NNajY0fgk0/E9oABwKVL8saTF033WnAw4OEhayhkYg0aiLmSKlQALl8W3wOHDskd1UsxQSIyF5pC7SdPgDNn5I4mf5ri7PBwwN5e3liszRdfiD88Dx+KeqQnT+SOSBcnh7Rufn4iSapbV8zh1awZsHmz3FHliwkSkbmwsVF+N9v9+6LeAGD3mhzs7cXCoSVKAEePAh9/LHdEL9y/L9aQA1h/ZM08PMT3QWgokJYmuloXL5Y7qjwxQSIyJ0pPkNasEaOoatYEatWSOxrr5OMjVlkHxCR9q1fLG4/Gpk1iXqwaNYCKFeWOhuRUpAjw22/in6jMTODdd8WUAAqbK4kJEpE5UfqSI5pCYbYeyatNGzE5HwAMGiTmHZIbu9coO3t7YOnSF9+nEycC77+vqPpKJkhE5kTJM2qfPw8cPCjmPAkPlzsamjwZaNIEePQI6NJFTDIql7Q0UbwPsHuNXlCpgM8/BxYsENuLFgH/+Y/4flEAJkhE5qRiRTG5nhILtTXF2a1bc4SSEtjZAdHRYjThqVMvZtyWw/btIkErV04sskuU3YcfiuVJHB1F11uLFqKIW2ZMkIjMiVJn1M7KelH3wu415fD2FkmSSiUKYTVJrKlpJod85x0RC1FO77wD7NghJpY9cAB2TZrAWeYJJZkgEZkbJRZq79wJXL8uZnPu0EHuaCi7Fi1EfQcAfPCB6Vsenz0TrQIA64/o5Ro1AvbtA8qWheriRbw9Zgxw7Jhs4TBBIjI3SlxyRFOc3a2baCYnZRk/HggJEbUd//0vkJpqumvv2SPWiStZUvwBJHqZKlWA+HhINWpA/fAhVNevyxYKEyQic6NpQTpxQhmrY6ekiPoBgN1rSmVrK5Z/8fICzp4VLUmmGlKt6V7r2FHEQfQq3t549vvvODRmDKSOHWULgwkSkblRWqH2L7+Ilgk/P6BhQ7mjofyULi3mRLKxEfViS5YY/5qSxOH99Hrc3ZHQoIGsITBBIjI3SptRO/vcRyzAVbbGjYFp08T20KHAyZPGvd6RI2KBWhcX0cVHZEaYIBGZI6UkSFevigJtAOjVS95YSD+ffiomknzyRNQjJScb71qa7rU2bVibRmaHCRKROVLKjNqaof3Nmok5bkj5bGzEcP833gAuXgQGDzZePVL24f1EZoYJEpE5UkKhtiS9mFeHxdnmpWRJsW6enZ34+O23hr/G+fOiINzeHmjXzvDnJzIyJkhE5qhiRcDdHUhPl69Q+8AB0QLh4gJ07ixPDPT6goOBL78U2yNHGr67VlOc3ayZ+F4lMjNMkIjMUfYZteWqQ9IUZ3fuLFbnJvMzciTQqRPw9KlYry0pyXDnZvcamTkmSETmSs46pCdPRNcMAPTpY/rrk2GoVGJF9fLlgcuXgQEDDFOPdOOGWLhYpRIJGJEZYoJEZK7knFF740bR2uDjI7pQyHwVKwasXStqhX79FZg7t/Dn3LhRfGzYUExOSWSGmCARmSs5C7U13Wu9e4vuPjJv9esDc+aI7VGjRH1ZYWi618LCCnceIhnxNxuRucpeqP3XX6a7bkICsG2b2Gb3muUYMkTUIT17JtbUu3//9c7z4MGLubFYf0RmjAkSkblSqeQp1I6OBjIzRfdJ5cqmuy4Zl0oFfP898OabwLVrYuqGrKyCn2fLFpFkVasmlp8hMlNMkIjMmRx1SJruNbYeWR53d2DdOkCtBjZtAr76quDn4Og1shBMkIjMmamXHDl+XKzf5eAgumHI8gQEAPPmie1x44A//tD/tY8fA1u3im3WH5GZY4JEZM5MXaitaT3q2BEoXtz41yN5DBoEhIeLrtRu3YA7d/R7XWwskJYGlC37ovuXyEwxQSIyZ6Ys1M7IEPVHAJcWsXQqFbBwIeDvD9y8KRYi1qceSTN7dliYOAeRGWOCRGTOVCrTdbNt2wbcvg2ULg2Ehhr3WiS/IkVEPZKTE7B9O/DFFy8//tmzF/MfsXuNLAATJCJzZ6oZtTXdaz17ikkFyfJVrw78739ie+LEF8P38/LHH8C9e0CJEsDbb5smPiIjYoJEZO5M0YJ0//6L1gF2r1mXfv2A/v1FF1uPHmIerLxoRq916ADY2ZksPCJjYYJEZO40Q/1PnjReofaaNWJB05o1xSgnsi7z54vWpMRE0YKYman7vCS9qD/i8H6yEEyQiMxdhQpA0aLGLdResUJ8ZOuRdXJ2FvVILi6im23yZN3njx0Tk0s6OwMtW8oTI5GBMUEiMnfZZ9Q2Rh3S+fNibS5bWzH0m6yTvz+waJHYnjZNFG5raLrXWrcWRd1EFoAJEpElMOaM2prWo9atAQ8Pw5+fzEfPnsB774kutfBw4MYNsZ/da2SBmCARWQJjFWpnZQErV4ptLi1CABAVJerQ7t4FuncHzp0DTp8Whdnt2skdHZHBMEEisgTZZ9R++tRw5925E7h+XdQ4dexouPOS+XJ0FPVIrq5iaH/79mJ/06ZAsWKyhkZkSEyQiCyBplD76VPDFmprute6dRN/GIkA4M03gSVLxPalS+Iju9fIwjBBIrIExphR+9Ej4OefxTZHr1FO//0vMGzYi887dZIvFiIjUESCtGDBApQvXx6Ojo4IDAzEoUOHXnr8unXr4O/vD0dHR9SoUQNbtmzReV6SJERGRsLLywtOTk4ICQnBxYsXdY7p2LEjypYtC0dHR3h5eaF37964efOmwd8bkckYOkH6+WcgNRXw8wMaNjTMOcmyzJoFDB4sRrWVKSN3NEQGJXuCtGbNGkRERGDixIk4evQoatWqhdDQUNy+fTvP4/fv348ePXpg4MCBOHbsGMLCwhAWFobTp09rj5k5cybmzZuHhQsX4uDBg3BxcUFoaCiePHmiPaZZs2ZYu3Ytzp8/j59//hmXLl3Cf//7X6O/XyKjMfSSI5qlRfr04cKjlDe1GvjuO+Czz+SOhMjwJJk1aNBAGjJkiPbzzMxMydvbW5o+fXqex3ft2lVq166dzr7AwEDpvffekyRJkrKysiRPT09p1qxZ2ueTkpIktVot/fjjj/nGsWHDBkmlUklPnz7VK+6HDx9KAKSHDx/qdby1efr0qbR+/Xq9v55kAJcuSRIgSQ4OkpSenuvpAt2TK1fEuQCxTQbHnxFl4f1QFmPeD33/fsu6YM7Tp09x5MgRjB07VrvPxsYGISEhiI+Pz/M18fHxiIiI0NkXGhqK9c/n4bh8+TISEhIQEhKifd7d3R2BgYGIj49H9+7dc53z/v37WLVqFYKDg2GfzyKc6enpSE9P136enJwMAMjIyECGsZZ3MGOarwm/Nib0xhuwK1YMqgcPkHH8OFC7ts7TBbknNsuXwxZAVtOmyPT2Nt4SJlaMPyPKwvuhLMa8H/qeU9YE6e7du8jMzIRHjsnnPDw8cO7cuTxfk5CQkOfxCc8XUNR8fNkxGp9++inmz5+PtLQ0NGzYEJs2bco31unTp2Nyzun1AWzfvh3Ozs75vs7axcbGyh2CVQn28UGpBw/w1/LluHrrVp7HvPKeSBJafPcdigA4XrMmrueo8SPD4s+IsvB+KIsx7kdaWppex1n1ksujRo3CwIEDcfXqVUyePBl9+vTBpk2boMqj3mLs2LE6LVfJycnw8fFBq1at4ObmZsqwzUJGRgZiY2PRsmXLfFvlyPBs9u4FTp5EjYwMVGvbVuc5fe+J6sAB2N28CcnZGTUmTUKNIkWMHbZV4s+IsvB+KIsx74emB+hVZE2QSpYsCVtbWyQmJursT0xMhKenZ56v8fT0fOnxmo+JiYnw8vLSOSYgxyrkJUuWRMmSJVGpUiVUqVIFPj4+OHDgAIKCgnJdV61WQ61W59pvb2/PH6aX4NfHxAIDAQC2x47BNp+v+yvvyapVAABV586w58R/RsefEWXh/VAWY9wPfc8n6yg2BwcH1K1bF3Fxcdp9WVlZiIuLyzNJAYCgoCCd4wHRBKc53tfXF56enjrHJCcn4+DBg/meU3NdADp1RkRmRzOS7eTJ15tR+8kTYM0asc25j4jIisnexRYREYG+ffuiXr16aNCgAaKiopCamor+/fsDAPr06YMyZcpg+vTpAIDhw4ejSZMmmD17Ntq1a4fVq1fjzz//xKLnq0yrVCqMGDEC06ZNg5+fH3x9fTFhwgR4e3sjLCwMAHDw4EEcPnwYb731FooVK4ZLly5hwoQJqFix4kuTKCLF8/UVyz08eCDWx6pTp2Cv/+03ICkJ8PEBmjUzSohEROZA9gSpW7duuHPnDiIjI5GQkICAgADExMRoi6yvXbsGG5sXDV3BwcGIjo7G+PHjMW7cOPj5+WH9+vWoXr269pjRo0cjNTUVgwcPRlJSEt566y3ExMTA8flSCc7Ozvjll18wceJEpKamwsvLC61bt8b48ePz7EYjMhuaGbV37BATRhY0QdLMfdS7N2Aj+zRpRESykT1BAoChQ4di6NCheT63a9euXPu6dOmCLl265Hs+lUqFKVOmYMqUKXk+X6NGDfz++++vFSuR4mVPkAYN0v91iYlATIzY7tPHOLEREZkJ/otIZGled0btVauAzExR6F25suHjIiIyI0yQiCyNJkE6dapghdqa7jUWZxMRMUEisjiaQu2nT0Whtj5OnBAj3xwcgG7djBsfEZEZYIJEZGk0hdqA/t1smtajjh2B4sWNExcRkRlhgkRkiTQJ0pEjrz42I0M7OSS714iIBCZIRJaoXj3xUZ8Eads24PZtoFQpIDTUuHEREZkJJkhElij7jNqvmh1e070WHg5wiQUiIgBMkIgsU/nyolA7I+PlhdoPHgAbN4ptdq8REWkxQSKyRNkLtV/WzbZmjRjtVrMmkGMxZyIia8YEichS6VOHxLmPiIjyxASJyFK9aqj/+fPAgQOArS3Qs6fp4iIiMgNMkIgsVfYZtfMq1F6xQnwMDQU8PU0XFxGRGWCCRGSpypcXkz7mVaidlQWsXCm22b1GRJQLEyQiS/WyQu1du4Dr14GiRcXs2UREpIMJEpEly68OSVOc3a0b4Oho2piIiMwAEyQiS5bXSLZHj4CffxbbffqYPiYiIjPABInIkuVRqK365RcgNRXw8wOCgmQMjohIuZggEVmycuW0hdqq54XaNpqFafv0EXVKRESUCxMkIkuWrVBbdfQonO7cgWrXLvFc797yxUVEpHBMkIgs3fM6JNXRo/DZtQsqSQKaNROtS0RElCc7uQMgIiPTtCAdOQKf27fFPhZnExG9FBMkIkunSZCOH0cRAJKzM1SdO8sbExGRwrGLjcjSlSsHlCih/VR65x3A1VXGgIiIlI8JEpGlyz6jNoAsFmcTEb0SEyQia/A8QUorWRJS06byxkJEZAaYIBFZg969Ib35Js717AnY8MeeiOhV+JuSyBpUqYJnZ87gevPmckdCRGQWmCARERER5cAEiYiIiCgHJkhEREREOTBBIiIiIsqBCRIRERFRDkyQiIiIiHJggkRERESUAxMkIiIiohyYIBERERHlwASJiIiIKAcmSEREREQ5MEEiIiIiyoEJEhEREVEOTJCIiIiIcrCTOwBzJUkSACA5OVnmSJQpIyMDaWlpSE5Ohr29vdzhEHhPlIb3Q1l4P5TFmPdD83db83c8P0yQXlNKSgoAwMfHR+ZIiIiIqKBSUlLg7u6e7/Mq6VUpFOUpKysLN2/ehKurK1QqldzhKE5ycjJ8fHxw/fp1uLm5yR0OgfdEaXg/lIX3Q1mMeT8kSUJKSgq8vb1hY5N/pRFbkF6TjY0N3njjDbnDUDw3Nzf+slEY3hNl4f1QFt4PZTHW/XhZy5EGi7SJiIiIcmCCRERERJQDEyQyCrVajYkTJ0KtVssdCj3He6IsvB/KwvuhLEq4HyzSJiIiIsqBLUhEREREOTBBIiIiIsqBCRIRERFRDkyQiIiIiHJggkQGNX36dNSvXx+urq4oXbo0wsLCcP78ebnDoudmzJgBlUqFESNGyB2K1bpx4wZ69eqFEiVKwMnJCTVq1MCff/4pd1hWKzMzExMmTICvry+cnJxQsWJFTJ069ZXrdJFh7NmzBx06dIC3tzdUKhXWr1+v87wkSYiMjISXlxecnJwQEhKCixcvmiQ2JkhkULt378aQIUNw4MABxMbGIiMjA61atUJqaqrcoVm9w4cP47vvvkPNmjXlDsVqPXjwAI0aNYK9vT22bt2KM2fOYPbs2ShWrJjcoVmtL7/8Et9++y3mz5+Ps2fP4ssvv8TMmTPxzTffyB2aVUhNTUWtWrWwYMGCPJ+fOXMm5s2bh4ULF+LgwYNwcXFBaGgonjx5YvTYOMyfjOrOnTsoXbo0du/ejcaNG8sdjtV69OgR6tSpg//973+YNm0aAgICEBUVJXdYVmfMmDHYt28f9u7dK3co9Fz79u3h4eGBxYsXa/d17twZTk5O+OGHH2SMzPqoVCr8+uuvCAsLAyBaj7y9vfHxxx/jk08+AQA8fPgQHh4eWLZsGbp3727UeNiCREb18OFDAEDx4sVljsS6DRkyBO3atUNISIjcoVi1jRs3ol69eujSpQtKly6N2rVr4/vvv5c7LKsWHByMuLg4XLhwAQBw4sQJ/PHHH2jTpo3MkdHly5eRkJCg83vL3d0dgYGBiI+PN/r1uVgtGU1WVhZGjBiBRo0aoXr16nKHY7VWr16No0eP4vDhw3KHYvX++ecffPvtt4iIiMC4ceNw+PBhfPTRR3BwcEDfvn3lDs8qjRkzBsnJyfD394etrS0yMzPx+eefIzw8XO7QrF5CQgIAwMPDQ2e/h4eH9jljYoJERjNkyBCcPn0af/zxh9yhWK3r169j+PDhiI2NhaOjo9zhWL2srCzUq1cPX3zxBQCgdu3aOH36NBYuXMgESSZr167FqlWrEB0djWrVquH48eMYMWIEvL29eU+sHLvYyCiGDh2KTZs2YefOnXjjjTfkDsdqHTlyBLdv30adOnVgZ2cHOzs77N69G/PmzYOdnR0yMzPlDtGqeHl5oWrVqjr7qlSpgmvXrskUEY0aNQpjxoxB9+7dUaNGDfTu3RsjR47E9OnT5Q7N6nl6egIAEhMTdfYnJiZqnzMmJkhkUJIkYejQofj111/x+++/w9fXV+6QrFqLFi1w6tQpHD9+XPuoV68ewsPDcfz4cdja2sodolVp1KhRrmkvLly4gHLlyskUEaWlpcHGRvdPoa2tLbKysmSKiDR8fX3h6emJuLg47b7k5GQcPHgQQUFBRr8+u9jIoIYMGYLo6Ghs2LABrq6u2n5id3d3ODk5yRyd9XF1dc1V/+Xi4oISJUqwLkwGI0eORHBwML744gt07doVhw4dwqJFi7Bo0SK5Q7NaHTp0wOeff46yZcuiWrVqOHbsGObMmYMBAwbIHZpVePToEf7++2/t55cvX8bx48dRvHhxlC1bFiNGjMC0adPg5+cHX19fTJgwAd7e3tqRbkYlERkQgDwfS5culTs0eq5JkybS8OHD5Q7Dav32229S9erVJbVaLfn7+0uLFi2SOySrlpycLA0fPlwqW7as5OjoKFWoUEH67LPPpPT0dLlDswo7d+7M829G3759JUmSpKysLGnChAmSh4eHpFarpRYtWkjnz583SWycB4mIiIgoB9YgEREREeXABImIiIgoByZIRERERDkwQSIiIiLKgQkSERERUQ5MkIiIiIhyYIJERERElAMTJCKi16RSqbB+/Xq5wyAiI2CCRERmqV+/flCpVLkerVu3ljs0IrIAXIuNiMxW69atsXTpUp19arVapmiIyJKwBYmIzJZarYanp6fOo1ixYgBE99e3336LNm3awMnJCRUqVMBPP/2k8/pTp06hefPmcHJyQokSJTB48GA8evRI55glS5agWrVqUKvV8PLywtChQ3Wev3v3Lt555x04OzvDz88PGzdu1D734MEDhIeHo1SpUnBycoKfn1+uhI6IlIkJEhFZrAkTJqBz5844ceIEwsPD0b17d5w9exYAkJqaitDQUBQrVgyHDx/GunXrsGPHDp0E6Ntvv8WQIUMwePBgnDp1Chs3bsSbb76pc43Jkyeja9euOHnyJNq2bYvw8HDcv39fe/0zZ85g69atOHv2LL799luULFnSdF8AInp9JlkSl4jIwPr27SvZ2tpKLi4uOo/PP/9ckiRJAiC9//77Oq8JDAyUPvjgA0mSJGnRokVSsWLFpEePHmmf37x5s2RjYyMlJCRIkiRJ3t7e0meffZZvDACk8ePHaz9/9OiRBEDaunWrJEmS1KFDB6l///6GecNEZFKsQSIis9WsWTN8++23OvuKFy+u3Q4KCtJ5LigoCMePHwcAnD17FrVq1YKLi4v2+UaNGiErKwvnz5+HSqXCzZs30aJFi5fGULNmTe22i4sL3NzccPv2bQDABx98gM6dO+Po0aNo1aoVwsLCEBwc/FrvlYhMiwkSEZktFxeXXF1ehuLk5KTXcfb29jqfq1QqZGVlAQDatGmDq1evYsuWLYiNjUWLFi0wZMgQfPXVVwaPl4gMizVIRGSxDhw4kOvzKlWqAACqVKmCEydOIDU1Vfv8vn37YGNjg8qVK8PV1RXly5dHXFxcoWIoVaoU+vbtix9++AFRUVFYtGhRoc5HRKbBFiQiMlvp6elISEjQ2WdnZ6cthF63bh3q1auHt956C6tWrcKhQ4ewePFiAEB4eDgmTpyIvn37YtKkSbhz5w6GDRuG3r17w8PDAwAwadIkvP/++yhdujTatGmDlJQU7Nu3D8OGDdMrvsjISNStWxfVqlVDeno6Nm3apE3QiEjZmCARkdmKiYmBl5eXzr7KlSvj3LlzAMQIs9WrV+PDDz+El5cXfvzxR1StWhUA4OzsjG3btmH48OGoX78+nJ2d0blzZ8yZM0d7rr59++LJkyf4+uuv8cknn6BkyZL473//q3d8Dg4OGDt2LK5cuQInJye8/fbbWL16tQHeOREZm0qSJEnuIIiIDE2lUuHXX39FWFiY3KEQkRliDRIRERFRDkyQiIiIiHJgDRIRWSRWDxBRYbAFiYiIiCgHJkhEREREOTBBIiIiIsqBCRIRERFRDkyQiIiIiHJggkRERESUAxMkIiIiohyYIBERERHlwASJiIiIKIf/B3tsLQHuOlvNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the validation accuracy from the training history\n",
        "val_acc = history.history['val_mse']\n",
        "train_acc =history.history['loss']\n",
        "\n",
        "epochs = range(1, len(val_acc) + 1)\n",
        "\n",
        "# Plotting the validation accuracy\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation MAE', color = 'red')\n",
        "#plt.plot(epochs, train_acc, 'b', label='Training MAE')\n",
        "plt.title('Validation MSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the plot as an image file (e.g., PNG format)\n",
        "plt.savefig('validation_mae_plot_single_beam_no_dom_res2.png')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnq3KFaMCc8l"
      },
      "source": [
        "##SAVE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eBB1aHsnW5T"
      },
      "outputs": [],
      "source": [
        "model_4.save(\"single_beam_no_dom_res2.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FqM_Z0kCt4D"
      },
      "source": [
        "##EXCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rconXHLlNKUu",
        "outputId": "7a8de32c-4ac8-440e-eb8c-ff06dd9f5e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 37s 20ms/step - loss: 14863203.0000 - mse: 14863205.0000\n",
            "Training MSE from Keras: [14863203.0, 14863205.0]\n"
          ]
        }
      ],
      "source": [
        "training_mse = model_shufflenet.evaluate(dataset[0])\n",
        "print(\"Training MSE from Keras:\", training_mse)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= get_data_h5(\"/content/drive/MyDrive/OpticsML/test_2.h5\",batch_size= 64)"
      ],
      "metadata": {
        "id": "CDUMSy6Hc94t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC-rLDBxxke5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ff4024-cbcf-4a85-f3f7-83290de8a341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "loss_mae = loss(mode=0)\n",
        "model_4 = tf.keras.models.load_model('/content/drive/MyDrive/OpticsML/single_beam_res2_improved.h5', custom_objects={'loss_mae': loss_mae})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hmUR2jK49zz",
        "outputId": "1eb2681a-a594-4a84-c840-7e2488503c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 49ms/step\n",
            "2/2 [==============================] - 0s 41ms/step\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "2/2 [==============================] - 0s 39ms/step\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "2/2 [==============================] - 0s 28ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 28ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "2/2 [==============================] - 0s 37ms/step\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 39ms/step\n",
            "2/2 [==============================] - 0s 37ms/step\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "2/2 [==============================] - 0s 36ms/step\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "2/2 [==============================] - 0s 39ms/step\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "2/2 [==============================] - 0s 36ms/step\n",
            "2/2 [==============================] - 0s 37ms/step\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as  np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "test_dataset = dataset[2]\n",
        "\n",
        "# Collect predictions and labels using the trained model\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "\n",
        "for batch in test_dataset:\n",
        "    batch_predictions = model_4.predict(batch[0])\n",
        "    batch_true_labels = batch[1]\n",
        "\n",
        "    test_predictions.append(batch_predictions)\n",
        "    test_labels.append(batch_true_labels)\n",
        "\n",
        "test_predictions = np.concatenate(test_predictions, axis=0)\n",
        "test_labels = np.concatenate(test_labels, axis=0)\n",
        "\n",
        "# Create separate columns for each mode's prediction and true label\n",
        "prediction_columns = [f'Predicted Mode {i+1}' for i in range(num_modes)]\n",
        "label_columns = [f'True Label Mode {i+1}' for i in range(num_modes)]\n",
        "\n",
        "# Create a DataFrame with the collected predictions and labels\n",
        "prediction_df = pd.DataFrame(data=np.hstack((test_predictions, test_labels)),\n",
        "                              columns=prediction_columns + label_columns)\n",
        "\n",
        "# Calculate the differences for each mode\n",
        "differences = np.absolute(test_predictions - test_labels)\n",
        "\n",
        "# Calculate the MSEs for each mode\n",
        "mses = [mean_squared_error(test_labels[:, i], test_predictions[:, i]) for i in range(num_modes)]\n",
        "\n",
        "# Create separate DataFrames for differences and MSEs\n",
        "difference_df = pd.DataFrame(data=differences, columns=[f'Difference Mode {i+1}' for i in range(num_modes)])\n",
        "mse_df = pd.DataFrame(data=[mses], columns=[f'MSE Mode {i+1}' for i in range(num_modes)])\n",
        "\n",
        "# Concatenate the DataFrames with differences and MSEs\n",
        "result_df = pd.concat([prediction_df, difference_df, mse_df], axis=1)\n",
        "\n",
        "# Reorganize the columns as desired\n",
        "column_order = []\n",
        "for i in range(num_modes):\n",
        "    column_order.append(f'Predicted Mode {i+1}')\n",
        "    column_order.append(f'True Label Mode {i+1}')\n",
        "    column_order.append(f'Difference Mode {i+1}')\n",
        "    column_order.append(f'MSE Mode {i+1}')\n",
        "\n",
        "result_df = result_df[column_order]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "vif7Wbicd9Xa",
        "outputId": "e1d7e129-8afc-4b8a-e8a3-5e9707ded63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   MSE Mode 1  MSE Mode 2  MSE Mode 3  MSE Mode 4  MSE Mode 5  MSE Mode 6  \\\n",
              "0     0.00906    0.005741    0.005954    0.006051    0.006192    0.002858   \n",
              "\n",
              "   MSE Mode 7  MSE Mode 8  MSE Mode 9  MSE Mode 10  MSE Mode 11  MSE Mode 12  \n",
              "0    0.002559    0.002589    0.003256     0.001969     0.001773     0.001393  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-042aa29b-eea4-47b5-bc58-f08a1ddd1a5a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSE Mode 1</th>\n",
              "      <th>MSE Mode 2</th>\n",
              "      <th>MSE Mode 3</th>\n",
              "      <th>MSE Mode 4</th>\n",
              "      <th>MSE Mode 5</th>\n",
              "      <th>MSE Mode 6</th>\n",
              "      <th>MSE Mode 7</th>\n",
              "      <th>MSE Mode 8</th>\n",
              "      <th>MSE Mode 9</th>\n",
              "      <th>MSE Mode 10</th>\n",
              "      <th>MSE Mode 11</th>\n",
              "      <th>MSE Mode 12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00906</td>\n",
              "      <td>0.005741</td>\n",
              "      <td>0.005954</td>\n",
              "      <td>0.006051</td>\n",
              "      <td>0.006192</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>0.002559</td>\n",
              "      <td>0.002589</td>\n",
              "      <td>0.003256</td>\n",
              "      <td>0.001969</td>\n",
              "      <td>0.001773</td>\n",
              "      <td>0.001393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-042aa29b-eea4-47b5-bc58-f08a1ddd1a5a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-042aa29b-eea4-47b5-bc58-f08a1ddd1a5a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-042aa29b-eea4-47b5-bc58-f08a1ddd1a5a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtKa9sKnuTcM"
      },
      "source": [
        "##execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hNq4Mc1rb1h"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf  # or import the relevant library for your CNN implementation\n",
        "\n",
        "test_dataset = dataset[2]\n",
        "exe = []\n",
        "for batch in test_dataset:\n",
        "    for input_data in batch[0]:\n",
        "        start_time = time.time()\n",
        "        batch_prediction = model_4.predict(np.expand_dims(input_data, axis=0))\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "        exe.append(execution_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "0lAcgIp11tPd",
        "outputId": "893482da-e6cd-49f5-de77-b9b758d26ccd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Predicted Mode 1  True Label Mode 1  Difference Mode 1  MSE Mode 1  \\\n",
              "0             -1.245517          -1.265618           0.020101    0.009264   \n",
              "1             -1.685060          -1.595906           0.089155         NaN   \n",
              "2              1.538995           1.506846           0.032149         NaN   \n",
              "3             -1.626450          -1.633206           0.006756         NaN   \n",
              "4             -0.489387          -0.570171           0.080784         NaN   \n",
              "...                 ...                ...                ...         ...   \n",
              "19995          1.805190           1.790131           0.015059         NaN   \n",
              "19996         -0.157307          -0.043542           0.113765         NaN   \n",
              "19997         -0.820602          -0.990896           0.170295         NaN   \n",
              "19998         -1.082060          -1.092400           0.010339         NaN   \n",
              "19999          1.474736           1.470218           0.004518         NaN   \n",
              "\n",
              "       Predicted Mode 2  True Label Mode 2  Difference Mode 2  MSE Mode 2  \\\n",
              "0             -0.184003          -0.167026           0.016977    0.005845   \n",
              "1              0.823530           0.801273           0.022257         NaN   \n",
              "2              0.847540           0.841907           0.005633         NaN   \n",
              "3             -0.074680          -0.004836           0.069844         NaN   \n",
              "4             -1.830074          -1.907276           0.077203         NaN   \n",
              "...                 ...                ...                ...         ...   \n",
              "19995          1.833509           1.838452           0.004943         NaN   \n",
              "19996         -0.667676          -0.776842           0.109166         NaN   \n",
              "19997          0.174518           0.125512           0.049006         NaN   \n",
              "19998          0.000846          -0.060052           0.060898         NaN   \n",
              "19999          0.650251           0.582790           0.067461         NaN   \n",
              "\n",
              "       Predicted Mode 3  True Label Mode 3  ...  Difference Mode 10  \\\n",
              "0             -0.327631          -0.264113  ...            0.032436   \n",
              "1             -0.593786          -0.663107  ...            0.048165   \n",
              "2             -0.347471          -0.381993  ...            0.009949   \n",
              "3              0.191641           0.336310  ...            0.034991   \n",
              "4             -0.219135          -0.208192  ...            0.003341   \n",
              "...                 ...                ...  ...                 ...   \n",
              "19995         -0.312131          -0.382644  ...            0.027450   \n",
              "19996          0.760952           0.747815  ...            0.090410   \n",
              "19997         -0.616851          -0.529450  ...            0.008704   \n",
              "19998          0.285715           0.312177  ...            0.006005   \n",
              "19999          0.572521           0.565670  ...            0.026100   \n",
              "\n",
              "       MSE Mode 10  Predicted Mode 11  True Label Mode 11  Difference Mode 11  \\\n",
              "0          0.00199           0.257885            0.216397            0.041488   \n",
              "1              NaN          -0.704151           -0.682731            0.021420   \n",
              "2              NaN          -0.414220           -0.420272            0.006052   \n",
              "3              NaN           0.467404            0.549746            0.082343   \n",
              "4              NaN           0.473566            0.506134            0.032569   \n",
              "...            ...                ...                 ...                 ...   \n",
              "19995          NaN          -0.234363           -0.216215            0.018149   \n",
              "19996          NaN           0.126612            0.119425            0.007187   \n",
              "19997          NaN           0.568340            0.635539            0.067199   \n",
              "19998          NaN           0.496998            0.567714            0.070716   \n",
              "19999          NaN           0.562712            0.551497            0.011215   \n",
              "\n",
              "       MSE Mode 11  Predicted Mode 12  True Label Mode 12  Difference Mode 12  \\\n",
              "0         0.001797          -0.223887           -0.217620            0.006267   \n",
              "1              NaN           0.526713            0.570112            0.043399   \n",
              "2              NaN           0.272489            0.291103            0.018614   \n",
              "3              NaN           0.509081            0.528174            0.019093   \n",
              "4              NaN          -0.473552           -0.483645            0.010093   \n",
              "...            ...                ...                 ...                 ...   \n",
              "19995          NaN           0.736883            0.714273            0.022610   \n",
              "19996          NaN          -0.091767           -0.033711            0.058056   \n",
              "19997          NaN          -0.642919           -0.646199            0.003280   \n",
              "19998          NaN          -0.362292           -0.353141            0.009151   \n",
              "19999          NaN          -0.501593           -0.478095            0.023498   \n",
              "\n",
              "       MSE Mode 12  \n",
              "0         0.001428  \n",
              "1              NaN  \n",
              "2              NaN  \n",
              "3              NaN  \n",
              "4              NaN  \n",
              "...            ...  \n",
              "19995          NaN  \n",
              "19996          NaN  \n",
              "19997          NaN  \n",
              "19998          NaN  \n",
              "19999          NaN  \n",
              "\n",
              "[20000 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a34deabe-e014-408b-83b2-c084c4cdc94d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted Mode 1</th>\n",
              "      <th>True Label Mode 1</th>\n",
              "      <th>Difference Mode 1</th>\n",
              "      <th>MSE Mode 1</th>\n",
              "      <th>Predicted Mode 2</th>\n",
              "      <th>True Label Mode 2</th>\n",
              "      <th>Difference Mode 2</th>\n",
              "      <th>MSE Mode 2</th>\n",
              "      <th>Predicted Mode 3</th>\n",
              "      <th>True Label Mode 3</th>\n",
              "      <th>...</th>\n",
              "      <th>Difference Mode 10</th>\n",
              "      <th>MSE Mode 10</th>\n",
              "      <th>Predicted Mode 11</th>\n",
              "      <th>True Label Mode 11</th>\n",
              "      <th>Difference Mode 11</th>\n",
              "      <th>MSE Mode 11</th>\n",
              "      <th>Predicted Mode 12</th>\n",
              "      <th>True Label Mode 12</th>\n",
              "      <th>Difference Mode 12</th>\n",
              "      <th>MSE Mode 12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.245517</td>\n",
              "      <td>-1.265618</td>\n",
              "      <td>0.020101</td>\n",
              "      <td>0.009264</td>\n",
              "      <td>-0.184003</td>\n",
              "      <td>-0.167026</td>\n",
              "      <td>0.016977</td>\n",
              "      <td>0.005845</td>\n",
              "      <td>-0.327631</td>\n",
              "      <td>-0.264113</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032436</td>\n",
              "      <td>0.00199</td>\n",
              "      <td>0.257885</td>\n",
              "      <td>0.216397</td>\n",
              "      <td>0.041488</td>\n",
              "      <td>0.001797</td>\n",
              "      <td>-0.223887</td>\n",
              "      <td>-0.217620</td>\n",
              "      <td>0.006267</td>\n",
              "      <td>0.001428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.685060</td>\n",
              "      <td>-1.595906</td>\n",
              "      <td>0.089155</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.823530</td>\n",
              "      <td>0.801273</td>\n",
              "      <td>0.022257</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.593786</td>\n",
              "      <td>-0.663107</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048165</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.704151</td>\n",
              "      <td>-0.682731</td>\n",
              "      <td>0.021420</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.526713</td>\n",
              "      <td>0.570112</td>\n",
              "      <td>0.043399</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.538995</td>\n",
              "      <td>1.506846</td>\n",
              "      <td>0.032149</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.847540</td>\n",
              "      <td>0.841907</td>\n",
              "      <td>0.005633</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.347471</td>\n",
              "      <td>-0.381993</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009949</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.414220</td>\n",
              "      <td>-0.420272</td>\n",
              "      <td>0.006052</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.272489</td>\n",
              "      <td>0.291103</td>\n",
              "      <td>0.018614</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.626450</td>\n",
              "      <td>-1.633206</td>\n",
              "      <td>0.006756</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.074680</td>\n",
              "      <td>-0.004836</td>\n",
              "      <td>0.069844</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.191641</td>\n",
              "      <td>0.336310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034991</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.467404</td>\n",
              "      <td>0.549746</td>\n",
              "      <td>0.082343</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.509081</td>\n",
              "      <td>0.528174</td>\n",
              "      <td>0.019093</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.489387</td>\n",
              "      <td>-0.570171</td>\n",
              "      <td>0.080784</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.830074</td>\n",
              "      <td>-1.907276</td>\n",
              "      <td>0.077203</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.219135</td>\n",
              "      <td>-0.208192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003341</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.473566</td>\n",
              "      <td>0.506134</td>\n",
              "      <td>0.032569</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.473552</td>\n",
              "      <td>-0.483645</td>\n",
              "      <td>0.010093</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>1.805190</td>\n",
              "      <td>1.790131</td>\n",
              "      <td>0.015059</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.833509</td>\n",
              "      <td>1.838452</td>\n",
              "      <td>0.004943</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.312131</td>\n",
              "      <td>-0.382644</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027450</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.234363</td>\n",
              "      <td>-0.216215</td>\n",
              "      <td>0.018149</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.736883</td>\n",
              "      <td>0.714273</td>\n",
              "      <td>0.022610</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>-0.157307</td>\n",
              "      <td>-0.043542</td>\n",
              "      <td>0.113765</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.667676</td>\n",
              "      <td>-0.776842</td>\n",
              "      <td>0.109166</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.760952</td>\n",
              "      <td>0.747815</td>\n",
              "      <td>...</td>\n",
              "      <td>0.090410</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.126612</td>\n",
              "      <td>0.119425</td>\n",
              "      <td>0.007187</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.091767</td>\n",
              "      <td>-0.033711</td>\n",
              "      <td>0.058056</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>-0.820602</td>\n",
              "      <td>-0.990896</td>\n",
              "      <td>0.170295</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.174518</td>\n",
              "      <td>0.125512</td>\n",
              "      <td>0.049006</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.616851</td>\n",
              "      <td>-0.529450</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008704</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.568340</td>\n",
              "      <td>0.635539</td>\n",
              "      <td>0.067199</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.642919</td>\n",
              "      <td>-0.646199</td>\n",
              "      <td>0.003280</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>-1.082060</td>\n",
              "      <td>-1.092400</td>\n",
              "      <td>0.010339</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000846</td>\n",
              "      <td>-0.060052</td>\n",
              "      <td>0.060898</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.285715</td>\n",
              "      <td>0.312177</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006005</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.496998</td>\n",
              "      <td>0.567714</td>\n",
              "      <td>0.070716</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.362292</td>\n",
              "      <td>-0.353141</td>\n",
              "      <td>0.009151</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>1.474736</td>\n",
              "      <td>1.470218</td>\n",
              "      <td>0.004518</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.650251</td>\n",
              "      <td>0.582790</td>\n",
              "      <td>0.067461</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.572521</td>\n",
              "      <td>0.565670</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.562712</td>\n",
              "      <td>0.551497</td>\n",
              "      <td>0.011215</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.501593</td>\n",
              "      <td>-0.478095</td>\n",
              "      <td>0.023498</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows  48 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a34deabe-e014-408b-83b2-c084c4cdc94d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a34deabe-e014-408b-83b2-c084c4cdc94d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a34deabe-e014-408b-83b2-c084c4cdc94d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-899e80cd-6d2b-4097-a458-64e0f370ba0c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-899e80cd-6d2b-4097-a458-64e0f370ba0c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-899e80cd-6d2b-4097-a458-64e0f370ba0c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChiWIs6N1GDi"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame as a CSV file\n",
        "result_df.to_csv('mse_single_dom_res_improved.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FHegwv8nPSj",
        "outputId": "ea7e9f7f-56df-4f9a-e0d7-70f2a124f2d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFNitk95CxdV"
      },
      "source": [
        "## EXPORT TO DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhy3jxpgnqaR"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "!cp /content/single_beam_no_dom_res2.h5 /content/drive/MyDrive/OpticsML/no_dom0068/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwO0NuFf8eTO"
      },
      "outputs": [],
      "source": [
        "!cp /content/validation_mae_plot_single_beam_no_dom_res2.png /content/drive/MyDrive/OpticsML/no_dom0068"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEhiOutd8rib"
      },
      "outputs": [],
      "source": [
        "!cp /content/mse_single_dom_res2.csv /content/drive/MyDrive/OpticsML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUuQTYdd4939"
      },
      "source": [
        "#CSV STAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLj0I0N_JNq8",
        "outputId": "b75a4544-cefd-40a1-c765-c78da1ee5358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YN5KFQqB55YMC0Xbcb33zF7v9JpLDiFX\n",
            "To: /content/mse_single_dom_res2.csv\n",
            "100% 14.7M/14.7M [00:00<00:00, 58.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1YN5KFQqB55YMC0Xbcb33zF7v9JpLDiFX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orpCUDLFJ7zy"
      },
      "outputs": [],
      "source": [
        "dataset= get_data_h5(\"/content/A_single_beam_weak_noise.h5\",batch_size= 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "w6wePa4I5Idm",
        "outputId": "3eb11e95-7936-44ed-be28-492e93346b49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "\n",
        "# Create histograms\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for i in range(12):\n",
        "  plt.clf()  # Clear the current figure\n",
        "  sns.histplot(result_df[f'Predicted Mode {i+1}'],kde=True, bins=30, alpha=0.5, label=\"Histogram A\", edgecolor='none')\n",
        "  sns.histplot(result_df[f'True Label Mode {i+1}'],kde=True, bins=30, alpha=0.5, label=\"Histogram B\", edgecolor='none')\n",
        "\n",
        "  plt.legend()\n",
        "  plt.savefig(f'plot_distribution_mode {i+1}.png')\n",
        "  shutil.copy(f'plot_distribution_mode {i+1}.png', \"/content/drive/MyDrive/OpticsML/plot_distribution/Hist\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "AgKVsWoO1t87",
        "outputId": "982ec7a8-5257-44d7-917b-92577b8fcab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Predicted Mode 1  True Label Mode 1  Difference Mode 1  MSE Mode 1  \\\n",
              "0              0.077487                0.0           0.077487    0.006685   \n",
              "1             -0.052059                0.0           0.052059         NaN   \n",
              "2              0.106197                0.0           0.106197         NaN   \n",
              "3             -0.001362                0.0           0.001362         NaN   \n",
              "4              0.028669                0.0           0.028669         NaN   \n",
              "...                 ...                ...                ...         ...   \n",
              "19995          0.031258                0.0           0.031258         NaN   \n",
              "19996          0.076802                0.0           0.076802         NaN   \n",
              "19997         -0.084120                0.0           0.084120         NaN   \n",
              "19998          0.048740                0.0           0.048740         NaN   \n",
              "19999         -0.092881                0.0           0.092881         NaN   \n",
              "\n",
              "       Predicted Mode 2  True Label Mode 2  Difference Mode 2  MSE Mode 2  \\\n",
              "0             -0.145000                0.0           0.145000    0.004949   \n",
              "1              0.006199                0.0           0.006199         NaN   \n",
              "2             -0.028096                0.0           0.028096         NaN   \n",
              "3             -0.044040                0.0           0.044040         NaN   \n",
              "4              0.042500                0.0           0.042500         NaN   \n",
              "...                 ...                ...                ...         ...   \n",
              "19995          0.002897                0.0           0.002897         NaN   \n",
              "19996         -0.035429                0.0           0.035429         NaN   \n",
              "19997          0.019632                0.0           0.019632         NaN   \n",
              "19998         -0.019346                0.0           0.019346         NaN   \n",
              "19999         -0.077869                0.0           0.077869         NaN   \n",
              "\n",
              "       Predicted Mode 3  True Label Mode 3  ...  Difference Mode 10  \\\n",
              "0             -0.822664          -0.780949  ...            0.067740   \n",
              "1              0.735874           0.700514  ...            0.005773   \n",
              "2              0.319022           0.438271  ...            0.029632   \n",
              "3              0.624398           0.603604  ...            0.030062   \n",
              "4             -0.175855          -0.229836  ...            0.047550   \n",
              "...                 ...                ...  ...                 ...   \n",
              "19995          0.632054           0.671073  ...            0.022582   \n",
              "19996          0.490871           0.492970  ...            0.048410   \n",
              "19997          0.199506           0.130406  ...            0.037183   \n",
              "19998         -0.006630          -0.057927  ...            0.018145   \n",
              "19999         -0.082683          -0.088288  ...            0.021959   \n",
              "\n",
              "       MSE Mode 10  Predicted Mode 11  True Label Mode 11  Difference Mode 11  \\\n",
              "0          0.00171          -0.359903           -0.327512            0.032391   \n",
              "1              NaN           0.267358            0.264489            0.002869   \n",
              "2              NaN          -0.258040           -0.293809            0.035769   \n",
              "3              NaN           0.097737            0.099919            0.002182   \n",
              "4              NaN          -0.142550           -0.174116            0.031566   \n",
              "...            ...                ...                 ...                 ...   \n",
              "19995          NaN          -0.261142           -0.306788            0.045646   \n",
              "19996          NaN           0.714113            0.736582            0.022469   \n",
              "19997          NaN           0.275376            0.229560            0.045816   \n",
              "19998          NaN           0.375515            0.392088            0.016573   \n",
              "19999          NaN           0.184385            0.207034            0.022649   \n",
              "\n",
              "       MSE Mode 11  Predicted Mode 12  True Label Mode 12  Difference Mode 12  \\\n",
              "0         0.001412           0.275138            0.298697            0.023559   \n",
              "1              NaN           0.700605            0.759998            0.059393   \n",
              "2              NaN          -0.131939           -0.214964            0.083025   \n",
              "3              NaN          -0.274861           -0.314222            0.039361   \n",
              "4              NaN           0.149009            0.151384            0.002375   \n",
              "...            ...                ...                 ...                 ...   \n",
              "19995          NaN           0.056484           -0.009812            0.066296   \n",
              "19996          NaN          -0.201369           -0.233451            0.032082   \n",
              "19997          NaN           0.066988            0.020371            0.046617   \n",
              "19998          NaN          -0.381526           -0.394820            0.013294   \n",
              "19999          NaN           0.513276            0.519534            0.006258   \n",
              "\n",
              "       MSE Mode 12  \n",
              "0          0.00118  \n",
              "1              NaN  \n",
              "2              NaN  \n",
              "3              NaN  \n",
              "4              NaN  \n",
              "...            ...  \n",
              "19995          NaN  \n",
              "19996          NaN  \n",
              "19997          NaN  \n",
              "19998          NaN  \n",
              "19999          NaN  \n",
              "\n",
              "[20000 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59725144-fe08-401c-ad48-9785ca1341ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted Mode 1</th>\n",
              "      <th>True Label Mode 1</th>\n",
              "      <th>Difference Mode 1</th>\n",
              "      <th>MSE Mode 1</th>\n",
              "      <th>Predicted Mode 2</th>\n",
              "      <th>True Label Mode 2</th>\n",
              "      <th>Difference Mode 2</th>\n",
              "      <th>MSE Mode 2</th>\n",
              "      <th>Predicted Mode 3</th>\n",
              "      <th>True Label Mode 3</th>\n",
              "      <th>...</th>\n",
              "      <th>Difference Mode 10</th>\n",
              "      <th>MSE Mode 10</th>\n",
              "      <th>Predicted Mode 11</th>\n",
              "      <th>True Label Mode 11</th>\n",
              "      <th>Difference Mode 11</th>\n",
              "      <th>MSE Mode 11</th>\n",
              "      <th>Predicted Mode 12</th>\n",
              "      <th>True Label Mode 12</th>\n",
              "      <th>Difference Mode 12</th>\n",
              "      <th>MSE Mode 12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.077487</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.077487</td>\n",
              "      <td>0.006685</td>\n",
              "      <td>-0.145000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.145000</td>\n",
              "      <td>0.004949</td>\n",
              "      <td>-0.822664</td>\n",
              "      <td>-0.780949</td>\n",
              "      <td>...</td>\n",
              "      <td>0.067740</td>\n",
              "      <td>0.00171</td>\n",
              "      <td>-0.359903</td>\n",
              "      <td>-0.327512</td>\n",
              "      <td>0.032391</td>\n",
              "      <td>0.001412</td>\n",
              "      <td>0.275138</td>\n",
              "      <td>0.298697</td>\n",
              "      <td>0.023559</td>\n",
              "      <td>0.00118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.052059</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.052059</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006199</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006199</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.735874</td>\n",
              "      <td>0.700514</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005773</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.267358</td>\n",
              "      <td>0.264489</td>\n",
              "      <td>0.002869</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.700605</td>\n",
              "      <td>0.759998</td>\n",
              "      <td>0.059393</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.106197</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.106197</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.028096</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028096</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.319022</td>\n",
              "      <td>0.438271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.029632</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.258040</td>\n",
              "      <td>-0.293809</td>\n",
              "      <td>0.035769</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.131939</td>\n",
              "      <td>-0.214964</td>\n",
              "      <td>0.083025</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.001362</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001362</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.044040</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.044040</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.624398</td>\n",
              "      <td>0.603604</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030062</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.097737</td>\n",
              "      <td>0.099919</td>\n",
              "      <td>0.002182</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.274861</td>\n",
              "      <td>-0.314222</td>\n",
              "      <td>0.039361</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.028669</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028669</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.042500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.042500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.175855</td>\n",
              "      <td>-0.229836</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047550</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.142550</td>\n",
              "      <td>-0.174116</td>\n",
              "      <td>0.031566</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.149009</td>\n",
              "      <td>0.151384</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>0.031258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.031258</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002897</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002897</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.632054</td>\n",
              "      <td>0.671073</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022582</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.261142</td>\n",
              "      <td>-0.306788</td>\n",
              "      <td>0.045646</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.056484</td>\n",
              "      <td>-0.009812</td>\n",
              "      <td>0.066296</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>0.076802</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.076802</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.035429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035429</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.490871</td>\n",
              "      <td>0.492970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048410</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.714113</td>\n",
              "      <td>0.736582</td>\n",
              "      <td>0.022469</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.201369</td>\n",
              "      <td>-0.233451</td>\n",
              "      <td>0.032082</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>-0.084120</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.084120</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.199506</td>\n",
              "      <td>0.130406</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037183</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.275376</td>\n",
              "      <td>0.229560</td>\n",
              "      <td>0.045816</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.066988</td>\n",
              "      <td>0.020371</td>\n",
              "      <td>0.046617</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>0.048740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.048740</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.019346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.019346</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.006630</td>\n",
              "      <td>-0.057927</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.375515</td>\n",
              "      <td>0.392088</td>\n",
              "      <td>0.016573</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.381526</td>\n",
              "      <td>-0.394820</td>\n",
              "      <td>0.013294</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>-0.092881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.092881</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.077869</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.077869</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.082683</td>\n",
              "      <td>-0.088288</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021959</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.184385</td>\n",
              "      <td>0.207034</td>\n",
              "      <td>0.022649</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.513276</td>\n",
              "      <td>0.519534</td>\n",
              "      <td>0.006258</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows  48 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59725144-fe08-401c-ad48-9785ca1341ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59725144-fe08-401c-ad48-9785ca1341ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59725144-fe08-401c-ad48-9785ca1341ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d34c28e3-4ace-4f8e-afd1-674ffcc7473f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d34c28e3-4ace-4f8e-afd1-674ffcc7473f')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d34c28e3-4ace-4f8e-afd1-674ffcc7473f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "for i in range(12):\n",
        "\n",
        "    data_a = result_df[f'Predicted Mode {i+1}']\n",
        "    data_b = result_df[f'True Label Mode {i+1}']\n",
        "    # Perform Mann-Whitney U test\n",
        "    stat, p_value = mannwhitneyu(data_a, data_b)\n",
        "    if p_value < 0.05:\n",
        "        result = \"different\"\n",
        "    else:\n",
        "        result = \"similar\"\n",
        "    print(p_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2YMrSKd1ct8",
        "outputId": "77f3011c-ea5a-47ae-877d-cc8e0b6129e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.135712594671168\n",
            "0.30622551691000366\n",
            "0.005083540857382524\n",
            "0.5943864713813083\n",
            "0.00981218112244338\n",
            "0.26252643405679665\n",
            "0.4459066688003298\n",
            "0.012303803509200918\n",
            "0.27315499961938583\n",
            "0.7389411756967643\n",
            "0.6082598704031928\n",
            "0.939796558159874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hIZaWE6H_5MK",
        "outputId": "9c1e4937-2b3f-42e4-c66b-286ffa5655cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box plots and results saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# Create box plots and perform Mann-Whitney U test\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for i in range(12):\n",
        "    plt.clf()  # Clear the current figure\n",
        "\n",
        "    data_a = result_df[f'Predicted Mode {i+1}']\n",
        "    data_b = result_df[f'True Label Mode {i+1}']\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(data=[data_a, data_b], notch=True, palette=\"Set3\")\n",
        "    plt.title(f'Box Plot - Mode {i+1}')\n",
        "\n",
        "    # Perform Mann-Whitney U test\n",
        "    stat, p_value = mannwhitneyu(data_a, data_b)\n",
        "    if p_value < 0.05:\n",
        "        result = \"different\"\n",
        "    else:\n",
        "        result = \"similar\"\n",
        "\n",
        "    plt.text(0.95, 0.85, f'p-value: {p_value:.4f}\\nDistribution: {result}',\n",
        "             transform=plt.gca().transAxes, ha='right', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "    plt.savefig(f'box_plot_mode_{i+1}.png')\n",
        "    plt.close()\n",
        "\n",
        "    shutil.copy(f'box_plot_mode_{i+1}.png', \"/content/drive/MyDrive/OpticsML/plot_distribution/Boxplot\")\n",
        "\n",
        "print(\"Box plots and results saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "6dmsZcLaAPLn",
        "outputId": "9d96f73b-25aa-450b-80ae-81b0c58e7847"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.95, 0.85, 'p-value: 0.9933\\nDistribution: similar')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAIUCAYAAAAaKeS5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSJUlEQVR4nO3de3zO9f/H8ee1s40NfQ0NaaOxWCqnGSPyZQg5FHIopyUlpEK+SL5OhTJ9RTnlW3SgMub0ky/lVEpJX8RWTtWc2sFm27Xt+v3ht+vnsvPsui4+Hvfbbbddn/f1/rw/r8/w8dznen8+H5PFYrEIAAAAMCAXZxcAAAAA2AthFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAc6MyZMwoODta6deucXcpNgZ8HAHsj7AK4Ja1bt07BwcE2X2FhYRowYIB27tzp8Hr2799vU8u9996rdu3a6aWXXtLp06fLZBvff/+9oqOjlZycXCbjXSs6OlrBwcGqV6+e/vjjjzzvX758WaGhoQoODta0adPKfPtlKTY2VuPGjdPf//53BQcHa8CAAfn2O3TokKZNm6bOnTurUaNGatOmjZ5//nn9+uuvDq4YgD25ObsAALgRo0aNUo0aNWSxWHTx4kV99tlnGj58uN555x099NBDDq9nwIABatiwobKysvTf//5XH330kXbu3Kn169eratWqNzT2wYMHtXDhQj366KPy9fUto4pteXh4aMOGDRo2bJhN+9atW+2yPXtYvXq1Dh8+rIYNGyoxMbHAfu+9956+//57dezYUcHBwTp//rw++OAD9ejRQx999JHuuecexxUNwG4IuwBuaREREWrYsKF1uVevXgoPD9eGDRucEnYbN26sjh07SpJ69uyp2rVra/r06fr8888VFRXl8HpKqnXr1tq4cWOesLthwwa1adNGW7ZscVJlxTdnzhxVrVpVLi4u6tKlS4H9nnzySb3xxhvy8PCwtnXq1EmPPPKIlixZojfeeMMR5QKwM6YxADAUX19feXp6ys3N9nf5tLQ0zZo1S61bt1aDBg3UoUMHLV26VBaLRZKUnp6ujh07qmPHjkpPT7eul5iYqJYtW6pPnz7Kzs4ucT3NmzeXdHVuamH27t2rfv36qVGjRmrcuLFGjBihuLg46/vR0dGaM2eOJKldu3bW6RJFjVtSXbp00ZEjR2y2ff78ee3bt6/A4Hjx4kVNnDhRLVq0UMOGDdW1a1d99tlnefolJydr/PjxevDBB9W4cWO9/PLLSklJyXfMuLg4jRo1Sk2bNlXDhg3Vo0cPbd++vVj7UL16dbm4FP3f2wMPPGATdCWpdu3aqlu3ruLj44u1LQA3P8IugFva5cuXdenSJV26dEnHjx/XlClTlJaWpq5du1r7WCwWjRgxQitWrFCrVq00YcIE3X333ZozZ45mzpwpSfLy8tLs2bN16tQpzZ8/37rutGnTlJKSopkzZ8rV1bXE9Z06dUqSVLFixQL77NmzR0OHDtXFixf17LPP6sknn9TBgwfVt29fa5ht3769NWxOmDBBc+bM0Zw5c1S5cuUS11SYJk2aqFq1atqwYYO1LTY2Vt7e3mrTpk2e/unp6RowYIDWr1+vRx55RC+99JIqVKig8ePHa+XKldZ+FotFzzzzjL744gt17dpVo0eP1p9//qmXX345z5jHjx/X448/rri4OA0bNkzjx4+Xt7e3Ro4cqW3btpXp/l7PYrHowoULqlSpkl23A8BxmMYA4Jb25JNP2ix7eHhoxowZCg8Pt7Zt375d+/bt0+jRozVixAhJ0hNPPKFRo0bp/fffV//+/VWrVi3dd999Gjp0qN599121b99eFy5c0MaNGzVx4kTdfffdxaonNTVVly5dUlZWlo4cOaJ//vOfMplM+vvf/17gOnPmzJGfn58++ugjayh++OGH9eijjyo6OlqzZ89WvXr1FBISog0bNujhhx9WjRo1SvaDKoFOnTpp48aNev755yVJMTExat++fZ6zoJL00UcfKS4uTq+//rr1F4w+ffpowIABevPNN9WzZ0+VL19e27dv17fffqsXX3xRQ4cOlST17dtXAwcOzDPmP//5T1WvXl1r1661brNfv37q27ev3njjDbVv395eu67169crISFBo0aNsts2ADgWZ3YB3NImT56s5cuXa/ny5Xr99dfVrFkzTZo0yeaCql27dsnV1TXPVfmDBw+WxWLRrl27rG3PPvus6tSpo5dfflmvvvqqmjZtmm8gK8jEiRMVFhamVq1aafjw4bpy5YpmzZplM6/4WufOndORI0f06KOP2pz9rVevnlq0aOGUO0s88sgjOnnypA4dOqSTJ0/qp59+0iOPPJJv3127dqlKlSo2Uxzc3d01YMAApaWl6dtvv7X2c3NzU9++fa39XF1d1b9/f5vxEhMTtW/fPkVGRtqctf/rr7/UsmVL/fbbb0pISLDDXl+dOjFt2jTdf//9evTRR+2yDQCOx5ldALe00NBQmyDZpUsXde/eXdOmTVObNm3k4eGhs2fPyt/fX+XLl7dZNygoSJJ09uxZa1vumeFevXrJ09NTM2bMkMlkKnY9I0eOVOPGjeXi4qJKlSopKCgoz/zha/3++++SlO+Z46CgIH399ddKS0uTt7d3sWuQrp5hTktLsy67uroWe8pDSEiIAgMDtWHDBvn6+qpKlSrWucfXO3v2rO666648c2Rzf7a5+3f27FlVqVJFPj4+Nv2u3+9Tp07JYrHorbfe0ltvvZXvNi9evHjDd7a43vnz5xUVFaUKFSrorbfeKtWUFQA3J8IuAENxcXFRs2bN9P777+vkyZOqW7duicf4+uuvJUkZGRk6efKkatasWex177nnHrVo0aLE2yxry5Yt08KFC63LAQEB+vLLL4u9fpcuXbR69Wr5+PgoMjKyWBd8lYWcnBxJV8+6t2rVKt8+tWrVKtNtpqSkaNiwYUpJSdEHH3xQ5kEagHMRdgEYTu5dE3LPbAYEBGjv3r26fPmyzdnd3CvuAwICrG1Hjx7V22+/rR49eujo0aOaNGmSYmJiVKFCBbvUeuedd0pSvg8yiI+PV6VKlaxndUtyhrl79+568MEHrcuenp4lquuRRx7RggULdP78eb3++usF9gsICNCxY8eUk5NjE4hzf7a5+xcQEKB9+/YpNTXV5uzu9fud+4uFu7u7Q35pyMjI0NNPP63ffvtNy5cvV506dey+TQCOxZxdAIZiNpu1e/duubu7Wz9Kj4iIUHZ2tj744AObvitWrJDJZFJERIR13QkTJsjf31+vvPKKZs6cqQsXLmjGjBl2q9ff31/169fX559/bvNktF9++UW7d+9W69atrW3lypWTpAJv13WtmjVrqkWLFtava4NvcdSqVUsTJ07UCy+8oNDQ0AL7RURE6Pz584qNjbW2ZWVladWqVfL29laTJk2s/bKysrR69Wprv+zsbP373/+2Ge+OO+5Q06ZN9dFHH+ncuXN5tnfp0qUS7UdhsrOzNXr0aP3www966623dP/995fZ2ABuHpzZBXBL27Vrl/Us4qVLlxQTE6PffvtNw4cPt57Fbdu2rZo1a6b58+fr7NmzCg4O1u7du7V9+3YNGjTI+rH4okWLdOTIEa1YsULly5dXvXr1NHLkSL355pvq2LGjTfAsSy+99JKGDRumxx9/XL169VJ6err+/e9/q0KFCnr22Wet/e69915J0vz589WpUye5u7vroYceKvF83uIaNGhQkX0ef/xxffTRRxo/frx+/vlnBQQEaMuWLfr+++81ceJEmz+DBx54QHPnztXZs2dVp04dbd26Nd/gPmXKFPXr10+PPPKIHnvsMdWsWVMXLlzQDz/8oD///FPr168vtKZvv/3WemHcpUuXlJaWpn/961+Srt5aLTeAz5o1S19++aUeeughJSYm6osvvrAZp1u3bkX/kADc9Ai7AG5pCxYssL729PRUYGCgpk6dqj59+ljbXVxctGjRIi1YsECxsbFat26dAgIC9NJLL2nw4MGSpJ9//lmLFy9W//79bS7GGj58uLZv365JkyZp48aNdnlMb4sWLfTee+9pwYIFWrBggdzc3NSkSRO9+OKLNvOFQ0ND9fzzz2vNmjX66quvlJOTo+3bt9st7BaHl5eXVq1apTfeeEOfffaZLl++rLvvvlszZ85Ujx49rP1y/wxmzJih9evXy2QyqW3btho/fry6d+9uM2adOnW0du1aLVy4UJ999pkSExNVuXJlhYSEaOTIkUXWtG/fPpv5ypKsF7s9++yz1rB79OhRSdKOHTu0Y8eOPOMQdgFjMFlyHx8EAAAAGAxzdgEAAGBYhF0AAAAYFmEXAAAAhkXYBQAAgGERdgEAAGBYhF0AAAAYFvfZzcfBgwdlsVjk7u7u7FIAAACQD7PZLJPJVOTTDwm7+bBYLOL2wwAAADev4mY1wm4+cs/oNmzY0MmVAAAAID8//fRTsfoxZxcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACG5ebsAgBJslgsyszMdHYZDmexWCRJJpPJyZU4noeHx2253wAAxyLswuksFovmzZun+Ph4Z5cCBwoMDNTYsWMJvAAAu2IaAwAAAAyLM7twOpPJpLFjx9520xgyMjI0YcIESdLMmTPl6enp5Ioci2kMAABHIOzipmAymW67sHctT0/P23r/AQCwF6YxAAAAwLAIuwAAADAswi4AAAAMizm7AADYAfcPv/0uQOXC25sTYRcAgDJWVvcP9/LykoeHRxlVBXu76667FBUVReC9yRB2AQC4CXl5eSkyMlLly5d3dikoJv6sbk6EXQAAylhZ3T/czc3NOi3gVpCdna2vvvpKktSqVSu5uro6uSLHcnV15azuTYiwCwCAHdyu9w/PyMiQdDX4ubkRM+B83I0BAAAAhkXYBQAAgGERdgEAAGBYhF0AAGAI0dHRCg4OdnYZuMk4ZOZ4XFycpk+froMHD8rHx0fdunXT6NGjC7134P79+zVw4MB837v77ru1efPmQvt16tRJ8+fPL5sdAAAAKKVPPvlEy5Yt05kzZ1S9enUNGDBAAwYMKNa6hw8f1vz583Xw4EFZLBbdf//9evHFF1W/fn2bfmazWYsXL9Znn32mhIQEVa1aVT179tTw4cNtLhQ8fvy4oqOj9fPPP+vChQvy8vJSnTp1NGTIELVt29ZmzI8//ljr169XfHy8kpOT5e/vr2bNmmnkyJGqUaPGjf9gHMTuYTcpKUmDBg1S7dq1FR0drYSEBM2aNUvp6emaPHlygevde++9+uijj2zaLl++rGHDhikiIiJP/5kzZyowMNC6XKlSpbLbCQAAgFJYs2aNpkyZog4dOuipp57SgQMHNH36dF25ckXDhw8vdN2ff/5Z/fr1U/Xq1fXss88qJydHH374ofr3769PPvnEJve8+OKL2rx5s3r27KkGDRroxx9/1FtvvaU//vhDr732mrXf77//rtTUVD366KPy9/fXlStXtHXrVo0YMULTpk3T448/bu373//+VzVq1FDbtm3l6+urM2fO6JNPPtGOHTv0xRdfqGrVqmX/A7MDu4fdNWvWKDU1VQsXLlTFihUlXb0P36uvvqqoqKgCf1Dly5dXo0aNbNrWrVunnJwcdenSJU//unXrqmHDhmVdPgAAQKmkp6dr/vz5atOmjRYsWCBJeuyxx5STk6NFixbp8ccfl5+fX4Hrv/XWW/Ly8tKaNWusJ/G6du2qDh06aP78+YqOjpYkHTp0SJs2bdIzzzyj559/XpLUt29fVapUScuXL9cTTzyhevXqSZJat26t1q1b22ynf//+6tGjh5YvX24TdqdOnZqnpocfflg9e/bUF198UWRYv1nYfc7url27FBYWZg26khQZGamcnBzt3r27RGNt2LBBtWvXVmhoaBlXCQAASip3jmxcXJyef/55NW3aVJMmTdJnn31mvd9uQZYuXarg4GCdPXs2z3tz585VgwYNlJSUJEk6cOCARo0apTZt2qhBgwZq3bq1ZsyYofT09EK3cebMGQUHB2vdunV53gsODraGxVwJCQmaMGGCWrRooQYNGqhz58769NNP86z7+++/Ky4urtBtS1enWiYmJqpfv3427U888YTS0tL0n//8p9D1Dxw4oLCwMJtPq/39/dW0aVPt2LFDqampkqTvvvtOktS5c2eb9Tt16iSLxaJNmzYVuh1XV1dVr15dKSkpRe5TQECAJCk5ObnIvjcLu4fd+Ph4m9PskuTr66sqVaqU6JnhFy5c0L59+/I9qytJw4cPV/369RUREaHZs2cX+Q8AAACUjdGjRysjI0OjR49W/fr19dVXX+V7VvBakZGRMplM+QaxTZs2KTw83HrWc/PmzUpPT1ffvn31j3/8Qy1bttS///1vvfTSS2W2DxcuXNBjjz2mvXv36oknntArr7yiWrVq6ZVXXtGKFSts+r788svq1KlTkWP+97//lSQ1aNDApv3ee++Vi4uLjhw5Uuj6mZmZ8vLyytPu5eUls9ms48ePW/tJyvMQk3Llykm6Ou/3emlpabp06ZJOnTqlFStWaNeuXWrevHm+dfz111+6ePGifvrpJ02YMEGSFBYWVmjtNxO7T2NITk6Wr69vnnY/Pz/rb2zFERsbq+zs7Dxht0KFCho6dKiaNGkiT09P7du3T8uWLVN8fLwWL15c6rotFovS0tJKvT5QlGsfI3rlyhVlZ2c7sRoANyMPD4+b+nHBucetgIAARUdHKzs7W3fccYe8vLy0fv16DRo0qMC7I1SpUkWhoaHauHGjBg0aZG3/6aefdPr0aY0YMUJms1mS9Pzzz9uEvh49eqhGjRp66623dOrUKVWvXt2mntz1srKyrO25bdfXn9s+d+5cZWdn67PPPrN+Gt2rVy+9+OKLWrhwoXr27GmtIScnx2Y7BUlISJCrq6t8fX1t+ppMJlWsWFF//vlnoWPUrl1bP/zwg9LT062PXjabzfrxxx8lXT3DfO+996pmzZqSpG+//VbVqlWzrr9//35rHddvZ8aMGfrkk08kSS4uLnr44Yc1YcKEfOuJiIiw/p9VsWJFTZgwQU2bNi20dpPJdMOPyy6KxWIp1uOZb5nn+MXExOjee+/V3XffbdMeEhKikJAQ63JYWJj8/f01bdo0HTp0qNRTHsxmc5G/cQE34tqDxNGjR+Xu7u7EagDcbNzc3BQQEKCsrCxruLrZ5J4UioyM1Pnz5611tmzZUrt379bmzZtVuXLlAtdv2bKl3n77bf3www/Wj8c/++wzubu7KzQ0VOfPn7f2zf2I/cqVK8rMzFTt2rVlsVi0b98+tWzZ0qae3PUuXbpkXffasa6t//z587JYLNq6davatGmjCxcu6MKFC9Y+oaGh2rRpk/bs2WO9Nuj111+32U5BEhMT5ebmlm8/Nzc3JScnFzpGly5dNH/+fL300kvq06ePLBaLVq1aZV3n/PnzOn/+vEJCQlS1alXNmTNHmZmZuueee3TkyBG9+eabcnV1VWpqap7tdOnSRc2aNdOFCxf0n//8R1euXNGff/6Zb0CdNWuWMjMzdfLkSf3P//yPLly4UGjdLi4ucnNz09mzZ62/cNhLYXf2ymX3sOvr65vvHJCkpKRCJ2Vf69SpUzp06JD11HlRIiMjNW3aNB0+fLjUYdfd3V116tQp1bpAcVx7QKlXr16x/sECuL3c7Gd2vb29JV0NhFWqVLGeWf3b3/4mFxcXJSYmqkqVKkpKSrL5Bd/T01MVKlRQz549tWjRIn3zzTcaPny4LBaLvvrqK7Vq1Uq1a9e29v/jjz+0cOFC7dixI89cURcXF1WpUsWmntzl3ONshQoVrG3X11+lShVdvHhRly9f1oYNG7Rhw4Z89zUnJyffMQpTsWJFZWVl5bteVlaWdVpnQYYMGaLLly9r+fLl2rJli6SrUyAGDx6sJUuWqGrVqtb1Fy9erBdeeEFTpkyRdPXvztixY7VkyZJ89//a5f79+2vYsGGaMmWKVq9eneds6d///nfr60ceeUSPPvqoqlSpkmcu8rVMJpP1z8NeTpw4Uax+dg+7gYGBeebm5v6Gdf1c3oLExMTIxcWlWPNjyooj/pBwe8v9SEq6Oq/q+rlWAHCzyz2Oubm5yd3dPU9IcnFxkbu7u8aMGaNvvvnG2v7oo49q1qxZCggIUOPGjbV161aNHDlSBw8e1B9//KEXX3zR+mlXdna2hg0bpqSkJA0bNkyBgYHy9vZWQkKCxo8fb93GtfXkLl/bfu2nZ7mhPLc9d72uXbvq0UcfzXdfg4ODS/wJXNWqVZWdna3k5GTdcccd1vbMzEwlJiaqWrVqRY45btw4DRs2TMePH1eFChUUHBysefPmSZLq1KljXb9+/frauHGjTpw4oaSkJNWpU0deXl6aM2eOmjZtWuR2IiMjNXnyZJ05c6bQfBYUFKSQkBDFxsbaTD/Jz7X397WH4kxhkBwQdiMiIvTOO+/YzN3dvHmzXFxcFB4eXqwxNm7cqKZNm8rf37/Y/SVxKzIAABzg5MmT1nmj0tWLvXJycqwPHnj55Zdtzshe+/95ZGSkXn31VcXHxys2NlblypXTQw89ZH3/l19+0W+//abZs2ere/fu1vbi3NEp9xPk688G//777zbLlStXlo+Pj3JyctSiRYti7HHx5D744fDhwza3+zp8+LBycnKstwMrip+fnxo3bmxd3rNnj6pVq5YnlJpMJtWtW9e6vHPnzmLvU+6F/ZcvXy5WX3vPxy1Ldr8bQ58+feTj46ORI0fq66+/1tq1azVnzhz16dPH5h67gwYNUvv27fOs/9///ldxcXEF3oVh3Lhxio6O1vbt2/X111/rjTfe0OzZs/Xwww8TdgEAcIAPPvjAZvnrr7+WJOtDoBo0aKAWLVpYv66dJtihQwe5urpq48aN2rx5s9q0aWPzyaqLy9Wocu10DovFovfff7/IusqXL69KlSrpwIEDNu0ffvihzbKrq6s6dOigLVu26JdffskzTu7c31zFvfVY8+bNVbFiRa1evdqmffXq1SpXrpzatGljs424uDhduXKl0DFjY2P1008/adCgQdafTX7S09P11ltvqUqVKja3JLt48WKevmazWV988YW8vLwUFBQk6eo0i/xuJHDo0CH98ssvee4wcTOz+5ldPz8/rVy5Uq+99ppGjhwpHx8f9erVS2PGjLHpl5OTk+/V6DExMfLw8FCHDh3yHb9u3bqKiYnRsmXLZDabFRAQoKeffvqWudExAAC3ujNnzujpp59WeHi4Nm3apO+++06dO3cu1pnLO+64Q82aNdPy5cuVmpqaZ8piYGCgatWqpdmzZyshIUHly5fXli1bin2f1969e2vJkiV65ZVX1KBBAx04cEC//vprnn4vvPCC9u/fr8cee0y9e/dWnTp1lJSUpJ9//ll79+61mYbx8ssv65tvvtGxY8cK3baXl5dGjRqladOmadSoUWrVqpUOHDig9evXa8yYMTbPIPjggw+0cOFCvf/++2rWrJmkq3dXePvttxUeHq6KFSvqxx9/1Lp169SqVSsNHDjQZlvPP/+8/P39VadOHV2+fFlr167V6dOntWTJEpUvX97ab/Lkybp8+bKaNGmiqlWr6vz584qJiVF8fLzGjx8vHx8fSVcv3mvTpo0iIyNVt25dlStXTr/88ovWrVunChUq6JlnninWz/9m4JC7MQQFBeW5R931Vq1alW/7yy+/rJdffrnA9aKiohQVFXUj5QEAgBvw5ptv6q233tL8+fOVk5Ojli1b2jyitiidOnXSnj175OPjk+fpXu7u7nrnnXc0ffp0LV68WJ6enmrfvr2eeOIJdevWrcixR44cqUuXLmnLli3atGmTIiIi9N577+W5T+zf/vY3ffLJJ3r77be1bds2rV69WhUrVlSdOnU0bty4Yu/L9Z544gm5u7tr2bJl+vLLL1W9enVNmDChyPmu0tU5v66urlq6dKlSU1NVo0YNjR49Wk8++WSe+bANGjTQunXr9NFHH8nLy0sPPvig5s6da51KkatTp0769NNPtXr1aiUmJsrHx0f33nuvxo0bp3bt2ln7eXl5qVevXtq/f7+2bNmijIwM+fv7q3PnzhoxYoR1isqtwGS5mS/zdJKffvpJEnN+YV8ZGRkaO3asJGnevHlcoAbglhMdHa2FCxdq7969qly5srKysqx3DejQoYPdL1DC7a24ec3uc3YBAAAAZyHsAgAAwLAIuwAAADAswi4AACiV5557TseOHSv0kcCAsxF2AQAAYFiEXQAAABgWYRcAgNtEdHS0goODHbKtJ598UgMGDLAu79+/X8HBwdq8ebNDtj9+/Hi1bdvWIdsqK/aoecCAATZ/DmfOnFFwcLDWrVtXptu5mRF2AQC4Ba1bt07BwcHWr4YNG6ply5YaMmSI3n//fV2+fLlMtpOQkKDo6GgdOXKkTMYrSzdzbbh5cLdnAABuYaNGjVKNGjWUlZWlCxcu6JtvvtGMGTO0YsUK/etf/7J5ZO+IESM0fPjwEo1/7tw5LVy4UAEBAXmexlWYJUuW2P2hEoXV9tprr+lWe26WPWpeunRpmY53KyLsAgBwC4uIiLB5glRUVJT27t2rp59+Ws8884xiY2Pl5eUlSXJzc7N7AM3MzJSHh4c8PDyc+gQ1d3d3p227tOxRs4eHR5mPea20tDR5e3vbdRs3imkMAAAYTFhYmJ555hmdPXtW69evt7bnN2d39+7d6tu3rxo3bqz7779fHTp00Lx58yRdnWfbq1cvSdKECROsUyZy53sOGDBAXbp00eHDh/XEE0/owQcf1MaNGyXlnbObKycnR/PmzVN4eLgaNWqkp59+Wn/88YdNn7Zt22r8+PF51r12/mlRteU3/zUtLU2zZs1S69at1aBBA3Xo0EFLly7NczY1ODhY06ZN0//8z/+oS5cuatCggTp37qxdu3blqSkuLk6///57nvbrXb58Wf/85z/Vtm1bNWjQQGFhYXrqqaf0888/W/tcX3Pu/NqlS5fqgw8+ULt27XTfffdp8ODB+uOPP2SxWPT2228rIiJCoaGhGjFihBITEwv8mRXk6NGjGj9+vNq1a6eGDRsqPDxcEyZM0F9//WXTL/fvz4kTJ/TCCy+oSZMm6tevX5H77myc2QUAwIC6deumefPm6euvv9Zjjz2Wb5/jx48rKipKwcHBGjVqlDw8PHTy5El9//33kqSgoCCNGjVKCxYs0OOPP64HH3xQkvTAAw9Yx0hMTNSwYcPUuXNnde7cWadPny60rkWLFslkMmnYsGG6ePGiVq5cqSeffFJffPGF9Qx0cRSntmtZLBaNGDHCGpLr16+vr776SnPmzFFCQoImTpxo0/+7777T1q1b1a9fP/n4+GjVqlUaNWqUduzYoUqVKln7derUSU2bNtWqVasKrXfKlCnasmWL+vfvr6CgICUmJuq7775TXFyc7r333kLXjYmJkdls1oABA5SYmKj33ntPo0ePVvPmzbV//34NGzZMJ0+e1L///W/Nnj1bM2fOLM6P0GrPnj06ffq0evTooSpVquj48eP6+OOPdeLECX388ccymUw2/Z9//nndddddGjNmzC0xVYSwCwCAAVWrVk0VKlQoNHzu3r1bZrNZ7777br4Phvjb3/6miIgILViwQI0aNVK3bt3y9Dl//rxeffVV9enTR1lZWdqyZUuhdSUlJSk2Nlbly5eXJIWEhGj06NH6+OOPNXDgwGLvX3Fqu9b27du1b98+jR49WiNGjJAkPfHEExo1apTef/999e/fX7Vq1bL2j4uLU2xsrLWtWbNm6tatmzZu3Kj+/fsXu85cO3fu1GOPPWZzxnrYsGHFWjchIUFbt25VhQoVJF09O7548WKlp6dr7dq11ukif/31l2JiYvTqq6+WaPpCv379NHjwYJu2Ro0aaezYsfruu+/UuHFjm/fq1aunuXPnFnt8Z2MaAwAABuXt7a3U1NQC3/f19ZV0NQjm5OSUahseHh7q0aNHsft3797dGnQlqWPHjqpSpYp27txZqu0X165du+Tq6prnI/3BgwfLYrHkmaLQokULm/Bbr149lS9fPs8vD8eOHSvyrK509Wf9448/KiEhocS1d+zY0Rp0JSk0NFSS1LVrV5t50aGhoTKbzSXexrVn1DMyMnTp0iXdd999kmQzzSJXnz59SjS+sxF2AQAwqLS0NPn4+BT4fqdOnfTAAw9o0qRJatGihcaMGaPY2NgSBd+qVauW6CziXXfdZbNsMpl011136ezZs8UeozTOnj0rf39/m6AtXZ0Okfv+tapXr55nDD8/PyUnJ5dq++PGjdPx48fVpk0b9erVS9HR0UVO+SioltzgW1B7UlJSiWpLTEzU9OnT1aJFC4WGhiosLEzt2rWTJKWkpOTpX6NGjRKN72xMYwAAwID+/PNPpaSk2JydvJ6Xl5c++OAD7d+/X//5z3/01VdfKTY2Vh999JGWLVsmV1fXIrdTknm2Nyo7O7tYNZWFgrZT2jmqnTp1UuPGjbVt2zbt3r1bS5cu1bvvvqvo6Gi1bt26VLW4uOR/zrKkNY4ePVoHDx7UkCFDVL9+fXl7eysnJ0dDhw7NdyxPT88Sje9snNkFAMCAvvjiC0lSy5YtC+3n4uKisLAwTZgwQbGxsRozZoz27dun/fv3S1Kei5Nu1MmTJ22WLRaLTp48qYCAAGtbQWdQr7/rQUlqCwgI0Llz5/I8bCM+Pt76vr35+/vriSee0L/+9S9t375dFStW1DvvvGP37RYmKSlJe/fu1bBhwzRq1Ci1b99e4eHhqlmzplPrKkuEXQAADGbv3r3617/+pRo1aqhr164F9rv+NlWSrA9nyMzMlCSVK1dOkkr98f31Pv/8c5vAuXnzZp0/f14RERHWtpo1a+rHH3+01iBJO3bsyHOLspLUFhERoezsbH3wwQc27StWrJDJZLLZfkkU59Zj2dnZeaYD3HHHHfL397fZR2co6KzxypUrHVyJ/TCNAQCAW9iuXbsUHx+v7OxsXbhwQfv379fu3bt15513atGiRYV+5Pz222/rwIEDat26tQICAnTx4kV9+OGHqlatmvVWXrVq1ZKvr6/WrFkjHx8feXt7KzQ0tNRn/vz8/NSvXz/16NHDeuuxu+66y+b2aL1799aWLVs0dOhQRUZG6tSpU4qJickzJaMktbVt21bNmjXT/PnzdfbsWQUHB2v37t3avn27Bg0aVOh0j8IU59Zjqampat26tTp06KB69erJ29tbe/bs0U8//ZTv/YQdqXz58mrSpInee+89mc1mVa1aVbt379aZM2ecWldZIuwCAHALW7BggaSrT9+qWLGi7rnnHk2cOFE9evTIczHW9dq2bauzZ89q7dq1+uuvv1SpUiU1bdpUzz33nPViJ3d3d82aNUvz5s3T1KlTlZWVpZkzZ5Y67D799NM6duyYlixZotTUVIWFhWnKlCnWs7SS1KpVK40fP17Lly/XjBkz1KBBA73zzjuaPXu2zVglqc3FxUWLFi3SggULFBsbq3Xr1ikgIEAvvfRSnttulTUvLy/17dtXu3fv1tatW2WxWFSrVi1NmTLlpngow9y5c/Xaa6/pww8/lMViUXh4uN599121atXK2aWVCZPlVrgbsIP99NNPkmTz+EWgrGVkZGjs2LGSpHnz5t1yE/4B4HrX3me3Q4cOTn1cMIyvuHmNObsAAAAwLMIuAAAADIuwCwAAAMMi7AIAAMCwCLsAAAAwLMIuAAAADIuwCwAAAMPiBngAAKDMcM9w3GwIuwAAoEyYTCbrU9tMJpOTqwGuIuwCAIAykZWVpS+++EKSdP/998vV1dXJFQGEXQAAUIaSk5OdXQJggwvUAAAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFhujthIXFycpk+froMHD8rHx0fdunXT6NGj5eHhUeh6bdu21dmzZ/O0Hzp0SJ6entblhIQETZ8+XV9//bXc3d3Vvn17TZgwQeXLly/zfQEAAMCtw+5hNykpSYMGDVLt2rUVHR2thIQEzZo1S+np6Zo8eXKR63fo0EGDBw+2abs2JJvNZg0dOlSSNHfuXKWnp2v27Nl64YUXtHjx4rLdGQAAANxS7B5216xZo9TUVC1cuFAVK1aUJGVnZ+vVV19VVFSUqlatWuj6f/vb39SoUaMC39+yZYuOHz+u2NhYBQYGSpJ8fX01ZMgQHTp0SKGhoWW1KwAAALjF2H3O7q5duxQWFmYNupIUGRmpnJwc7d69u0zGDw4OtgZdSQoPD1fFihW1c+fOGx4fAAAAty67h934+HibICpdPfNapUoVxcfHF7l+TEyMGjRooPvvv1/Dhg3TsWPHihzfZDLp7rvvLtb4AAAAMC67T2NITk6Wr69vnnY/Pz8lJSUVum7btm0VGhqqO++8U6dPn9Y777yjfv366fPPP1fNmjWt41eoUKFU4xfGYrEoLS2t1OsDRcnMzLS+vnLlirKzs51YDQDcOI5rcCSLxSKTyVRkP4fcjaG0Jk2aZH3duHFjhYeHKzIyUkuXLtXUqVPtum2z2awjR47YdRu4vZnNZuvro0ePyt3d3YnVAMCN47gGRyvqzl6SA8Kur6+vUlJS8rQnJSXJz8+vRGP5+/vrwQcf1M8//2wz/uXLl/Mdv3r16iUv+P+4u7urTp06pV4fKMq1Z0Dq1atXrH+wAHAz47gGRzpx4kSx+tk97AYGBuaZO5uSkqLz58/nmWtb2vF/+eUXmzaLxaJff/1V4eHhpR7XZDLJ29v7RssDCuTq6mp9Xa5cOZt7RwPArYjjGhypOFMYJAdcoBYREaE9e/YoOTnZ2rZ582a5uLiUOIwmJCTou+++U8OGDW3GP3r0qH777Tdr2969e5WYmKjWrVvfcP0AAAC4ddn9zG6fPn20atUqjRw5UlFRUUpISNCcOXPUp08fm3vsDho0SL///ru2bdsmSdqwYYN27Nih1q1by9/fX6dPn9aSJUvk6uqqp556yrpehw4dtHjxYj333HMaO3asrly5ojlz5qhNmzbcYxcAAOA2Z/ew6+fnp5UrV+q1117TyJEj5ePjo169emnMmDE2/XJycmyu2qxRo4bOnTunGTNmKCUlRRUqVFDz5s01atQo650YpKtza9977z1Nnz5dY8eOlZubm9q3b6+JEyfae9cAAABwk3PI3RiCgoK0YsWKQvusWrXKZrlRo0Z52gpStWpVRUdHl7Y8AAAAGJTd5+wCAAAAzkLYBQAAgGERdgEAAGBYhF0AAAAYFmEXAAAAhkXYBQAAgGERdgEAAGBYhF0AAAAYFmEXAAAAhkXYBQAAgGERdgEAAGBYhF0AAAAYFmEXAAAAhkXYBQAAgGERdgEAAGBYhF0AAAAYlpuzCwAAGJfZbFZmZqazy4CDXPtnnZaWpqysLCdWA0dxcXFRuXLlnF1GgQi7AAC7+fHHH7V8+XJnlwEnmDRpkrNLgIMEBwdr1KhRzi6jQExjAAAAgGFxZhcAYHd16lTR8OEtnV0GHMBisUiSTCaTkyuBvf3ww2l9+OEBZ5dRJMLuTcRsNuvKlSvOLgMOcu3ctpSUFGVkZDixGjiKm5ubvL29nV2Gw5lMkosL4ef2wJ/z7eJW+YWGsHsTYW7b7WvKlCnOLgEOcrPPbQMAo2HOLgAAAAyLM7s3IZ/qVXV3hzbOLgMOwNy220di/Emd2bXP2WUAwG2HsAs4ESEXAAD7YhoDAAAADIuwCwAAAMMi7AIAAMCwCLsAAAAwLMIuAAAADIuwCwAAAMMi7AIAAMCwCLsAAAAwLMIuAAAADIuwCwAAAMMi7AIAAMCwCLsAAAAwLMIuAAAADIuwCwAAAMMi7AIAAMCwCLsAAAAwLMIuAAAADIuwCwAAAMMi7AIAAMCwCLsAAAAwLMIuAAAADIuwCwAAAMMi7AIAAMCwCLsAAAAwLMIuAAAADIuwCwAAAMNyc8RG4uLiNH36dB08eFA+Pj7q1q2bRo8eLQ8PjwLXOXfunFasWKHdu3fr1KlTqlChgpo0aaKxY8cqICDA2m///v0aOHBgnvU7deqk+fPn22V/AAAAcGuwe9hNSkrSoEGDVLt2bUVHRyshIUGzZs1Senq6Jk+eXOB6P//8s7Zt26aePXvqvvvu019//aVFixapd+/e2rBhgypXrmzTf+bMmQoMDLQuV6pUyW77BAAAgFuD3cPumjVrlJqaqoULF6pixYqSpOzsbL366quKiopS1apV813vwQcf1KZNm+Tm9v8lPvDAA2rTpo0+//xzDR482KZ/3bp11bBhQ7vtBwAAAG49dp+zu2vXLoWFhVmDriRFRkYqJydHu3fvLnA9X19fm6ArSdWqVVPlypV17tw5e5ULAAAAA7H7md34+Hj17NnTps3X11dVqlRRfHx8icb69ddfdfHiRQUFBeV5b/jw4UpMTFSVKlXUuXNnPf/88/Ly8ip13RaLRWlpaaVevzQyMjIcuj0Ajpedne3wY4szcVwDjM9ZxzWLxSKTyVRkP7uH3eTkZPn6+uZp9/PzU1JSUrHHsVgsmj59uvz9/dW5c2dre4UKFTR06FA1adJEnp6e2rdvn5YtW6b4+HgtXry41HWbzWYdOXKk1OuXxtmzZx26PQCOl5qa6vBjizNxXAOMz5nHtcJudpDLIXdjKAvR0dHat2+f3nvvPXl7e1vbQ0JCFBISYl0OCwuTv7+/pk2bpkOHDik0NLRU23N3d1edOnVuuO6SSE9Pd+j2ADiej4+P6tev7+wyHIbjGmB8zjqunThxolj97B52fX19lZKSkqc9KSlJfn5+xRrj448/1ttvv61//vOfCgsLK7J/ZGSkpk2bpsOHD5c67JpMJptQ7Qienp4O3R4Ax3N1dXX4scWZOK4Bxues41pxpjBIDrhALTAwMM/c3JSUFJ0/f97mVmEF2bZtm6ZOnapRo0apV69e9ioTAAAABmT3sBsREaE9e/YoOTnZ2rZ582a5uLgoPDy80HX379+vsWPHqnfv3ho5cmSxt7lx40ZJ4lZkAAAAtzm7T2Po06ePVq1apZEjRyoqKkoJCQmaM2eO+vTpY3OP3UGDBun333/Xtm3bJF196trIkSNVu3ZtdevWTT/88IO1b+XKlVWrVi1J0rhx43TXXXcpJCTEeoHaihUr9PDDDxN2AQAAbnN2D7t+fn5auXKlXnvtNY0cOVI+Pj7q1auXxowZY9MvJydH2dnZ1uUff/xRKSkpSklJUd++fW36Pvroo5o1a5akqw+TiImJ0bJly2Q2mxUQEKCnn35aw4cPt/euAQAA4CbnkLsxBAUFacWKFYX2WbVqlc1yjx491KNHjyLHjoqKUlRU1I2UBwAAAIOy+5xdAAAAwFkIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAsh4TduLg4PfXUU2rUqJHCw8M1Z84cZWZmFrmexWLRkiVL1KZNG4WGhurxxx/XDz/8kKdfQkKCnnvuOd1///1q2rSpXnnlFV2+fNkOewIAAIBbiZu9N5CUlKRBgwapdu3aio6OVkJCgmbNmqX09HRNnjy50HXfffddLViwQOPGjVNwcLA++OADDR48WF988YVq1qwpSTKbzRo6dKgkae7cuUpPT9fs2bP1wgsvaPHixfbePbtIv/SXTu3YLZOrq1zcXOXi6mp9bfP9+nZXV5ncXOXi6vZ/3/+/zWQyOXu3gFuKxWKRJTtHluxs5WRnX/2elW1dtr7OypYlO+tqn6x8+v5fW2bK7f0L+JkziXrvvd1yd3e1fnl4XP3u5ma7fP2Xh0f+fVxd+XASKImcHIvM5mxlZWXLbL76lZn5/6+v/crM/P9+BfX56680Z+9Ssdg97K5Zs0apqalauHChKlasKEnKzs7Wq6++qqioKFWtWjXf9TIyMrR48WINHjxYTz75pCTpwQcfVMeOHbV06VJNnTpVkrRlyxYdP35csbGxCgwMlCT5+vpqyJAhOnTokEJDQ+29i2UuOyNTySfPlOmYJheXggPztYG6yD5uNu8XFLhNLi4lCtgWi0WWrOwy3Wfc3Er6S5glJ8c2UF4XLK2vC+yTlef9a8NrfkEVZefKFbOOHk0o0zFdXEz5huPCgnPBYdtFHh5u//eei9zd3f7vu6s8PNzk5layYxpQHBaLRdnZllKHzfzWKez9rKwcZ++yU9g97O7atUthYWHWoCtJkZGRmjJlinbv3q0ePXrku97333+vy5cvKzIy0trm4eGh9u3ba9u2bTbjBwcHW4OuJIWHh6tixYrauXPnLRV2g4KC9NRTTykzM1NZWVnKzMyU2Wy2fl2/nF/btcvZ1/xnbcnJuRoWzGaH7c/1Idp0XTi+Nhin/nle5supDqsNzufq5anyd1a7+nfTGjSz8pwRzX0ti8VptZpMJrm7u8vDw0Pu7u75fl3/XkHLlSpVctp+OEPdunUVFRVlc3y6/ntBrwv6nisnx6KMjCxlZGQ5ZF/c3HIDscv/heJrX+cN0lWr+ur++2s6pDY4359/JumHH84oKyvnupCZI7M5y/o6MzNLZnOONcjm5Djn2Obm5pbv8Sr3dUHf83t9bca7Gdk97MbHx6tnz542bb6+vqpSpYri4+MLXU+STYiVrgbClStXKj09XV5eXoqPj8/Tx2Qy6e677y50/KJYLBalpTn29Lynp6dCQkLKZKzs7GxlZGToypUrSktLU1pams3r3OXr309NTbUJyTfCkpWt7GKerXVxdy+TbeLWkZ2eoaT4kw7dpslkkpeXl7y9veXt7a1y5cpZXxfW5unpKTc3tzI7s+foY4szubu7q06dOmU2nsViUVZWljIyMpSWlqbU1FSb49q1X9e/Z77BX/azsnKUlVX09Sa5vLzc9emnB29om0BhXFxcbI5ZPj4+KleunHx8fGyOYde2lytXTu7u7nJxKdtpQM44rlkslmIdl+0edpOTk+Xr65un3c/PT0lJSYWu5+HhIU9PT5t2X19fWSwWJSUlycvLS8nJyapQoUKJxy+K2WzWkSNHSr1+aaSkpOj3339Xdna2srOzlZWVle/3wt7L/W5x4lkwmUz5nsktbOqDR0VfVQqs7bya4VBX/vpLSb+dlrJzlJM7L9ZmSsHVObDXT0mw5NzYR3AWi8X6S97FixdLvL6rq6vc3Nxsvhf0uqDvFSpUUEBAwA3tx63kypUrSkxMtDlG5XfcKuqYdn0fZx3jXF1N153RvXa6w/+f2XVzc1Fc3AWlpzvu0zQ4X4UKnqpb119ZWbZncAuaXnCjcnJylJqaqtTUkn0ymntMyu84df3xrLA+ud+9vb2d9qmVh4dHkX3sHnZvVWV9NqI4Dh48qN27d5f5uAVNI8jTft1FbXmDqlveeb359S3j3xZhPD7+VeTjX6XE6129aCz/ubb5heXrLxS7/uKy/C82sx332ukTub9s3og6dero4YcfvqExbiUHDx7Upk2b7LqNks7VzZ2fW/Bc3YL7uLiU7FqEzEzmft9OPDyKfy3C1U8p/n/KQ3Hm5d5In2unS+QeyzIyMspkv+vUqaOoqKgyGaskTpw4Uax+dg+7vr6+SklJydOelJQkPz+/QtfLzMxURkaGzdnd5ORkmUwm67q+vr753mYsKSlJ1atXL3XdJpNJ3t7epV6/NHL309XLUxUCqhfrjOj1QTW/AMpFFTAKk8kkk5ubXNwc93u6JScnn3nEWflf3JYbvPO54C0zOVlp5y5az4LcLnKPa56ebrrzTr9i3WWhJBed3cwXjplMJnl6ck4J+bt6LcDVv8eOkJ2dN1gXdta5OBfJJSVd0YULqU47rhX3377d/xUGBgbmmTubkpKi8+fP55lre/16kvTrr7+qXr161vb4+Hjdeeed8vLysvb75ZdfbNa1WCz69ddfFR4eXla74VBelSqqRqtmzi4DgK7eycTVw0XSjc0rT4w/qbRzJZ86YRS1alVSVFQrZ5cB3LZcXV3k6uoiL6+yu0bm4MHT+uCDb8tsPHux++fNERER2rNnj5KTk61tmzdvlouLS6Fh9IEHHlD58uVtPv4ym83aunWrIiIibMY/evSofvvtN2vb3r17lZiYqNatW5ftzgAAAOCWYvew26dPH/n4+GjkyJH6+uuvtXbtWs2ZM0d9+vSxucfuoEGD1L59e+uyp6enoqKitGzZMq1cuVJ79+7VCy+8oMTERA0ZMsTar0OHDqpbt66ee+457dixQ7GxsZo4caL1qWsAAAC4fdl9GoOfn59Wrlyp1157TSNHjpSPj4969eqlMWPG2PTLycnJc+HHsGHDZLFYtGzZMl26dEn169fX0qVLrU9Pk65eSPbee+9p+vTpGjt2rNzc3NS+fXtNnDjR3rsGAACAm5xDZs4HBQVpxYoVhfZZtWpVnjaTyaSoqKgir/CrWrWqoqOjb6REAAAAGBD3iAIAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWIRdAAAAGBZhFwAAAIZF2AUAAIBhEXYBAABgWA4Ju19++aW6du2qhg0bqkOHDlq7dm2R6xw6dEgTJkxQ+/btdd999+nvf/+75s6dq7S0NJt+0dHRCg4OzvO1evVqe+0OAAAAbhFu9t7AgQMH9Oyzz6pXr16aOHGi9u3bp1deeUU+Pj7q2LFjgett2rRJJ0+e1NChQ1W7dm2dOHFCCxYs0I8//qj333/fpq+Xl5dWrlxp01azZk277A8AAABuHXYPu4sWLVJoaKimTZsmSWrevLlOnz6tBQsWFBp2hw0bpsqVK1uXmzVrJl9fX40bN06HDx9WgwYNrO+5uLioUaNGdtsHAAAA3JrsOo0hMzNT+/fvzxNqO3XqpLi4OJ05c6bAda8NurlCQkIkSefOnSvbQgEAAGBIdg27p06dktlsVmBgoE17UFCQJCk+Pr5E43333XeSlGe89PR0NW/eXCEhIerUqZM+/vjjG6gaAAAARmHXaQxJSUmSJF9fX5v23OXc94vj0qVLio6OVrt27VS7dm1re61atTRu3DiFhIQoIyNDMTEx+sc//qGUlBQNGTKk1LVbLJY8F8PZW0ZGhkO3B8DxsrOzHX5scSaOa4DxOeu4ZrFYZDKZiuxX4rCbkpJSrGkEZXmBmNls1tixYyVJU6dOtXmvW7duNstt2rSR2WzWokWLNHDgQLm7u5d6m0eOHCnVuqV19uxZh24PgOOlpqY6/NjiTBzXAONz5nHNw8OjyD4lDrubN2/WpEmTiuwXGxsrPz8/SVcD8rWSk5Mlyfp+YSwWiyZOnKhDhw7pww8/lL+/f5HrREZGasuWLTp16pR1ykRJubu7q06dOqVat7TS09Mduj0Ajufj46P69es7uwyH4bgGGJ+zjmsnTpwoVr8Sh93evXurd+/exeqbmZkpd3d3xcfHq1WrVtb23Lm618+9zc/s2bO1adMmvfvuu6pXr15Jyy01k8kkb29vh21Pkjw9PR26PQCO5+rq6vBjizNxXAOMz1nHteJMYZDsfIGah4eHmjVrpi1btti0x8bGKigoSDVq1Ch0/SVLlmjFihWaNWuWwsLCir3d2NhY+fr6qlatWqWqGwAAAMZg9/vsjhgxQgMHDtTUqVMVGRmp/fv3a8OGDZo/f75Nv5CQEHXv3l0zZsyQJMXExGju3Lnq2rWratSooR9++MHat1atWtZbk/Xo0UPdu3dXYGCg0tPTFRMTo61bt2rixImlnq8LAAAAY7B72G3cuLGio6P15ptv6tNPP9Wdd96p6dOnKzIy0qZfdna2cnJyrMu7d++WJK1fv17r16+36Ttz5kz16NFD0tXgu2LFCl24cEEmk0n33HOPXn/9dXXt2tXOewYAAICbnd3DriS1a9dO7dq1K7TPsWPHbJZnzZqlWbNmFTn2m2++eSOlAQAAwMDsOmcXAAAAcCbCLgAAAAyLsAsAAADDIuwCAADAsAi7AAAAMCzCLgAAAAyLsAsAAADDIuwCAADAsAi7AAAAMCzCLgAAAAyLsAsAAADDIuwCAADAsAi7AAAAMCzCLgAAAAyLsAsAAADDIuwCAADAsAi7AAAAMCzCLgAAAAyLsAsAAADDIuwCAADAsAi7AAAAMCzCLgAAAAyLsAsAAADDIuwCAADAsAi7AAAAMCzCLuBEFotFFovF2WUAAGBYbs4uAHllJCbpzFf7nV0G7Mxisejy739KksrfWU0mk8nJFcGeMlMuO7sEALgtEXZvQllX0pUY95uzy4ADJcWfdHYJAAAYEmH3JlKjRg11797d2WXAQbKysrRhwwZJUpcuXeTmxj/H20HlypWdXQIA3Fb43/UmUq1aNVWrVs3ZZcBBMjIyrGG3bdu28vT0dHJFAHDjcq9DYGoWbhaEXQCA3Z0+/ZcWLvyPs8uAnVksFv35Z4okqVq1CgReg7t8OdPZJRQLYRcAYHfp6Vn67bdLzi4DDnTy5F/OLgGQRNgFANhRUFCQhg8f7uwy4CBms1nLly+XJD311FNyd3d3ckVwhPLlyzu7hEIRdgEAdlOpUiVVqlTJ2WXAQTIyMqyvGzZsyLUIuCnwUAkAAAAYFmEXAAAAhkXYBQAAgGERdgEAAGBYhF0AAAAYFmEXAAAAhkXYBQAAgGERdgEAAGBYhF0AAAAYFmEXAAAAhkXYBQAAgGERdgEAAGBYhF0AAAAYFmEXAAAAhkXYBQAAgGERdgEAAGBYDgm7X375pbp27aqGDRuqQ4cOWrt2bZHrnDlzRsHBwXm+HnvssTx9v//+ez3++OMKDQ3VQw89pCVLlshisdhjVwAAAHALcbP3Bg4cOKBnn31WvXr10sSJE7Vv3z698sor8vHxUceOHYtcf+zYsWrWrJl12cfHx+b9kydPasiQIQoPD9fo0aN17NgxvfHGG3J1ddWQIUPKfH8AAABw67B72F20aJFCQ0M1bdo0SVLz5s11+vRpLViwoFhh96677lKjRo0KfH/p0qWqVKmS5s2bJw8PD4WFhenSpUt65513NGDAAHl4eJTVrgAAAOAWY9dpDJmZmdq/f3+eUNupUyfFxcXpzJkzN7yNXbt2qV27djahtlOnTkpOTtbBgwdveHwAAADcuuwadk+dOiWz2azAwECb9qCgIElSfHx8kWNMnTpV9evXV1hYmCZNmqTExETre2lpafrjjz/yjB8YGCiTyVSs8QEAAGBcdp3GkJSUJEny9fW1ac9dzn0/Px4eHurbt69atmwpX19f/fjjj3rnnXd0+PBhffLJJ3J3d1dKSkq+43t4eKhcuXKFjl8Ui8WitLS0Uq8PFCUzM9P6+sqVK8rOznZiNQBw4ziuwZEsFotMJlOR/UocdlNSUnTu3Lki+9WsWbOkQ9vw9/fX1KlTrctNmzZV3bp1FRUVpW3btqlTp043NH5RzGazjhw5Ytdt4PZmNputr48ePSp3d3cnVgMAN47jGhytONdmlTjsbt68WZMmTSqyX2xsrPz8/CTJegY2V3JysiRZ3y+u1q1by9vbWz///LM6deqkChUq5Dt+Zmamrly5UuLxr+Xu7q46deqUen2gKNeeAalXrx4XUwK45XFcgyOdOHGiWP1KHHZ79+6t3r17F6tvZmam3N3dFR8fr1atWlnbc+fSXj/XtqS8vb1VvXr1PHNzf/31V1kslhsa32Qyydvb+4bqAwrj6upqfV2uXDl5eno6sRoAuHEc1+BIxZnCINn5AjUPDw81a9ZMW7ZssWmPjY1VUFCQatSoUaLxduzYobS0NDVs2NDaFhERoe3bt9t8dBIbGytfX1/df//9N7YDAAAAuKXZ/T67I0aM0MCBAzV16lRFRkZq//792rBhg+bPn2/TLyQkRN27d9eMGTMkSbNmzZLJZFKjRo3k6+urQ4cOafHixWrQoIEefvhh63pDhgxRTEyMXnjhBfXt21e//PKLli5dqjFjxvDxCQAAwG3O7mG3cePGio6O1ptvvqlPP/1Ud955p6ZPn67IyEibftnZ2crJybEuBwUFafXq1fr444+Vnp6uqlWrqlevXho1apTc3P6/7LvuuktLly7VrFmzNHz4cFWuXFmjRo3S4MGD7b1rAAAAuMnZPexKUrt27dSuXbtC+xw7dsxmuSRzgx944AF9/PHHpa4PAAAAxmTXObsAAACAMxF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABiWQ8Lul19+qa5du6phw4bq0KGD1q5dW+Q60dHRCg4Ozvdr8uTJRfZbvXq1PXcJAAAAtwA3e2/gwIEDevbZZ9WrVy9NnDhR+/bt0yuvvCIfHx917NixwPV69+6tVq1a2bR9++23euONNxQREWHT7uXlpZUrV9q01axZs+x2AgAAALcku4fdRYsWKTQ0VNOmTZMkNW/eXKdPn9aCBQsKDbvVqlVTtWrVbNrWrFkjPz+/PGHXxcVFjRo1KvPaAQAAcGuz6zSGzMxM7d+/P0+o7dSpk+Li4nTmzJlij5WRkaFt27apQ4cO8vDwKOtSAQAAYEB2PbN76tQpmc1mBQYG2rQHBQVJkuLj41WjRo1ijbVjxw5dvnxZXbp0yfNeenq6mjdvruTkZNWuXVtPPvmkHnvssRuq3WKxKC0t7YbGAAqTmZlpfX3lyhVlZ2c7sRoAuHEc1+BIFotFJpOpyH52DbtJSUmSJF9fX5v23OXc94tjw4YNqlq1qpo0aWLTXqtWLY0bN04hISHKyMhQTEyM/vGPfyglJUVDhgwpde1ms1lHjhwp9fpAUcxms/X10aNH5e7u7sRqAODGcVyDoxXn0/4Sh92UlBSdO3euyH5leYFYcnKydu7cqf79+8vFxXbmRbdu3WyW27RpI7PZrEWLFmngwIGl/ofm7u6uOnXqlLpmoCjXngGpV68e03MA3PI4rsGRTpw4Uax+JQ67mzdv1qRJk4rsFxsbKz8/P0lXA/K1kpOTJcn6flG2bNmizMxMPfLII8XqHxkZqS1btujUqVPWKRMlZTKZ5O3tXap1geJwdXW1vi5Xrpw8PT2dWA0A3DiOa3Ck4kxhkEoRdnv37q3evXsXq29mZqbc3d0VHx9vcxux+Ph4Scozl7cgGzZsUGBgoEJCQkpaLgAAAG5jdr0bg4eHh5o1a6YtW7bYtMfGxiooKKhYF6edO3dO33zzTb4XphUkNjZWvr6+qlWrVolrBgAAgHHY/T67I0aM0MCBAzV16lRFRkZq//792rBhg+bPn2/TLyQkRN27d9eMGTNs2mNjY5WTk1PgFIYePXqoe/fuCgwMVHp6umJiYrR161ZNnDiRifEAAAC3ObuH3caNGys6OlpvvvmmPv30U915552aPn26IiMjbfplZ2crJycnz/oxMTEKDQ0t8CxtrVq1tGLFCl24cEEmk0n33HOPXn/9dXXt2tUu+wMAAIBbh93DriS1a9dO7dq1K7TPsWPH8m1fu3Ztoeu9+eabpS0LAAAABmfXObsAAACAMxF2AQAAYFiEXQAAABiWQ+bsAgBwu7FYLDZPFLsdZGRk5Pv6duHh4VHsBx3AcQi7AACUMYvFonnz5lkfonQ7mjBhgrNLcLjAwECNHTuWwHuTYRoDAAAADIszuwAAlDGTyaSxY8fedtMYpKtntSXdlmc3mcZwcyLsAgBgByaTSZ6ens4uA7jtMY0BAAAAhkXYBQAAgGExjQE3BW7Rwy16AACwB8IunI5b9HCLHgAA7IVpDAAAADAszuzC6bhFD7foAQDAXgi7uClwix4AAGAPTGMAAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYbk5u4CbkdlslsVi0U8//eTsUgAAAJCPzMxMmUymIvsRdvNRnB8cAAAAnMdkMhUrs5ksFovFAfUAAAAADsecXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXQAAABgWYRcAAACGRdgFAACAYRF2AQAAYFiEXcAJ4uLi9NRTT6lRo0YKDw/XnDlzlJmZ6eyyAKDUTp48qcmTJ6tbt24KCQlRly5dnF0SIElyc3YBwO0mKSlJgwYNUu3atRUdHa2EhATNmjVL6enpmjx5srPLA4BSOX78uHbu3Kn77rtPOTk5slgszi4JkETYBRxuzZo1Sk1N1cKFC1WxYkVJUnZ2tl599VVFRUWpatWqzi0QAEqhbdu2evjhhyVJ48eP1+HDh51cEXAV0xgAB9u1a5fCwsKsQVeSIiMjlZOTo927dzuvMAC4AS4uRArcnPibCThYfHy8AgMDbdp8fX1VpUoVxcfHO6kqAACMibALOFhycrJ8fX3ztPv5+SkpKckJFQEAYFyEXQAAABgWYRdwMF9fX6WkpORpT0pKkp+fnxMqAgDAuAi7gIMFBgbmmZubkpKi8+fP55nLCwAAbgxhF3CwiIgI7dmzR8nJyda2zZs3y8XFReHh4U6sDAAA4+E+u4CD9enTR6tWrdLIkSMVFRWlhIQEzZkzR3369OEeuwBuWVeuXNHOnTslSWfPntXly5e1efNmSVLTpk1VuXJlZ5aH25jJwiNOAIeLi4vTa6+9poMHD8rHx0fdunXTmDFj5OHh4ezSAKBUzpw5o3bt2uX73vvvv69mzZo5uCLgKsIuAAAADIs5uwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAwyLsAgAAwLAIuwAAADAswi4AAAAMi7ALAAAAw/pfxTsWSQE20ZwAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "data_a = result_df[f'Predicted Mode {i+1}']\n",
        "data_b = result_df[f'True Label Mode {i+1}']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=[data_a, data_b], notch=True, palette=\"Set3\")\n",
        "plt.title(f'Box Plot - Mode {i+1}')\n",
        "\n",
        "# Perform Mann-Whitney U test\n",
        "stat, p_value = mannwhitneyu(data_a, data_b)\n",
        "if p_value < 0.05:\n",
        "    result = \"different\"\n",
        "else:\n",
        "    result = \"similar\"\n",
        "\n",
        "plt.text(0.95, 0.85, f'p-value: {p_value:.4f}\\nDistribution: {result}',\n",
        "        transform=plt.gca().transAxes, ha='right', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16w5i7m6EMav"
      },
      "source": [
        "##GEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkdY1NBpJK2Q"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Add, Activation, BatchNormalization, \\\n",
        "    AveragePooling2D, Concatenate\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import metrics, regularizers, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from colorsys import hls_to_rgb\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import h5py\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "#from .colorconv import color_circle, colorize\n",
        "import functools\n",
        "import shutil\n",
        "import os, time, h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#from .labels import gen_label\n",
        "#from . import funcs, colorconv\n",
        "import h5py, json, os, time, zipfile, glob, shutil\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import Callback, TensorBoard\n",
        "#from .labels import gen_label\n",
        "#from .generator import generate_from_labels, Generator, configure, add_noise, infieldap\n",
        "#from .training import get_data_h5, loss_new, loss, acc_pass\n",
        "from PIL import Image, ImageOps\n",
        "from numpy import asarray\n",
        "from tensorflow.keras.models import Model\n",
        "#from .funcs import plot_intensity_row, plot_intensity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H8Cr1FOEKcL"
      },
      "outputs": [],
      "source": [
        "def gen_label(mode, n_bases, num, p=None):\n",
        "    def all_zero():  # 0\n",
        "        return np.zeros(( num, n_bases))\n",
        "\n",
        "    def uniform_random_1():  # 1\n",
        "        a_nm_vec = np.zeros((num, n_bases))\n",
        "        a_nm_vec[:, 0:2] = np.random.uniform(-2, 2, (num, 2))\n",
        "        for j in range(2, n_bases):\n",
        "            a_nm_vec[:, j] = np.random.uniform(-0.8, 0.8, num)\n",
        "        return a_nm_vec\n",
        "\n",
        "    def uniform_random_2():  # 2\n",
        "        # Without translation bases\n",
        "        a_nm_vec = np.zeros((num, n_bases))\n",
        "        for j in range(2, n_bases):\n",
        "            a_nm_vec[:, j] = np.random.uniform(-0.8, 0.8, num)\n",
        "        return a_nm_vec\n",
        "\n",
        "    def one_base_dominant():  # 3\n",
        "        a_nm_vec = np.zeros((num, n_bases))\n",
        "        dom_base = np.random.randint(0, n_bases, num)\n",
        "        dom_amp, amp = 2, 0.1\n",
        "        for i in range(num):\n",
        "            a_nm_vec[i, :n_bases] = np.random.uniform(-amp, amp, n_bases)\n",
        "            a_nm_vec[i, dom_base[i]] = np.random.uniform(-dom_amp, dom_amp)\n",
        "        return np.expand_dims(a_nm_vec, 0)\n",
        "\n",
        "    def mixed9010():  # 4\n",
        "        uf = uniform_random_1()[:, :(num * 9) // 10]\n",
        "        bd = one_base_dominant()[:, :(num * 1) // 10]\n",
        "        a_nm_vec = np.concatenate([uf, bd], 1)\n",
        "        np.random.shuffle(a_nm_vec)\n",
        "        return a_nm_vec\n",
        "\n",
        "    def ladder():  # 5\n",
        "        nrange = np.linspace(-2, 2, 101)\n",
        "        asi = []\n",
        "        for i in range(n_bases):\n",
        "            a_i = np.zeros((len(nrange), n_bases))\n",
        "            a_i[:, i] = nrange\n",
        "            asi.append(a_i)\n",
        "        a_nm_vec = np.stack(asi, 0)\n",
        "        return a_nm_vec\n",
        "\n",
        "    def uniform_random_1_add_noise(): # 6  #add noise by concat axis 1\n",
        "        a_nm_vec = uniform_random_1()\n",
        "        return np.concatenate([a_nm_vec, np.random.uniform(-p[\"noise_z\"], p[\"noise_z\"], (num, 10))], 1)\n",
        "\n",
        "    def uniform_random_1_weak(): # 7\n",
        "        a_nm_vec = np.random.uniform(-2,2,(num,2))\n",
        "        a_nm_vec2 = np.random.uniform(-0.8, 0.8, (num, n_bases-12))\n",
        "        a_nm_vec3 = np.random.uniform(-p[\"noise_z\"], p[\"noise_z\"], (num, 10))\n",
        "        return np.concatenate([a_nm_vec, a_nm_vec2, a_nm_vec3], 1)\n",
        "\n",
        "    def mix_1_2():\n",
        "        a_nm_vec = np.zeros((num, n_bases))\n",
        "\n",
        "        uniform_range = np.random.uniform(-0.8, 0.8, num)\n",
        "\n",
        "        if n_bases % 2 == 0:\n",
        "            a_nm_vec[:, 0:2] = np.random.uniform(-2, 2, (num, 2))\n",
        "            a_nm_vec[:, 2:] = uniform_range[:, np.newaxis]\n",
        "        else:\n",
        "            a_nm_vec[:, 2:] = uniform_range[:, np.newaxis]\n",
        "\n",
        "        return a_nm_vec\n",
        "\n",
        "    f_list = [all_zero, uniform_random_1, uniform_random_2, one_base_dominant,\n",
        "              mixed9010, ladder, uniform_random_1_add_noise, uniform_random_1_weak, mix_1_2]\n",
        "\n",
        "    return f_list[mode]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5WaBDioDw-y",
        "outputId": "50566bc1-af8c-4e6d-aa36-20ed1d21fcf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical devices cannot be modified after being initialized\n"
          ]
        }
      ],
      "source": [
        "gen_label = gen_label\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "\n",
        "\n",
        "class Zernike:\n",
        "    def __init__(self, theta, rho):\n",
        "        self.theta = theta\n",
        "        self.rho = rho\n",
        "        self.Z = dict()\n",
        "\n",
        "    def Rln(self, n, l): # Radial Polynomial\n",
        "        fac = np.math.factorial\n",
        "        R_terms = [((-1) ** k * fac(n - k)) * self.rho ** (n - 2 * k)\n",
        "                   / (fac(k) * fac((n + l) // 2 - k) * fac((n - l) // 2 - k)) for k in range((n - l) // 2 + 1)]\n",
        "        R_terms = np.stack(R_terms, 0)\n",
        "        return np.sum(R_terms, 0)\n",
        "\n",
        "    def Z_j(self, j): # Mode j Zernike\n",
        "        def nlf(j): # Ordering Zernike coefficients\n",
        "            n = np.ceil((np.sqrt(1 + 8 * j)) / 2 - 1.5)\n",
        "            k = j - (n + 1) * n // 2\n",
        "            l = (-1) ** (k + (n + 1) // 2) * (n % 2 + ((k + (n + 1) % 2 - 1) // 2) * 2)\n",
        "            return int(n), int(l)\n",
        "\n",
        "        if not j in self.Z.keys():\n",
        "            n, l = nlf(j)\n",
        "            if l < 0:\n",
        "                self.Z[j] = self.Rln(n, -l) * np.sin(l * self.theta) * (2 * n + 2) ** 0.5\n",
        "            elif l > 0:\n",
        "                self.Z[j] = self.Rln(n, l) * np.cos(l * self.theta) * (2 * n + 2) ** 0.5\n",
        "            else:\n",
        "                self.Z[j] = self.Rln(n, l) * (n + 1) ** 0.5\n",
        "\n",
        "    def get_Z(self, coeffs):\n",
        "        n_bases = coeffs.shape[1]\n",
        "        for i in range(n_bases):\n",
        "            self.Z_j(i + 1)\n",
        "        zs = tf.stack([self.Z[i + 1] for i in range(n_bases)], 0)[tf.newaxis, :]\n",
        "        return tf.reduce_sum(zs * coeffs[:, :, tf.newaxis, tf.newaxis], 1)\n",
        "\n",
        "class Generator: # Simulation\n",
        "    def __init__(self, dtype=tf.float32):\n",
        "        self.limited = False # True if limited memory capacity else risking OOM\n",
        "        self.initialized = False\n",
        "        self.dtype = dtype\n",
        "        self.wavelength = 532e-9  # laser wavelength[m]\n",
        "        self.wave_k = None\n",
        "\n",
        "        self.Dist = 1e3  # Distance to receiver[m](far - field for 2mm beam) DEFAULT 1000 m\n",
        "        DiagHolo = 2e-3  # Diameter of aperture on SLM(beam / hologram diameter) [m]\n",
        "        xb = 2e-2  # spacing in OUTPUT grid(dist=1e3, 2mm beam, r0 = 7cm)\n",
        "        self.gridA = 40  # Size for checking far-field  needs to be an even number  grid will be 2 * gridA X 2 * gridA\n",
        "        self.radius = 1  # radius of aperture to apply to hologram & input intensity\n",
        "        self.TIP = 0  # Additional Z2 for spatial separation of 0th & 1st order diffraction\n",
        "        self.TILT = 0  # Additional Z3 for spatial separation of 0th & 1st order diffraction\n",
        "        self.noise = lambda x: 0\n",
        "\n",
        "        self.C, self.D = [None, None]\n",
        "        self.E, self.F = [None, None]\n",
        "\n",
        "        self.field_func = infieldap(0)\n",
        "        self.Aperture = None\n",
        "        self.X, self.Y = None, None\n",
        "        self.theta = None\n",
        "        self.rho = None\n",
        "        self.zernike = None\n",
        "        self.inFieldAp = None\n",
        "        self.cnst_tensor = None\n",
        "        self.unperturbed = None\n",
        "\n",
        "    def initialize(self):\n",
        "        dtype = self.dtype\n",
        "        # ** ** ** ** BEAM DIAMETER = 2mm ** ** ** ** *\n",
        "        self.wave_k = 2 * np.pi / self.wavelength\n",
        "        DiagHolo = 2e-3  # Diameter of aperture on SLM(beam / hologram diameter) [m]\n",
        "        xb = 2e-2  # spacing in OUTPUT grid(dist=1e3, 2mm beam, r0 = 7cm)\n",
        "\n",
        "        # ** ** ** ** ** ** Parameters ** ** ** ** ** **\n",
        "\n",
        "        xa = DiagHolo / (2 * self.gridA)  # spacing between pixels in INPUT grid\n",
        "        xrange = 1\n",
        "\n",
        "        # INPUT grid\n",
        "        aa = np.linspace(-xa * self.gridA, xa * self.gridA, 2 * self.gridA)  # length of input grid\n",
        "        self.C, self.D = np.meshgrid(aa, aa)  # input grid\n",
        "        l1 = len(self.C)\n",
        "\n",
        "        # OUTPUT grid\n",
        "        bb = np.linspace(-xb * self.gridA, xb * self.gridA, 2 * self.gridA)  # length of output grid\n",
        "        self.E, self.F = np.meshgrid(bb, bb)  # output grid\n",
        "        l2 = len(self.E)\n",
        "\n",
        "        xx = np.arange(-xrange, xrange + ((2 * xrange) / (l1 - 1)),\n",
        "                       ((2 * xrange) / (l1 - 1)))\n",
        "        [X, Y] = np.meshgrid(xx, xx)\n",
        "        [theta, rho] = np.arctan2(Y, X), np.sqrt(X ** 2 + Y ** 2)  # This makes theta[-pi, pi]\n",
        "        x = len(rho)\n",
        "        Aperture = np.zeros((x, x))\n",
        "        for k in range(x):\n",
        "            for j in range(x):\n",
        "                if rho[k, j] <= self.radius:\n",
        "                    Aperture[k, j] = 1\n",
        "\n",
        "        self.X, self.Y = tf.cast(X, dtype), tf.cast(Y, dtype)\n",
        "        self.Aperture = tf.cast(Aperture, dtype)\n",
        "        self.theta = tf.cast(theta, dtype)\n",
        "        self.rho = tf.cast(rho, dtype)\n",
        "        self.zernike = Zernike(self.theta, self.rho)\n",
        "        self.inFieldAp = tf.multiply(self.field_func(self.rho, self.theta), self.Aperture) ** 0.5\n",
        "\n",
        "        self.Cnst_tensor()\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "        self.unperturbed = self.generate_imgs(np.zeros((1, 2)))[0]\n",
        "\n",
        "    def set_Infield(self, field_func):\n",
        "        self.field_func = field_func\n",
        "        if self.initialized:\n",
        "            self.initialize()\n",
        "\n",
        "    def Phasemask(self, coeffs):\n",
        "        out = []\n",
        "        n = 1000\n",
        "        for i in range(coeffs.shape[0] // n + 1):\n",
        "            coeff = coeffs[i * n: (i + 1) * n]\n",
        "            PhM = self.zernike.get_Z(coeff) # Generate function from Zernike Polynomials\n",
        "            PhM_turbAp = tf.cast(PhM * self.Aperture[tf.newaxis, :, :], tf.dtypes.complex128)\n",
        "            ei_PhM = tf.exp(1.j * PhM_turbAp)\n",
        "            ei_PhM = tf.cast(ei_PhM, tf.dtypes.complex64)\n",
        "            out.append(ei_PhM)\n",
        "        return tf.concat(out, 0)\n",
        "        # return tf.cast(ei_PhM, tf.dtypes.complex64)\n",
        "\n",
        "    def Cnst_tensor(self):\n",
        "\n",
        "        inFieldAp = tf.cast(self.inFieldAp, tf.dtypes.complex128) # Input field through the aperture\n",
        "\n",
        "        X = self.E[:, :, tf.newaxis, tf.newaxis] # Output space\n",
        "        Y = self.F[:, :, tf.newaxis, tf.newaxis]\n",
        "        x = self.C[tf.newaxis, tf.newaxis, :, :] # Input space\n",
        "        y = self.D[tf.newaxis, tf.newaxis, :, :]\n",
        "\n",
        "        diffr_tensor = self.Dist ** 2 + (X - x) ** 2 + (Y - y) ** 2\n",
        "        ei_diffr_tensor = tf.exp(1.j * self.wave_k * diffr_tensor ** 0.5) # Path difference\n",
        "\n",
        "        if self.limited:\n",
        "            c1 = []\n",
        "            for i1 in range(2 * self.gridA):\n",
        "                c2 = []\n",
        "                for i2 in range(2 * self.gridA):\n",
        "                    c3 = inFieldAp * ei_diffr_tensor[i1, i2] / diffr_tensor[i1, i2]\n",
        "                    c2.append(tf.cast(c3, tf.dtypes.complex64))\n",
        "                c1.append(tf.stack(c2))\n",
        "            self.cnst_tensor = tf.stack(c1)\n",
        "        else:\n",
        "            inFieldAp = inFieldAp[tf.newaxis, tf.newaxis, :, :]\n",
        "            self.cnst_tensor = inFieldAp * ei_diffr_tensor / diffr_tensor\n",
        "            self.cnst_tensor = tf.cast(self.cnst_tensor, tf.dtypes.complex64)\n",
        "\n",
        "    def generate_imgs(self, coeffs):\n",
        "        if not self.initialized:\n",
        "            self.initialize()\n",
        "        t = time.time()\n",
        "        n_samples, n_bases = tf.shape(coeffs)\n",
        "        coeffs = tf.concat([tf.zeros((n_samples, 1), dtype=self.dtype), coeffs], 1) # Adding Z0,0 mode\n",
        "\n",
        "        phasemask = self.Phasemask(coeffs)[:, tf.newaxis, tf.newaxis, :, :] # Generate phase distribution\n",
        "        phasemask = tf.cast(phasemask, tf.dtypes.complex64) # Cast to complex number\n",
        "\n",
        "        outbeamturb = []\n",
        "        for i in range(n_samples):\n",
        "            outfieldturb = phasemask[i] * self.cnst_tensor # Diffraction Equation\n",
        "            outfieldturb = tf.reduce_sum(outfieldturb, (2, 3)) # Integrate over aperture space\n",
        "            outbeamturb_i = tf.abs(outfieldturb) ** 2 # Intensity\n",
        "            outbeamturb_i += tf.cast(tf.abs(np.random.normal(0, self.noise(0), outbeamturb_i.shape)), outbeamturb_i.dtype) # White Noise\n",
        "            outbeamturb.append(outbeamturb_i)\n",
        "\n",
        "        outbeamturb = tf.stack(outbeamturb, 0) # Stacking output together\n",
        "        outbeamturb_norm = outbeamturb / tf.reduce_sum(outbeamturb, (1, 2), keepdims=True) # Normalization\n",
        "\n",
        "        print(f\"Total time: {time.time() - t}\")\n",
        "        print(f\"Average time per picture: {(time.time() - t) / n_samples.numpy()}\")\n",
        "\n",
        "        return outbeamturb_norm\n",
        "\n",
        "\n",
        "\n",
        "def add_noise(imgs, strength=1e-5):\n",
        "    noise = np.abs(np.random.normal(0, strength, imgs.shape))\n",
        "    imgs += tf.convert_to_tensor(noise, dtype=imgs.dtype)\n",
        "    return imgs / tf.reduce_sum(imgs, (1, 2), keepdims=True)\n",
        "\n",
        "\n",
        "def configure(o):\n",
        "    g = Generator()\n",
        "    g.gridA = o[\"width\"] // 2\n",
        "    if \"limited_memory\" not in o:\n",
        "        o[\"limited_memory\"] = False\n",
        "    if \"infield\" not in o:\n",
        "        o[\"infield\"] = [0]\n",
        "    elif type(o[\"infield\"]) == int:\n",
        "        o[\"infield\"] = [o[\"infield\"]]\n",
        "    if \"noise_gauss\" not in o:\n",
        "        o[\"noise_gauss\"] = 0\n",
        "    if o[\"noise_gauss\"] > 0:\n",
        "        g.noise = lambda x: o[\"noise_gauss\"]\n",
        "    else:\n",
        "        g.noise = lambda x: np.random.uniform(0,-o[\"noise_gauss\"])\n",
        "    g.limited = o['limited_memory']\n",
        "    g.field_func = infieldap(*o[\"infield\"])\n",
        "    g.initialize()\n",
        "    return g\n",
        "\n",
        "\n",
        "def generate(generator, order, id):\n",
        "    n_samples = [order[\"n_training\"], order[\"n_validation\"], order[\"n_test\"]]\n",
        "    labels = [gen_label(order[\"labels_mode\"][i], order[\"n_bases\"], n_samples[i], order) for i in range(3)]\n",
        "    with h5py.File(f\"logs/{id}.h5\", \"w\") as file:\n",
        "        for i, label in enumerate(labels):\n",
        "            file.create_dataset(f\"labels_{i}\", data=label[:, :order['n_bases']])\n",
        "            dataset = []\n",
        "            for num in np.arange(0, len(label), 10000):\n",
        "                dataset.append(generator.generate_imgs(label[num:num + 10000]).numpy())\n",
        "            file.create_dataset(f\"dataset_{i}\", data=np.concatenate(dataset))\n",
        "\n",
        "\n",
        "def generate_from_labels(generator, labels, n_bases, batch_size=64):\n",
        "    datasets = []\n",
        "    for i, label in enumerate(labels):\n",
        "        dataset = []\n",
        "        for num in np.arange(0, len(label), 10000):\n",
        "            dataset.append(generator.generate_imgs(label[num:num + 10000]))\n",
        "        dataset = tf.concat(dataset, 0)\n",
        "        label = tf.data.Dataset.from_tensor_slices(label[:, :n_bases])\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(tf.expand_dims(dataset, -1))\n",
        "        tf_dataset = tf.data.Dataset.zip((dataset, label)).batch(batch_size)\n",
        "        datasets.append(tf_dataset)\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def infieldap(mode, num=1, dr=0, waist=0.5):\n",
        "    def f2(rho, theta):\n",
        "        InField = tf.exp((-2 * (rho - dr * np.max(rho)) ** 2 / waist ** 2)) * np.cos(theta * num / 2) ** 2\n",
        "        return InField\n",
        "\n",
        "    def f(rho, theta):\n",
        "        dth = np.array([np.pi * 2 * i / num for i in range(num)])\n",
        "        dxs, dys = dr * np.cos(dth), dr * np.sin(dth)\n",
        "        X, Y = rho * np.cos(theta), rho * np.sin(theta)\n",
        "        dds = [((X - dx) ** 2 + (Y - dy) ** 2) ** 0.5 for dx, dy in zip(dxs, dys)]\n",
        "        InField = [tf.exp((-2 * dd ** 2 / waist ** 2)) for dd in dds]\n",
        "        InField = tf.reduce_sum(InField, 0)\n",
        "        # InField += np.abs(np.random.uniform(0, 1E-2, InField.shape))\n",
        "        return InField\n",
        "    return [f, f2][mode]\n",
        "\n",
        "def kolmogorov_phase():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ImQ0uBCJhM8"
      },
      "source": [
        "##CAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl58iODgJq6d",
        "outputId": "f8b0e319-c263-4caf-e6a4-95aebf629b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhHc-_IfRAyI"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "df = pandas.read_csv('/content/drive/MyDrive/OpticsML/ssim_scores_improved.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzGUt1O_kIFe",
        "outputId": "52e824ff-71fd-4ed9-a3a4-b7521032d934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSIM less than 0.7: 0\n",
            "SSIM between 0.7-0.8: 0\n",
            "SSIM more than 0.8 but equal or less than 0.9: 26\n",
            "SSIM more than 0.9: 19974\n"
          ]
        }
      ],
      "source": [
        "print('SSIM less than 0.7:',sum(df['SSIM Score'] < 0.70))\n",
        "print('SSIM between 0.7-0.8:', sum((df['SSIM Score']>=0.7) & (df['SSIM Score'] <= 0.80)))\n",
        "print('SSIM more than 0.8 but equal or less than 0.9:',sum((df['SSIM Score']>0.8) & (df['SSIM Score'] <= 0.90)))\n",
        "print('SSIM more than 0.9:',sum(df['SSIM Score'] > 0.90))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFFj7mwalKS-",
        "outputId": "b21e2806-5c0a-4e07-ab7a-4b8672ead02c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0.990298\n",
              "1        0.960496\n",
              "2        0.982398\n",
              "3        0.975223\n",
              "4        0.994307\n",
              "           ...   \n",
              "19995    0.994253\n",
              "19996    0.995237\n",
              "19997    0.988770\n",
              "19998    0.992365\n",
              "19999    0.997650\n",
              "Name: SSIM Score, Length: 20000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df['SSIM Score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDoBXgQBK5G6"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset[2]\n",
        "\n",
        "# Collect predictions and labels using the trained model\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "\n",
        "for batch in test_dataset:\n",
        "    batch_predictions = model_4.predict(batch[0])\n",
        "    batch_true_labels = batch[1]\n",
        "\n",
        "    test_predictions.append(batch_predictions)\n",
        "    test_labels.append(batch_true_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIbVQwvvDqvb"
      },
      "outputs": [],
      "source": [
        "generator = Generator()\n",
        "predicted_images = []\n",
        "for batch_coeffs in test_predictions:\n",
        "    batch_images = generator.generate_imgs(np.array(batch_coeffs))\n",
        "    predicted_images.append(batch_images)\n",
        "predicted_images = np.concatenate(predicted_images, axis=0)\n",
        "\n",
        "# Step 2: Convert the true labels into images\n",
        "true_images = []\n",
        "for batch in test_dataset:\n",
        "    batch_true_labels = batch[1]\n",
        "    batch_true_images = generator.generate_imgs(np.array(batch_true_labels))\n",
        "    true_images.append(batch_true_images)\n",
        "true_images = tf.concat(true_images, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz2q6cVXZJAH",
        "outputId": "ffa6a324-4f5d-4f46-94ef-4cc370b9426b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dbfb95c63654>:16: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim = metrics.structural_similarity(true_img_reshaped, pred_img_reshaped, multichannel=False)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from skimage import metrics\n",
        "\n",
        "# Calculate SSIM for each image and store in a list\n",
        "ssim_scores = []\n",
        "for pred_img, true_img in zip(predicted_images, true_images):\n",
        "    pred_img_reshaped = np.squeeze(pred_img)  # Remove the channel dimension using NumPy\n",
        "    true_img_reshaped = np.squeeze(true_img)\n",
        "\n",
        "    # Normalize images to [0, 1]\n",
        "    pred_img_reshaped = (pred_img_reshaped - pred_img_reshaped.min()) / (pred_img_reshaped.max() - pred_img_reshaped.min())\n",
        "    true_img_reshaped = (true_img_reshaped - true_img_reshaped.min()) / (true_img_reshaped.max() - true_img_reshaped.min())\n",
        "\n",
        "    # Calculate SSIM\n",
        "    ssim = metrics.structural_similarity(true_img_reshaped, pred_img_reshaped, multichannel=False)\n",
        "    ssim_scores.append(ssim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQUFpSPIQ2BX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Calculate SSIM scores and store in the ssim_scores list (as shown in your code)\n",
        "\n",
        "# Create a DataFrame to store the data\n",
        "data = {'SSIM Score': ssim_scores}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_filename = 'ssim_scores_improved.csv'\n",
        "df.to_csv(csv_filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPSDQGjWmuSn"
      },
      "outputs": [],
      "source": [
        "!cp ssim_scores_improved.csv /content/drive/MyDrive/OpticsML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "OBSsLjLEmzfD",
        "outputId": "c56f066f-f442-4c72-cd5a-f7f723aa598d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_to_plot = predicted_images[190]\n",
        "plt.imshow(image_to_plot, cmap='turbo')  # Assuming the image is grayscale\n",
        "plt.title('Predicted Image')\n",
        "plt.colorbar()  # If you want to add a colorbar\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "nm2CQmsbovbV",
        "outputId": "eb387250-2781-4a04-c735-9703184d88ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "image_to_plot = true_images[190]\n",
        "plt.imshow(image_to_plot, cmap='turbo')  # Assuming the image is grayscale\n",
        "plt.title('True Image')\n",
        "plt.colorbar()  # If you want to add a colorbar\n",
        "plt.axis('off')  # Turn off axis labels\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2R3wy3hrpB4k",
        "outputId": "00917c3e-6bbb-4f9e-ab44-fb4bb6f8ede4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yay\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "  plt.clf()\n",
        "  plt.imshow(true_images[i], cmap='turbo')\n",
        "  plt.title(f'True Image {i+1}')\n",
        "  plt.colorbar()\n",
        "  plt.axis('off')\n",
        "  plt.savefig(f'True image {i+1}.png')\n",
        "  plt.close()\n",
        "  shutil.copy(f'True image {i+1}.png', \"/content/drive/MyDrive/OpticsML/images/true\")\n",
        "\n",
        "print('Yay')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aYrSprvrfXy",
        "outputId": "f2a6ef67-647e-40d5-8c97-d5f0e6943a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yay\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "  plt.clf()\n",
        "  plt.imshow(predicted_images[i], cmap='turbo')\n",
        "  plt.title(f'Predict Image {i+1}')\n",
        "  plt.colorbar()\n",
        "  plt.axis('off')\n",
        "  plt.savefig(f'Predict image {i+1}.png')\n",
        "  plt.close()\n",
        "  shutil.copy(f'Predict image {i+1}.png', \"/content/drive/MyDrive/OpticsML/images/predict\")\n",
        "\n",
        "print('Yay')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OExGRTHZMW2L"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 3: Calculate image accuracies for each image\n",
        "def calculate_image_accuracies(true_images, predicted_images, tolerance=0.00005):\n",
        "    image_accuracies = []\n",
        "\n",
        "    for i in range(len(true_images)):\n",
        "        true_image = true_images[i]\n",
        "        predicted_image = predicted_images[i]\n",
        "\n",
        "        # Compare images element-wise with a tolerance\n",
        "        diff = np.abs(true_image - predicted_image)\n",
        "        accurate_pixels = np.sum(diff < tolerance)\n",
        "\n",
        "        # Calculate accuracy for this image\n",
        "        accuracy = accurate_pixels / true_image.numpy().size  # Use .numpy().size here\n",
        "        image_accuracies.append(accuracy)\n",
        "\n",
        "    return image_accuracies\n",
        "\n",
        "image_accuracies = calculate_image_accuracies(true_images, predicted_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK5lq4WBPS9K",
        "outputId": "92e68301-8725-4051-8079-d0c14db03274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0, 0.8271875, 0.9540625, 0.9321875, 0.9515625, 0.9953125, 0.978125, 1.0, 0.9896875, 0.92484375, 0.99125, 0.976875, 1.0, 1.0, 0.9571875, 0.95796875, 0.886875, 1.0, 1.0, 0.95953125, 0.97921875, 0.838125, 0.8359375, 0.88375, 0.99328125, 1.0, 0.8809375, 0.98625, 0.93921875, 0.9784375, 1.0, 0.91625, 0.933125, 1.0, 1.0, 1.0, 0.96140625, 1.0, 0.88453125, 0.98625, 0.9378125, 0.9984375, 0.8803125, 0.99484375, 0.96234375, 1.0, 0.9478125, 1.0, 0.95125, 1.0, 0.89765625, 0.97828125, 0.973125, 1.0, 0.85515625, 1.0, 0.96703125, 0.93, 0.95921875, 0.99078125, 1.0, 0.95046875, 0.95765625, 0.99578125, 0.9778125, 0.988125, 0.99125, 1.0, 0.76625, 0.93390625, 0.94859375, 0.989375, 0.984375, 0.83890625, 0.88078125, 0.99703125, 1.0, 0.98484375, 0.99046875, 0.92328125, 1.0, 0.83140625, 0.9171875, 0.94953125, 0.9503125, 0.87671875, 0.97015625, 0.98671875, 0.98734375, 1.0, 0.9715625, 0.9784375, 1.0, 0.93546875, 1.0, 1.0, 0.9984375, 0.86796875, 0.72, 0.99421875, 1.0, 0.99984375, 1.0, 0.9459375, 0.96296875, 0.9853125, 1.0, 1.0, 1.0, 0.99078125, 0.9709375, 0.9103125, 0.981875, 0.9309375, 1.0, 0.94609375, 0.90515625, 0.9953125, 0.8559375, 0.97734375, 0.994375, 0.8040625, 0.8871875, 0.925625, 0.8625, 1.0, 1.0, 0.878125, 0.8259375, 0.869375, 0.94, 0.89828125, 0.9884375, 0.9375, 0.85625, 0.9896875, 0.94203125, 1.0, 0.9440625, 1.0, 0.995625, 0.84625, 0.904375, 0.99046875, 0.91546875, 0.8959375, 0.93921875, 0.9628125, 0.98390625, 0.955625, 0.97328125, 0.99046875, 0.96984375, 0.87421875, 1.0, 0.95359375, 0.96765625, 0.98296875, 0.92375, 0.846875, 0.8671875, 0.981875, 0.96734375, 0.98015625, 0.94765625, 0.94734375, 1.0, 1.0, 0.99265625, 0.92171875, 0.87875, 0.91578125, 0.9225, 1.0, 0.8890625, 0.9896875, 0.96421875, 1.0, 0.96671875, 0.9884375, 0.95859375, 0.9421875, 1.0, 0.95828125, 1.0, 0.98625, 0.94234375, 0.85140625, 0.999375, 0.87625, 1.0, 1.0, 1.0, 0.99890625, 0.9809375, 0.97375, 0.99015625, 0.92609375, 1.0, 0.98125, 0.88234375, 0.96296875, 0.80046875, 0.94609375, 0.95015625, 1.0, 0.97703125, 0.930625, 0.9415625, 0.98984375, 0.985, 0.9978125, 0.98796875, 0.95, 0.896875, 0.99375, 0.97453125, 1.0, 0.98359375, 0.9828125, 0.98546875, 0.98546875, 0.9740625, 1.0, 0.93953125, 0.94296875, 0.890625, 0.9915625, 0.9590625, 1.0, 0.8296875, 1.0, 0.98890625, 0.9840625, 0.99125, 0.95328125, 0.95265625, 0.98546875, 1.0, 0.963125, 0.999375, 0.97921875, 0.98921875, 1.0, 0.8434375, 0.8946875, 0.953125, 1.0, 0.96390625, 0.98109375, 0.87296875, 0.976875, 0.9703125, 0.87171875, 0.94546875, 1.0, 0.9890625, 1.0, 0.97140625, 1.0, 1.0, 0.98015625, 1.0, 0.92515625, 0.93375, 0.9590625, 1.0, 0.99265625, 0.99578125, 0.90828125, 0.87015625, 0.9665625, 0.87234375, 1.0, 0.97265625, 0.92765625, 0.9728125, 0.94765625, 0.95578125, 0.95125, 0.9878125, 0.96015625, 1.0, 0.96484375, 0.974375, 1.0, 0.92296875, 1.0, 1.0, 0.9475, 1.0, 0.9125, 1.0, 0.9078125, 0.98515625, 0.90296875, 0.8321875, 0.8546875, 0.914375, 0.965, 1.0, 1.0, 1.0, 1.0, 0.934375, 0.99515625, 1.0, 0.7609375, 0.86, 1.0, 0.8640625, 1.0, 0.79171875, 0.98484375, 1.0, 0.94921875, 0.82796875, 0.9290625, 0.96671875, 0.88609375, 0.9909375, 0.9584375, 0.96484375, 1.0, 0.9803125, 0.971875, 0.924375, 0.82515625, 0.94625, 0.83515625, 0.8428125, 0.9996875, 0.9996875, 0.990625, 0.98921875, 0.92546875, 0.95859375, 0.86328125, 0.94125, 0.96390625, 0.924375, 0.995625, 0.918125, 1.0, 0.90359375, 0.94015625, 0.9971875, 0.93734375, 0.87140625, 1.0, 0.9390625, 1.0, 1.0, 0.97328125, 0.950625, 0.9728125, 0.95453125, 0.966875, 0.9353125, 0.98015625, 0.94453125, 0.980625, 0.975625, 1.0, 1.0, 0.98625, 0.97234375, 0.93015625, 0.88796875, 1.0, 0.9946875, 1.0, 0.981875, 1.0, 0.97375, 0.94265625, 1.0, 0.83625, 0.948125, 0.91828125, 0.98171875, 0.97015625, 0.99875, 0.9728125, 0.97984375, 0.9725, 0.98734375, 0.92640625, 0.90140625, 0.91875, 1.0, 1.0, 0.94171875, 0.960625, 1.0, 0.9328125, 0.88796875, 0.97484375, 0.86078125, 0.98625, 0.94703125, 0.99953125, 0.9509375, 0.935, 0.96953125, 0.9434375, 0.99734375, 0.98625, 0.90359375, 1.0, 0.8596875, 0.9925, 0.84375, 0.99609375, 0.9415625, 0.97421875, 0.94, 0.95046875, 0.9975, 0.84046875, 0.94890625, 0.86359375, 1.0, 1.0, 0.7646875, 0.900625, 0.96234375, 0.9959375, 0.95453125, 0.85890625, 0.9175, 0.95609375, 0.940625, 0.96359375, 0.98171875, 1.0, 0.94703125, 0.900625, 0.9721875, 0.9809375, 0.9921875, 1.0, 0.9934375, 0.83515625, 0.8728125, 0.99796875, 1.0, 0.99390625, 0.8925, 0.889375, 0.96984375, 0.89640625, 0.9709375, 1.0, 0.95125, 0.913125, 0.92515625, 0.96484375, 1.0, 0.98234375, 0.995, 0.97484375, 1.0, 0.973125, 0.96328125, 0.97984375, 0.983125, 1.0, 0.9965625, 0.99421875, 0.98453125, 1.0, 0.97578125, 0.89375, 1.0, 1.0, 0.95375, 0.96859375, 0.89796875, 1.0, 0.96, 0.94828125, 0.9928125, 0.95109375, 0.799375, 0.895625, 0.92328125, 0.90546875, 1.0, 0.9728125, 1.0, 0.99546875, 0.875, 1.0, 0.93234375, 0.84921875, 0.980625, 1.0, 0.96390625, 0.996875, 0.971875, 0.9, 0.919375, 0.9396875, 1.0, 0.9234375, 0.97125, 0.989375, 0.9859375, 0.94125, 0.97984375, 0.9525, 1.0, 0.99609375, 0.94578125, 0.90765625, 0.974375, 1.0, 0.96828125, 1.0, 0.9959375, 1.0, 1.0, 0.97859375, 0.99203125, 1.0, 0.96359375, 0.998125, 0.94953125, 0.979375, 1.0, 0.9975, 1.0, 1.0, 1.0, 0.8209375, 0.935625, 1.0, 0.60859375, 0.92078125, 0.936875, 0.985, 0.95640625, 0.90453125, 0.943125, 1.0, 0.968125, 0.95890625, 1.0, 0.8990625, 0.905625, 0.9840625, 0.98765625, 0.8959375, 1.0, 0.9103125, 1.0, 0.99109375, 1.0, 0.87828125, 0.97828125, 0.9959375, 0.98921875, 0.94109375, 1.0, 0.87203125, 0.9609375, 0.97640625, 0.92625, 0.99765625, 1.0, 0.9775, 1.0, 0.96171875, 0.969375, 0.85625, 1.0, 0.99625, 0.91765625, 1.0, 0.9965625, 0.94328125, 0.91984375, 0.93453125, 1.0, 1.0, 0.9640625, 0.940625, 0.930625, 0.984375, 0.9584375, 0.9896875, 0.9665625, 0.99484375, 0.9565625, 0.9034375, 0.98953125, 0.994375, 0.99703125, 0.98734375, 0.844375, 0.95421875, 0.97296875, 0.8946875, 0.8965625, 1.0, 1.0, 0.88359375, 0.9853125, 1.0, 0.96515625, 1.0, 0.95859375, 0.9946875, 0.93125, 0.89890625, 0.96, 0.90953125, 0.92671875, 0.97796875, 0.9471875, 0.89328125, 0.8615625, 0.97, 0.985, 0.90671875, 0.7525, 1.0, 1.0, 1.0, 0.97109375, 0.94109375, 0.955625, 1.0, 0.980625, 0.97234375, 0.95453125, 0.9134375, 0.94328125, 0.955625, 0.885625, 0.935, 0.89140625, 0.87421875, 1.0, 0.9646875, 1.0, 1.0, 1.0, 0.94234375, 0.9984375, 0.99765625, 0.88640625, 0.98140625, 0.9846875, 0.9621875, 0.9396875, 0.99390625, 0.95515625, 0.889375, 0.9528125, 1.0, 0.98984375, 0.93671875, 0.9953125, 1.0, 0.88390625, 0.98578125, 0.91078125, 0.965, 0.8946875, 1.0, 0.9534375, 0.9653125, 0.9015625, 0.976875, 0.9740625, 0.9575, 1.0, 1.0, 0.991875, 0.9475, 0.9803125, 0.99984375, 0.93171875, 0.9846875, 0.98875, 0.95796875, 0.9959375, 0.95953125, 1.0, 0.95453125, 0.93375, 1.0, 0.93640625, 0.95453125, 0.99546875, 0.85140625, 0.9296875, 0.90796875, 0.97375, 1.0, 0.923125, 0.9175, 0.98109375, 1.0, 0.9959375, 0.94125, 0.95078125, 0.8725, 0.9365625, 1.0, 0.80671875, 0.9878125, 0.95453125, 0.838125, 0.97796875, 0.95140625, 1.0, 0.9875, 0.9290625, 0.97546875, 0.96046875, 0.9321875, 0.95921875, 1.0, 0.94375, 1.0, 0.834375, 0.835625, 0.915625, 0.86796875, 0.94890625, 0.9365625, 0.96359375, 1.0, 1.0, 1.0, 0.94125, 0.9959375, 0.95796875, 0.99765625, 1.0, 0.886875, 1.0, 0.9059375, 1.0, 0.9871875, 0.97078125, 0.9325, 0.9075, 0.94890625, 0.91296875, 0.985, 1.0, 0.9815625, 0.93140625, 0.86765625, 0.98234375, 0.94, 1.0, 1.0, 1.0, 0.95109375, 0.99203125, 0.9771875, 1.0, 0.94546875, 0.9821875, 1.0, 1.0, 0.9234375, 1.0, 0.94796875, 0.95625, 0.94953125, 0.99265625, 1.0, 0.84484375, 1.0, 1.0, 0.86296875, 0.754375, 0.89734375, 0.909375, 1.0, 0.9753125, 0.93171875, 1.0, 0.9796875, 0.9584375, 0.96296875, 0.92421875, 0.883125, 0.95859375, 0.98375, 1.0, 0.90859375, 0.871875, 0.98671875, 0.89171875, 0.94984375, 1.0, 1.0, 0.9228125, 0.8615625, 1.0, 0.973125, 0.91640625, 1.0, 1.0, 1.0, 0.94453125, 0.8484375, 1.0, 0.9303125, 0.9121875, 0.87421875, 0.98703125, 0.9978125, 0.93171875, 0.78296875, 0.9, 0.98046875, 0.920625, 1.0, 0.9978125, 0.97796875, 1.0, 0.97921875, 1.0, 0.96453125, 0.97984375, 0.93453125, 0.9228125, 1.0, 0.94828125, 0.9675, 0.94640625, 0.88734375, 0.95171875, 0.87046875, 0.9225, 0.98890625, 0.98265625, 0.97625, 1.0, 0.91, 0.978125, 1.0, 1.0, 1.0, 0.95171875, 1.0, 0.9940625, 0.8978125, 0.9159375, 0.8975, 0.94453125, 0.98671875, 0.944375, 0.9203125, 0.9446875, 0.910625, 1.0, 0.82796875, 1.0, 0.99625, 1.0, 0.98828125, 0.90703125, 1.0, 1.0, 1.0, 0.9875, 0.95015625, 0.92671875, 0.90375, 1.0, 0.94671875, 0.9696875, 1.0, 0.835625, 0.95796875, 0.978125, 0.93484375, 0.8821875, 0.9840625, 0.96375, 0.9340625, 0.97125, 0.8428125, 0.999375, 0.846875, 0.91078125, 0.99375, 0.96140625, 0.99296875, 0.9275, 0.9275, 1.0, 0.96015625, 0.95859375, 0.98328125, 0.980625, 0.995, 0.9440625, 0.93375, 0.88640625, 0.901875, 1.0, 1.0, 0.95234375, 1.0, 0.92171875, 0.9565625, 0.9590625, 1.0, 1.0, 1.0, 0.92375, 1.0, 1.0, 0.94625, 0.93390625, 0.98390625, 0.9996875, 0.949375, 1.0, 0.980625, 0.9784375, 0.98046875, 0.95125, 0.9221875, 0.98359375, 0.99875, 1.0, 1.0, 1.0, 0.98859375, 0.89109375, 0.8790625, 0.98640625, 1.0, 0.98421875, 0.99859375, 0.9259375, 1.0, 1.0, 0.88875, 0.935, 0.985625, 1.0, 0.87765625, 0.9965625, 0.94546875, 0.935, 1.0, 0.92484375, 1.0, 0.90078125, 0.98140625, 1.0, 0.9828125, 1.0, 1.0, 0.9846875, 0.94765625, 0.7984375, 0.99171875, 0.97875, 0.993125, 0.99421875, 1.0, 1.0, 0.9140625, 0.94453125, 0.97875, 1.0, 1.0, 0.99015625, 0.8921875, 1.0, 0.9584375, 1.0, 0.8490625, 0.8653125, 0.9459375, 1.0, 1.0, 1.0, 1.0, 0.98703125, 1.0, 0.97796875, 1.0, 0.9821875, 0.90296875, 1.0, 0.99796875, 0.93375, 0.99765625, 0.94359375, 1.0, 0.86765625, 0.99109375, 1.0, 1.0, 0.95546875, 0.9546875, 0.9171875, 0.98875, 0.9403125, 1.0, 1.0, 0.86203125, 0.92875, 0.9978125, 0.88453125, 0.84578125, 0.97765625, 0.9934375, 0.96890625, 1.0, 0.894375, 1.0, 0.9321875, 0.96546875, 0.98421875, 0.89984375, 0.94078125, 0.9184375, 0.91828125, 0.95234375, 0.8628125, 0.96671875, 0.95796875, 0.8884375, 0.9453125, 1.0, 0.855, 0.92875, 0.89265625, 0.97671875, 0.88, 0.9346875, 1.0, 0.999375, 0.99859375, 0.96, 0.9615625, 1.0, 0.9315625, 0.99765625, 0.9921875, 0.904375, 1.0, 0.8265625, 0.91734375, 0.97109375, 0.9215625, 0.8634375, 1.0, 1.0, 0.9975, 0.86171875, 1.0, 0.96484375, 0.96859375, 0.97015625, 0.9675, 0.9665625, 0.99796875, 1.0, 0.988125, 0.99640625, 0.96359375, 0.98140625, 0.96953125, 0.84125, 0.894375, 0.8771875, 0.84984375, 0.9828125, 0.858125, 0.99859375, 0.941875, 0.98421875, 0.92375, 0.89671875, 0.99921875, 0.975, 0.86890625, 0.84171875, 0.92171875, 0.88546875, 0.9834375, 0.9121875, 0.97140625, 0.90921875, 1.0, 0.97109375, 0.9296875, 0.98375, 0.8959375, 0.96828125, 0.94234375, 0.9846875, 1.0, 0.993125, 0.92625, 0.9925, 0.9890625, 0.8928125, 0.9371875, 0.98171875, 0.85546875, 0.92578125, 0.9834375, 0.8915625, 0.85359375, 1.0, 1.0, 1.0, 0.93375, 0.78, 0.96890625, 0.99125, 0.98078125, 1.0, 0.91515625, 0.96453125, 1.0, 0.9953125, 0.89921875, 1.0, 0.90890625, 0.8734375, 0.91375, 1.0, 0.98875, 0.96484375, 0.98546875, 0.9765625, 0.884375, 1.0, 0.82765625, 0.8815625, 1.0, 0.98203125, 0.99296875, 0.95140625, 0.965625, 1.0, 0.94890625, 0.9834375, 0.95734375, 1.0, 0.99671875, 1.0, 0.9840625, 0.98078125, 1.0, 0.986875, 1.0, 0.97984375, 0.981875, 0.9890625, 1.0, 1.0, 0.99921875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9225, 0.96453125, 1.0, 0.96984375, 0.79734375, 0.99921875, 0.97125, 0.8709375, 1.0, 1.0, 0.92015625, 0.8553125, 0.92640625, 0.92328125, 0.9871875, 1.0, 1.0, 1.0, 1.0, 0.94421875, 0.8965625, 0.94265625, 0.91375, 1.0, 0.87609375, 1.0, 0.9078125, 1.0, 0.91640625, 1.0, 0.844375, 0.99828125, 1.0, 0.98734375, 0.92875, 1.0, 1.0, 0.85265625, 0.914375, 0.9296875, 0.8928125, 1.0, 0.965625, 0.9534375, 0.9525, 0.98359375, 0.9240625, 0.9715625, 0.99078125, 0.9728125, 1.0, 0.96890625, 1.0, 0.89109375, 0.98484375, 0.97609375, 1.0, 1.0, 0.94359375, 0.92828125, 0.8946875, 1.0, 0.85625, 0.81515625, 1.0, 0.933125, 0.978125, 0.878125, 0.82453125, 1.0, 0.984375, 0.911875, 1.0, 1.0, 0.94765625, 0.99796875, 0.88921875, 0.9575, 0.95109375, 0.9659375, 0.95515625, 0.945625, 1.0, 0.91265625, 0.86328125, 0.88421875, 0.969375, 0.9421875, 0.9834375, 0.95984375, 1.0, 0.9559375, 0.92109375, 1.0, 0.8721875, 1.0, 1.0, 0.90953125, 0.98, 0.98515625, 0.9925, 0.906875, 0.949375, 0.9309375, 0.9653125, 0.96265625, 0.97203125, 0.99828125, 0.9040625, 0.9821875, 0.96375, 0.98828125, 0.9175, 0.90546875, 0.95, 0.9734375, 0.97859375, 0.9678125, 1.0, 0.845625, 1.0, 0.79265625, 0.88234375, 0.87984375, 0.89921875, 0.970625, 0.9046875, 0.8465625, 0.96125, 1.0, 0.85984375, 0.9303125, 1.0, 0.91796875, 1.0, 0.9590625, 1.0, 0.98015625, 0.73359375, 1.0, 0.916875, 0.85328125, 1.0, 0.9734375, 0.99203125, 0.95625, 1.0, 0.86671875, 0.92546875, 1.0, 0.8925, 0.9534375, 0.975, 0.82203125, 0.90515625, 0.95140625, 0.8640625, 0.95421875, 0.9984375, 0.943125, 0.9834375, 1.0, 0.9215625, 0.97859375, 0.8603125, 0.9659375, 0.9675, 0.9496875, 0.96734375, 0.97921875, 0.94140625, 1.0, 1.0, 0.97890625, 0.99671875, 0.99375, 0.985, 0.94078125, 0.936875, 0.9496875, 0.971875, 1.0, 0.90390625, 0.89703125, 0.94078125, 0.82390625, 1.0, 1.0, 0.979375, 0.9934375, 0.94125, 1.0, 0.92796875, 0.88921875, 0.9446875, 0.93359375, 1.0, 1.0, 0.96953125, 0.920625, 0.975625, 1.0, 0.8865625, 0.985625, 0.986875, 1.0, 0.98046875, 0.97640625, 1.0, 1.0, 0.905625, 0.9846875, 0.999375, 0.885625, 1.0, 1.0, 0.90984375, 1.0, 1.0, 0.910625, 0.9665625, 0.9384375, 0.93046875, 0.9053125, 0.89640625, 0.93234375, 0.9871875, 0.96359375, 0.910625, 1.0, 0.98765625, 0.91109375, 0.95046875, 0.948125, 0.96984375, 0.93140625, 0.99546875, 0.9296875, 0.96140625, 1.0, 0.98671875, 0.98359375, 0.9721875, 1.0, 0.95296875, 1.0, 0.83234375, 0.91, 0.91640625, 0.93625, 1.0, 0.945, 0.9475, 0.96546875, 0.9790625, 0.90203125, 1.0, 0.9965625, 0.91515625, 0.9490625, 0.9925, 0.9290625, 0.87375, 0.9415625, 0.99546875, 0.99875, 1.0, 0.891875, 0.98125, 0.97453125, 0.8996875, 0.94640625, 0.99359375, 0.97234375, 1.0, 0.96234375, 0.77015625, 1.0, 1.0, 0.94, 0.97125, 0.9659375, 0.92703125, 0.9840625, 0.98375, 0.971875, 0.87359375, 0.92, 0.99484375, 1.0, 0.9209375, 0.97046875, 0.8540625, 0.9725, 0.9696875, 0.91078125, 0.82828125, 0.90875, 0.9578125, 1.0, 1.0, 0.98578125, 1.0, 1.0, 0.97984375, 0.97734375, 0.859375, 1.0, 0.95265625, 0.981875, 1.0, 1.0, 0.8828125, 0.95515625, 1.0, 0.84859375, 0.9528125, 0.99734375, 0.98796875, 0.95671875, 0.9496875, 0.98125, 1.0, 0.983125, 0.89828125, 0.73078125, 0.97875, 0.95328125, 0.8903125, 1.0, 0.83421875, 0.98109375, 0.97546875, 0.9490625, 0.9809375, 0.886875, 0.9609375, 0.9175, 1.0, 0.9990625, 0.8728125, 0.9659375, 0.92328125, 0.9071875, 0.96484375, 0.98765625, 0.90890625, 0.971875, 0.965, 1.0, 0.99359375, 0.98296875, 1.0, 0.93171875, 0.865625, 1.0, 0.961875, 0.96796875, 0.82984375, 0.941875, 0.8296875, 0.9425, 0.8996875, 0.9609375, 0.939375, 0.98296875, 0.74296875, 0.91625, 0.98671875, 0.8775, 0.9615625, 0.9921875, 1.0, 0.96890625, 0.98046875, 0.9553125, 1.0, 0.8996875, 0.92078125, 0.9209375, 1.0, 0.93, 0.96359375, 0.97875, 0.9690625, 0.9734375, 1.0, 1.0, 0.86890625, 0.970625, 0.9209375, 0.98890625, 0.95203125, 1.0, 1.0, 0.8784375, 0.95515625, 0.9109375, 0.96703125, 0.9228125, 0.97421875, 0.92796875, 1.0, 0.983125, 1.0, 1.0, 0.98921875, 1.0, 0.8971875, 0.945625, 0.9365625, 0.95140625, 1.0, 1.0, 0.993125, 0.94796875, 1.0, 0.9346875, 1.0, 0.904375, 0.90390625, 0.90609375, 0.97796875, 0.856875, 1.0, 0.9865625, 1.0, 0.96453125, 1.0, 0.98703125, 0.990625, 1.0, 0.83203125, 0.8771875, 0.8578125, 1.0, 0.96484375, 0.92671875, 0.94921875, 0.96234375, 0.95609375, 1.0, 0.905, 0.94109375, 1.0, 0.99328125, 1.0, 0.91, 0.93296875, 0.95140625, 0.99171875, 0.9559375, 0.87109375, 0.79796875, 1.0, 0.98625, 0.9696875, 0.90765625, 0.86734375, 1.0, 1.0, 0.92859375, 0.98515625, 1.0, 0.985625, 1.0, 0.92109375, 1.0, 1.0, 0.94609375, 1.0, 0.9284375, 0.84484375, 0.9540625, 1.0, 0.91921875, 0.9425, 1.0, 0.9771875, 0.94046875, 1.0, 1.0, 0.98421875, 0.86953125, 0.99078125, 1.0, 1.0, 0.96390625, 0.97, 0.91484375, 0.85796875, 0.95625, 0.77484375, 0.95984375, 0.90671875, 1.0, 0.91953125, 0.915625, 0.93421875, 0.89140625, 0.99609375, 0.85625, 1.0, 1.0, 1.0, 0.985, 0.92703125, 0.9228125, 1.0, 0.99390625, 0.9709375, 0.98125, 0.93921875, 1.0, 0.99453125, 0.95, 0.99125, 0.9790625, 0.97078125, 1.0, 0.899375, 1.0, 0.8403125, 0.9553125, 0.8821875, 0.9925, 0.97421875, 0.97921875, 0.936875, 0.98953125, 0.86484375, 0.89375, 0.9684375, 1.0, 1.0, 0.861875, 0.929375, 0.98953125, 0.941875, 1.0, 0.88765625, 0.96921875, 0.9659375, 0.97078125, 0.9546875, 1.0, 0.92890625, 0.96203125, 1.0, 0.99125, 0.93734375, 1.0, 0.99578125, 0.88359375, 1.0, 0.97296875, 0.9528125, 0.9925, 0.9909375, 0.998125, 0.944375, 0.9771875, 0.9784375, 1.0, 0.90015625, 0.91265625, 0.87265625, 0.92671875, 1.0, 1.0, 1.0, 0.9659375, 0.91390625, 1.0, 0.896875, 0.9971875, 1.0, 1.0, 1.0, 0.818125, 0.8515625, 0.98078125, 0.925625, 1.0, 0.99171875, 0.99234375, 1.0, 0.9921875, 1.0, 0.97234375, 0.8365625, 0.88515625, 0.99234375, 0.915625, 0.97578125, 0.94, 1.0, 0.9875, 0.915625, 1.0, 0.90875, 1.0, 0.93390625, 0.90671875, 0.9609375, 0.94609375, 0.90171875, 1.0, 0.9740625, 1.0, 1.0, 0.9725, 0.98734375, 1.0, 0.99203125, 1.0, 1.0, 1.0, 0.95421875, 0.946875, 0.90046875, 1.0, 0.9225, 0.97921875, 0.87515625, 0.93640625, 0.97546875, 0.87046875, 0.99421875, 1.0, 1.0, 0.99375, 0.90734375, 1.0, 0.9946875, 0.97296875, 0.976875, 0.9946875, 0.97703125, 0.890625, 0.935625, 0.93828125, 1.0, 1.0, 0.93578125, 0.9959375, 0.87546875, 0.9990625, 0.939375, 1.0, 0.96046875, 0.97375, 0.98640625, 0.96359375, 0.92546875, 0.9984375, 0.98078125, 1.0, 0.97125, 0.97078125, 0.91984375, 0.96171875, 1.0, 0.95921875, 0.89171875, 0.90359375, 0.99546875, 0.97078125, 0.97703125, 0.93359375, 0.99078125, 0.84421875, 1.0, 1.0, 0.890625, 1.0, 0.8365625, 0.985, 1.0, 0.99578125, 0.91828125, 0.93984375, 1.0, 0.92578125, 0.939375, 0.88859375, 0.9878125, 0.97734375, 0.95, 0.98890625, 1.0, 1.0, 0.8821875, 1.0, 0.85328125, 0.86875, 0.91453125, 0.91609375, 1.0, 1.0, 0.97734375, 0.92046875, 0.94734375, 0.9990625, 0.963125, 0.97109375, 0.85203125, 0.89078125, 0.9896875, 0.953125, 1.0, 0.9378125, 0.9175, 0.91421875, 0.94453125, 0.86671875, 0.978125, 0.92375, 0.99171875, 0.946875, 0.90796875, 0.95484375, 0.956875, 0.97703125, 0.96859375, 0.86265625, 0.87296875, 0.94703125, 0.869375, 0.8996875, 0.90921875, 0.9353125, 1.0, 0.8475, 0.9390625, 1.0, 0.98625, 0.89953125, 0.78640625, 1.0, 0.9246875, 0.945625, 0.989375, 1.0, 0.93390625, 1.0, 0.96046875, 1.0, 0.909375, 1.0, 0.9175, 0.86796875, 1.0, 0.97703125, 0.9496875, 0.9034375, 0.99828125, 0.9978125, 0.96171875, 1.0, 0.84171875, 0.97796875, 0.98296875, 0.933125, 0.95265625, 1.0, 0.9275, 0.88453125, 0.966875, 0.98578125, 0.999375, 0.98453125, 0.9784375, 0.9559375, 0.928125, 0.9584375, 0.94140625, 0.99734375, 0.9015625, 0.9646875, 0.93921875, 0.94984375, 0.975, 1.0, 1.0, 1.0, 1.0, 0.93046875, 1.0, 0.93328125, 0.93484375, 0.9765625, 0.93984375, 0.89, 0.95171875, 0.92578125, 0.92546875, 0.93828125, 0.9878125, 0.95359375, 0.95015625, 0.98171875, 0.95890625, 0.92890625, 1.0, 0.93890625, 0.9696875, 0.93828125, 1.0, 0.94015625, 1.0, 0.87015625, 0.9253125, 0.82984375, 1.0, 0.9096875, 0.923125, 1.0, 0.9275, 0.99671875, 0.914375, 0.93609375, 0.87984375, 0.92375, 0.9109375, 1.0, 0.9765625, 1.0, 0.923125, 0.96078125, 0.88359375, 0.86296875, 0.98453125, 1.0, 1.0, 0.98140625, 0.96703125, 0.945, 0.97078125, 1.0, 0.8978125, 0.93171875, 1.0, 1.0, 0.89, 0.9990625, 1.0, 0.9571875, 0.99265625, 0.96671875, 1.0, 1.0, 0.9840625, 1.0, 0.96125, 0.98265625, 0.96078125, 1.0, 1.0, 1.0, 0.93515625, 0.9603125, 0.9140625, 0.9240625, 0.90875, 0.92765625, 1.0, 0.90953125, 0.90046875, 0.9709375, 0.9296875, 0.88, 0.9609375, 1.0, 0.9528125, 1.0, 0.979375, 0.99578125, 0.95828125, 0.95265625, 0.92875, 0.98734375, 0.965625, 0.89578125, 0.981875, 0.87328125, 0.94953125, 0.9546875, 0.976875, 0.8934375, 1.0, 0.9790625, 1.0, 0.86796875, 0.9353125, 1.0, 0.8534375, 0.92046875, 1.0, 0.965, 0.95484375, 0.97265625, 1.0, 0.92453125, 0.98828125, 0.98640625, 1.0, 0.94734375, 1.0, 0.95609375, 0.90578125, 0.9825, 0.9596875, 0.91125, 0.9953125, 0.84296875, 0.99359375, 0.94875, 0.98703125, 0.9334375, 0.98296875, 1.0, 1.0, 0.9471875, 0.974375, 0.99390625, 0.97171875, 0.99765625, 1.0, 0.9225, 0.996875, 0.95328125, 0.9153125, 0.8834375, 0.9596875, 1.0, 0.97265625, 1.0, 0.96328125, 0.99828125, 0.9884375, 0.92375, 1.0, 0.89953125, 1.0, 0.9853125, 1.0, 0.8515625, 0.93296875, 0.9209375, 0.973125, 1.0, 0.8990625, 1.0, 0.99515625, 0.92234375, 1.0, 1.0, 1.0, 0.9540625, 0.96078125, 0.94265625, 0.9453125, 0.9978125, 0.8971875, 0.8340625, 0.95125, 0.9940625, 0.9728125, 1.0, 0.83921875, 0.819375, 1.0, 0.9728125, 0.95171875, 0.939375, 0.9275, 1.0, 0.975625, 0.92140625, 0.87328125, 0.933125, 0.93203125, 1.0, 0.99390625, 0.96453125, 0.98609375, 0.96515625, 0.89890625, 0.9521875, 0.92953125, 0.98640625, 1.0, 0.8984375, 0.9590625, 1.0, 0.81625, 0.97328125, 1.0, 0.9665625, 0.96390625, 0.9040625, 1.0, 0.85046875, 1.0, 0.94328125, 1.0, 0.9675, 0.9109375, 0.9396875, 1.0, 0.99640625, 0.9890625, 1.0, 0.703125, 0.97890625, 0.8228125, 0.92046875, 1.0, 1.0, 1.0, 0.92921875, 0.98140625, 1.0, 0.923125, 0.90390625, 0.98890625, 0.98296875, 0.963125, 0.9896875, 0.96828125, 1.0, 0.95640625, 0.9328125, 0.92765625, 1.0, 1.0, 0.9715625, 1.0, 0.9746875, 0.97359375, 0.93625, 0.95296875, 0.97359375, 0.93328125, 0.98265625, 0.96328125, 0.90671875, 1.0, 0.8703125, 1.0, 1.0, 0.936875, 0.9775, 0.93921875, 0.985, 0.92390625, 0.89171875, 0.85328125, 1.0, 1.0, 0.948125, 0.9375, 1.0, 0.99953125, 0.9978125, 1.0, 0.99296875, 0.9365625, 0.969375, 0.87, 1.0, 0.84140625, 1.0, 0.8815625, 1.0, 1.0, 0.92921875, 0.97875, 1.0, 0.99375, 0.981875, 1.0, 1.0, 0.981875, 1.0, 0.85359375, 0.985, 0.9828125, 0.875625, 0.97828125, 0.9184375, 0.93453125, 0.95109375, 0.97625, 0.9425, 0.94375, 1.0, 0.9678125, 0.99203125, 0.950625, 0.994375, 0.97296875, 1.0, 0.9984375, 0.8671875, 0.8365625, 1.0, 0.96015625, 0.81421875, 1.0, 1.0, 0.93484375, 0.98375, 0.87046875, 0.86265625, 0.8721875, 1.0, 0.8915625, 0.94703125, 0.981875, 0.85578125, 0.9959375, 0.9884375, 0.9365625, 0.859375, 0.828125, 1.0, 1.0, 0.96875, 0.961875, 0.905625, 0.988125, 0.873125, 0.95765625, 1.0, 1.0, 1.0, 0.92921875, 1.0, 0.93890625, 0.98984375, 0.8825, 0.824375, 1.0, 0.96703125, 1.0, 0.86703125, 0.99296875, 0.9725, 0.87171875, 1.0, 0.948125, 0.97890625, 1.0, 1.0, 1.0, 0.95734375, 0.98078125, 0.94453125, 0.82296875, 0.79265625, 0.98859375, 0.94875, 0.9909375, 0.9178125, 0.99671875, 1.0, 1.0, 0.9684375, 1.0, 0.91953125, 0.95921875, 1.0, 0.85453125, 1.0, 0.93828125, 0.9559375, 0.99515625, 0.99859375, 0.95078125, 0.9084375, 1.0, 1.0, 0.96828125, 1.0, 1.0, 0.961875, 0.94046875, 0.949375, 0.890625, 0.9521875, 0.97578125, 0.9725, 0.96671875, 0.9928125, 0.9978125, 0.97578125, 0.82, 0.98328125, 0.8334375, 0.9315625, 1.0, 1.0, 1.0, 1.0, 0.96078125, 0.99671875, 0.93390625, 1.0, 1.0, 0.94078125, 0.95921875, 0.95609375, 1.0, 0.9134375, 0.96828125, 0.98734375, 0.97015625, 0.98171875, 0.93328125, 0.8575, 0.97265625, 0.9525, 0.989375, 0.91109375, 0.98, 0.89546875, 1.0, 0.97171875, 0.97828125, 0.94703125, 1.0, 0.98625, 0.995625, 0.99453125, 1.0, 0.78859375, 1.0, 0.88875, 0.9246875, 0.975625, 0.8928125, 0.7215625, 0.87859375, 0.94015625, 0.94234375, 0.98921875, 0.96546875, 0.99171875, 0.985625, 0.98640625, 0.85796875, 0.825, 0.9190625, 0.96109375, 0.9815625, 1.0, 1.0, 0.93046875, 0.9021875, 1.0, 0.9271875, 0.85609375, 0.94078125, 0.98734375, 0.91828125, 0.95765625, 0.978125, 0.92140625, 0.9725, 0.8828125, 0.99, 0.98953125, 0.98234375, 0.9646875, 1.0, 0.97515625, 0.889375, 1.0, 0.91, 0.9775, 1.0, 0.96, 0.989375, 0.9840625, 0.93234375, 0.94515625, 1.0, 0.998125, 0.8759375, 0.88859375, 1.0, 0.96046875, 0.9828125, 0.97375, 1.0, 0.95625, 0.99015625, 1.0, 0.99453125, 0.9903125, 0.973125, 0.905625, 0.9753125, 0.97953125, 0.993125, 0.8628125, 1.0, 1.0, 1.0, 1.0, 0.96609375, 0.9959375, 1.0, 0.8946875, 0.9146875, 0.903125, 0.963125, 1.0, 1.0, 0.85078125, 0.965625, 1.0, 0.98140625, 1.0, 0.94015625, 0.98578125, 0.98171875, 0.986875, 1.0, 0.96984375, 0.99984375, 0.97796875, 0.93078125, 0.96578125, 0.9834375, 0.89140625, 0.94328125, 0.93703125, 0.9334375, 0.9571875, 1.0, 1.0, 0.929375, 1.0, 0.969375, 0.97390625, 1.0, 0.933125, 0.99828125, 0.95375, 1.0, 0.84515625, 1.0, 0.90296875, 0.8278125, 0.99859375, 0.983125, 1.0, 0.99640625, 1.0, 0.99296875, 0.9475, 0.98421875, 0.88859375, 0.97171875, 0.94484375, 0.97421875, 0.8978125, 0.975625, 0.9409375, 0.97515625, 0.98421875, 0.92890625, 0.91375, 1.0, 0.9390625, 0.98890625, 1.0, 0.97921875, 1.0, 1.0, 0.9921875, 0.9796875, 1.0, 0.97109375, 1.0, 0.88875, 0.90578125, 0.9490625, 0.9059375, 0.93703125, 0.99671875, 1.0, 0.8540625, 0.929375, 0.98625, 0.91609375, 0.9475, 1.0, 0.86109375, 0.9053125, 0.98515625, 0.95578125, 1.0, 1.0, 0.983125, 0.94796875, 1.0, 1.0, 0.9, 0.97421875, 0.99453125, 0.915, 0.8978125, 1.0, 0.99609375, 0.8396875, 0.91484375, 0.9046875, 0.92921875, 0.87828125, 0.93296875, 0.89890625, 0.96484375, 0.9921875, 0.98078125, 0.991875, 0.95734375, 0.91546875, 0.868125, 0.9825, 0.9621875, 1.0, 0.9803125, 0.92921875, 0.86421875, 0.92359375, 0.9615625, 1.0, 0.96671875, 0.99921875, 0.8040625, 0.9540625, 0.8575, 0.90421875, 0.96375, 0.88265625, 1.0, 0.98484375, 0.97421875, 0.99921875, 0.9803125, 0.985, 0.94328125, 0.92203125, 0.8690625, 0.95296875, 0.9934375, 0.9365625, 0.996875, 0.94921875, 0.98703125, 1.0, 0.965625, 0.96296875, 1.0, 0.965625, 0.95375, 0.96625, 0.88421875, 0.86421875, 0.8846875, 0.84609375, 0.91015625, 0.9840625, 1.0, 1.0, 0.9021875, 0.88421875, 0.811875, 0.990625, 1.0, 0.99125, 0.901875, 1.0, 1.0, 0.85953125, 0.88625, 1.0, 0.995625, 0.9225, 1.0, 0.86984375, 0.97015625, 1.0, 0.9246875, 0.8553125, 0.95328125, 0.97375, 0.95125, 1.0, 0.96046875, 0.93203125, 1.0, 0.9640625, 0.97625, 0.96921875, 0.9859375, 0.99015625, 0.97828125, 1.0, 0.98421875, 1.0, 0.9296875, 0.99640625, 0.98671875, 0.981875, 0.9671875, 1.0, 1.0, 1.0, 0.9353125, 0.95203125, 0.96828125, 0.98734375, 0.9490625, 0.73328125, 1.0, 0.963125, 1.0, 0.79453125, 0.92140625, 0.981875, 0.845625, 0.866875, 0.973125, 0.87453125, 0.98890625, 1.0, 0.93640625, 0.9478125, 0.91890625, 0.9228125, 0.9421875, 1.0, 0.9890625, 1.0, 0.99140625, 0.93625, 0.919375, 0.9790625, 0.9634375, 0.99203125, 1.0, 1.0, 1.0, 0.9671875, 0.98375, 0.97046875, 0.96375, 1.0, 1.0, 1.0, 0.92625, 0.87734375, 0.9575, 1.0, 0.99390625, 1.0, 0.9428125, 0.88640625, 0.90515625, 0.95546875, 1.0, 0.8478125, 0.9428125, 0.87359375, 0.91421875, 0.96703125, 1.0, 0.94859375, 0.97703125, 0.92265625, 1.0, 0.853125, 0.978125, 0.89109375, 0.986875, 0.95578125, 0.97203125, 0.94421875, 1.0, 0.878125, 0.94484375, 0.98125, 0.97140625, 0.9759375, 0.9884375, 0.880625, 0.95234375, 0.9615625, 0.93578125, 0.9653125, 0.92703125, 0.97671875, 0.9915625, 0.9878125, 0.9878125, 0.88671875, 0.9809375, 1.0, 0.99984375, 0.944375, 0.88671875, 0.99796875, 1.0, 1.0, 0.90375, 0.98390625, 0.90203125, 0.878125, 0.9809375, 0.8971875, 0.99796875, 0.99125, 0.94421875, 0.9771875, 1.0, 0.8990625, 0.93671875, 1.0, 1.0, 0.8725, 0.931875, 0.9828125, 0.9165625, 1.0, 0.8990625, 0.979375, 0.9575, 0.96671875, 0.89125, 0.96265625, 1.0, 0.83421875, 0.983125, 1.0, 0.95421875, 0.9628125, 0.86953125, 0.98140625, 0.9546875, 0.96734375, 1.0, 0.9796875, 0.93640625, 0.97828125, 0.9275, 0.935625, 0.995, 0.9959375, 0.9678125, 1.0, 0.99265625, 0.9490625, 0.925, 0.95796875, 0.9965625, 0.99265625, 0.926875, 0.94921875, 1.0, 0.930625, 0.81, 0.94359375, 0.91359375, 0.8528125, 0.9525, 0.9715625, 0.97703125, 0.936875, 0.89125, 0.9575, 0.96890625, 1.0, 0.898125, 0.986875, 0.948125, 0.9234375, 0.9815625, 0.99734375, 0.9275, 0.8803125, 0.9846875, 1.0, 0.82859375, 0.93484375, 1.0, 1.0, 0.92015625, 0.9603125, 0.9515625, 0.90546875, 0.9809375, 0.8715625, 0.905625, 1.0, 1.0, 0.95328125, 0.97140625, 0.964375, 1.0, 1.0, 0.97875, 0.9425, 0.96515625, 0.97296875, 0.97109375, 1.0, 1.0, 0.923125, 0.95421875, 1.0, 0.97921875, 0.95421875, 0.99890625, 0.858125, 0.91140625, 0.981875, 0.90515625, 0.92875, 0.99515625, 1.0, 0.98546875, 0.9565625, 0.9565625, 0.9571875, 1.0, 0.9521875, 1.0, 0.98109375, 1.0, 0.9640625, 0.96828125, 0.9246875, 0.998125, 0.9815625, 1.0, 1.0, 0.92140625, 1.0, 0.98703125, 0.9284375, 0.98890625, 0.87328125, 0.91484375, 0.910625, 0.96765625, 0.991875, 0.98203125, 1.0, 1.0, 0.9815625, 0.896875, 0.9915625, 0.95359375, 0.94953125, 0.88890625, 0.964375, 0.9915625, 0.95921875, 0.99234375, 1.0, 0.9115625, 1.0, 0.9471875, 0.92953125, 0.8434375, 0.9825, 1.0, 0.993125, 0.9915625, 0.95140625, 0.9925, 1.0, 1.0, 0.9521875, 0.97515625, 0.903125, 0.99359375, 0.97953125, 0.97015625, 0.995625, 0.99734375, 1.0, 0.88984375, 0.9596875, 0.8696875, 0.9903125, 1.0, 0.92453125, 0.94796875, 1.0, 0.929375, 1.0, 0.89484375, 0.85203125, 0.9846875, 0.96890625, 0.95734375, 1.0, 0.89453125, 0.9625, 1.0, 0.86515625, 0.99828125, 0.9675, 0.89484375, 0.98109375, 0.96359375, 0.94953125, 0.938125, 0.8340625, 0.9803125, 1.0, 0.91625, 1.0, 1.0, 0.99765625, 0.89234375, 0.9909375, 0.93046875, 1.0, 0.99796875, 0.9734375, 0.96171875, 1.0, 0.995625, 1.0, 1.0, 0.98046875, 1.0, 0.98765625, 1.0, 0.95515625, 0.980625, 0.9503125, 0.99015625, 0.90953125, 0.9846875, 1.0, 1.0, 0.99828125, 0.99671875, 0.9546875, 0.9690625, 1.0, 1.0, 0.97578125, 0.9053125, 1.0, 0.85421875, 0.84515625, 0.8884375, 0.98921875, 0.8990625, 0.905625, 0.96890625, 1.0, 0.9953125, 0.98703125, 1.0, 0.9834375, 1.0, 0.87109375, 0.89359375, 1.0, 0.889375, 1.0, 1.0, 1.0, 1.0, 0.8853125, 0.97078125, 0.981875, 0.93328125, 0.9771875, 0.944375, 1.0, 0.87234375, 0.89390625, 0.8921875, 0.96828125, 0.9184375, 0.94984375, 0.8846875, 0.99453125, 1.0, 0.83390625, 0.871875, 0.9125, 1.0, 0.79828125, 0.954375, 0.98328125, 0.97953125, 1.0, 0.8403125, 0.99203125, 1.0, 0.8359375, 1.0, 0.99671875, 0.9746875, 0.904375, 0.89828125, 1.0, 0.948125, 0.958125, 1.0, 0.97671875, 0.906875, 0.7621875, 0.96953125, 1.0, 0.96859375, 0.993125, 0.87515625, 0.919375, 0.9490625, 1.0, 0.95828125, 1.0, 1.0, 0.93359375, 1.0, 0.983125, 0.95203125, 1.0, 1.0, 0.86578125, 0.93421875, 0.984375, 0.95671875, 0.813125, 0.99609375, 0.9415625, 0.93484375, 0.91234375, 0.97328125, 0.9884375, 0.95578125, 0.9859375, 1.0, 0.90421875, 0.81390625, 1.0, 1.0, 1.0, 1.0, 0.9715625, 0.97734375, 0.86, 0.93921875, 0.96734375, 0.90671875, 0.9784375, 0.974375, 0.99828125, 1.0, 0.8378125, 0.97953125, 0.9878125, 0.9846875, 0.9434375, 0.9753125, 0.99578125, 0.94125, 0.98703125, 0.940625, 0.95046875, 0.87546875, 0.93640625, 1.0, 0.9753125, 0.945625, 0.92234375, 0.99046875, 0.86609375, 0.94796875, 0.9615625, 0.961875, 1.0, 0.9946875, 0.9903125, 0.91828125, 0.98015625, 1.0, 1.0, 0.98328125, 1.0, 0.94390625, 0.97125, 0.97515625, 1.0, 0.9965625, 1.0, 1.0, 0.97515625, 1.0, 0.95671875, 1.0, 0.98078125, 0.9640625, 0.9290625, 0.9625, 1.0, 0.9365625, 0.973125, 0.95046875, 0.86640625, 0.99375, 0.91546875, 0.939375, 1.0, 0.966875, 0.9134375, 0.94875, 0.97859375, 0.95984375, 1.0, 0.98421875, 0.96796875, 1.0, 1.0, 0.9196875, 0.94703125, 0.930625, 1.0, 0.8828125, 1.0, 0.9159375, 1.0, 0.96328125, 1.0, 1.0, 0.95671875, 0.8734375, 1.0, 0.989375, 0.92578125, 0.9746875, 1.0, 0.9225, 0.86609375, 0.96140625, 0.93046875, 0.95078125, 1.0, 0.9975, 1.0, 0.93859375, 0.99421875, 0.9765625, 1.0, 0.98765625, 0.94359375, 0.9828125, 0.989375, 1.0, 0.8790625, 0.9834375, 0.945, 0.9946875, 1.0, 0.84484375, 0.93859375, 0.93203125, 0.9928125, 0.9703125, 0.9628125, 1.0, 0.94828125, 0.9809375, 0.9384375, 0.943125, 0.9446875, 0.97203125, 0.90671875, 0.8603125, 0.95859375, 0.95609375, 1.0, 0.98890625, 0.9853125, 0.945625, 0.9790625, 0.85125, 1.0, 1.0, 1.0, 0.9128125, 1.0, 0.87515625, 1.0, 1.0, 0.99796875, 0.966875, 0.9253125, 1.0, 0.9334375, 0.98671875, 0.9878125, 0.94671875, 1.0, 1.0, 0.938125, 0.90921875, 0.983125, 0.8890625, 0.9171875, 0.96953125, 0.8771875, 0.96015625, 1.0, 0.96046875, 0.96234375, 0.983125, 0.9971875, 0.88515625, 1.0, 0.98671875, 0.98046875, 0.884375, 1.0, 1.0, 1.0, 0.97265625, 0.97296875, 0.799375, 0.968125, 0.98796875, 0.98046875, 0.98578125, 0.9490625, 1.0, 0.9228125, 0.93875, 0.96421875, 1.0, 0.95359375, 0.9378125, 0.855, 1.0, 0.92140625, 0.8753125, 0.9634375, 0.8740625, 0.9440625, 0.97625, 0.9984375, 1.0, 0.929375, 0.97765625, 0.90265625, 0.9753125, 1.0, 0.99234375, 0.89171875, 0.9715625, 0.9809375, 0.9340625, 0.98203125, 0.96796875, 0.98109375, 0.9928125, 0.995, 0.98828125, 0.9559375, 1.0, 0.963125, 0.9921875, 1.0, 0.98703125, 0.8984375, 1.0, 0.95953125, 0.9728125, 1.0, 1.0, 0.86578125, 0.9853125, 0.95875, 1.0, 0.99375, 0.9146875, 0.97453125, 0.94828125, 0.98234375, 0.944375, 0.9146875, 1.0, 0.93046875, 1.0, 0.99015625, 1.0, 0.91921875, 0.98375, 1.0, 0.86046875, 1.0, 0.93140625, 1.0, 0.9278125, 0.9953125, 0.9790625, 1.0, 0.9275, 0.8775, 0.96765625, 1.0, 0.83859375, 0.91359375, 0.9309375, 0.88234375, 1.0, 1.0, 0.98859375, 1.0, 0.96390625, 1.0, 0.99703125, 0.958125, 1.0, 0.92890625, 0.9834375, 0.91859375, 0.90984375, 1.0, 0.93375, 0.93453125, 0.99390625, 0.990625, 0.90046875, 0.95390625, 0.94125, 1.0, 1.0, 1.0, 0.976875, 0.9278125, 0.9653125, 0.9740625, 0.9946875, 0.89703125, 0.89, 0.94203125, 1.0, 0.96140625, 0.9165625, 0.94046875, 0.86078125, 0.99984375, 0.90234375, 1.0, 1.0, 0.9428125, 0.79484375, 1.0, 1.0, 0.92984375, 0.9159375, 0.87828125, 0.93796875, 1.0, 0.91921875, 0.82375, 0.97703125, 0.9065625, 1.0, 1.0, 0.99796875, 0.9525, 0.993125, 0.974375, 0.96265625, 0.92859375, 0.9825, 1.0, 0.96578125, 0.9946875, 0.840625, 1.0, 0.98984375, 0.90125, 0.9228125, 1.0, 0.9690625, 0.96359375, 0.97828125, 0.8865625, 0.9271875, 0.9759375, 0.92953125, 1.0, 0.998125, 1.0, 0.97765625, 0.93140625, 0.9771875, 1.0, 0.875, 0.89296875, 0.991875, 0.96578125, 1.0, 0.93953125, 0.93359375, 0.88140625, 0.95703125, 0.98609375, 0.956875, 0.91609375, 0.9490625, 1.0, 0.871875, 0.9471875, 1.0, 0.956875, 0.98796875, 0.8953125, 0.95203125, 0.9259375, 0.99484375, 0.82125, 0.9171875, 0.95078125, 0.9875, 0.91125, 0.99390625, 1.0, 0.975625, 0.9534375, 0.905625, 1.0, 0.9271875, 0.945625, 0.93046875, 1.0, 0.94921875, 0.92078125, 0.90296875, 1.0, 1.0, 0.986875, 0.91234375, 0.91859375, 0.97203125, 0.88640625, 1.0, 1.0, 0.90828125, 0.9840625, 0.773125, 0.99265625, 1.0, 0.99984375, 0.99296875, 0.9265625, 0.9675, 0.8034375, 0.85390625, 0.91828125, 0.99015625, 0.99875, 1.0, 1.0, 0.99109375, 1.0, 0.9746875, 1.0, 1.0, 1.0, 0.878125, 0.981875, 0.92578125, 1.0, 1.0, 0.9934375, 0.91203125, 1.0, 0.97625, 0.92109375, 0.981875, 0.99953125, 0.98140625, 0.86203125, 0.8859375, 0.98484375, 0.89421875, 0.99890625, 0.93109375, 0.95109375, 0.93015625, 0.97078125, 0.99484375, 0.8875, 0.9384375, 0.95109375, 0.94234375, 0.9553125, 1.0, 0.91796875, 0.94015625, 0.98203125, 0.95796875, 0.95171875, 1.0, 0.83390625, 0.9603125, 0.9221875, 0.93046875, 0.9634375, 0.96953125, 0.8975, 0.91640625, 0.9475, 0.99109375, 0.9540625, 0.9690625, 1.0, 1.0, 0.92734375, 1.0, 0.9703125, 1.0, 0.97953125, 0.97734375, 0.86578125, 1.0, 0.8590625, 0.97265625, 0.9809375, 0.90984375, 1.0, 0.91859375, 1.0, 0.9896875, 0.86890625, 0.94421875, 0.91046875, 0.81359375, 0.97625, 0.8821875, 1.0, 0.96890625, 0.9115625, 0.9065625, 0.9834375, 1.0, 0.86140625, 0.89734375, 1.0, 0.97859375, 0.96109375, 0.96015625, 0.8528125, 1.0, 0.90515625, 1.0, 0.98921875, 1.0, 0.9996875, 0.91875, 0.96171875, 0.9634375, 1.0, 0.95359375, 0.9715625, 0.933125, 0.94234375, 0.9925, 0.898125, 0.94578125, 0.990625, 0.9821875, 0.80203125, 0.999375, 0.9925, 1.0, 0.88109375, 0.90265625, 0.98015625, 0.9571875, 0.89078125, 0.88875, 1.0, 1.0, 0.863125, 0.95203125, 0.99234375, 0.905, 0.9696875, 0.9228125, 0.8784375, 0.86125, 0.9471875, 0.99421875, 1.0, 0.95984375, 1.0, 0.9525, 0.95390625, 0.905625, 1.0, 1.0, 0.97359375, 0.97171875, 0.9809375, 1.0, 1.0, 0.9015625, 0.8578125, 0.83078125, 0.91484375, 0.84296875, 1.0, 0.9059375, 1.0, 1.0, 0.98390625, 1.0, 1.0, 0.97171875, 0.8725, 1.0, 0.98328125, 1.0, 1.0, 0.9471875, 1.0, 0.978125, 0.97625, 0.950625, 0.963125, 0.93015625, 0.91703125, 0.995, 0.934375, 1.0, 0.9153125, 1.0, 1.0, 0.96109375, 0.92859375, 0.92578125, 1.0, 1.0, 1.0, 1.0, 0.9084375, 0.916875, 0.97734375, 1.0, 0.92703125, 0.93296875, 0.96671875, 0.85296875, 0.7715625, 0.969375, 0.97671875, 0.98890625, 1.0, 1.0, 1.0, 0.84875, 0.97046875, 0.8465625, 0.93140625, 1.0, 1.0, 0.7475, 0.91140625, 1.0, 0.985625, 0.9475, 0.925, 0.92859375, 0.85875, 0.99, 1.0, 0.99796875, 0.97859375, 1.0, 0.90890625, 1.0, 0.8665625, 0.9521875, 1.0, 1.0, 0.9659375, 1.0, 0.96890625, 0.95453125, 1.0, 0.98984375, 0.9946875, 0.9553125, 0.9159375, 1.0, 0.91890625, 0.9909375, 0.818125, 1.0, 0.97515625, 1.0, 0.99515625, 0.99828125, 0.91984375, 0.8003125, 0.9325, 0.93765625, 1.0, 0.98484375, 0.9953125, 0.9796875, 0.9753125, 0.98125, 1.0, 0.94640625, 0.99375, 1.0, 0.95078125, 0.959375, 0.98171875, 0.949375, 0.91765625, 1.0, 0.8975, 0.973125, 0.87953125, 1.0, 1.0, 0.88578125, 0.97421875, 0.95515625, 0.96875, 1.0, 0.9475, 1.0, 1.0, 0.9409375, 0.95421875, 0.88453125, 0.9046875, 0.8809375, 0.985, 1.0, 0.85, 0.97453125, 1.0, 1.0, 1.0, 0.98078125, 0.99125, 0.98953125, 0.97625, 1.0, 0.97203125, 1.0, 0.78734375, 0.97328125, 0.9975, 1.0, 1.0, 0.94703125, 0.9921875, 1.0, 0.869375, 1.0, 0.96125, 0.93609375, 1.0, 0.9265625, 0.99765625, 1.0, 1.0, 0.92515625, 0.92875, 1.0, 0.919375, 0.9259375, 0.92953125, 0.9928125, 1.0, 0.99484375, 0.88578125, 1.0, 0.91078125, 0.95859375, 1.0, 0.96984375, 0.9090625, 0.9528125, 0.93375, 1.0, 0.905, 0.9365625, 0.96984375, 0.915625, 0.9646875, 0.93546875, 1.0, 1.0, 0.9609375, 0.96921875, 0.9846875, 0.93203125, 0.88890625, 0.9378125, 0.91265625, 0.97484375, 0.90234375, 0.90375, 0.9840625, 1.0, 0.99859375, 1.0, 0.970625, 0.99453125, 0.9678125, 0.99390625, 0.9565625, 0.996875, 0.9696875, 1.0, 0.999375, 1.0, 1.0, 0.95375, 1.0, 0.88, 0.9840625, 1.0, 0.96015625, 0.976875, 1.0, 1.0, 0.946875, 1.0, 1.0, 0.97484375, 0.9459375, 0.9909375, 0.94546875, 0.92203125, 0.9509375, 1.0, 1.0, 0.89078125, 0.81046875, 0.94296875, 0.953125, 0.98796875, 0.91046875, 0.87125, 0.93875, 0.97953125, 0.97625, 0.98109375, 0.95078125, 0.9425, 1.0, 1.0, 0.9428125, 0.896875, 0.9709375, 0.831875, 0.90109375, 0.88171875, 0.91109375, 0.874375, 0.92421875, 1.0, 1.0, 0.9225, 0.92984375, 1.0, 1.0, 0.954375, 0.99703125, 0.96734375, 0.974375, 0.99125, 0.99265625, 0.940625, 0.895625, 0.83265625, 0.99125, 1.0, 0.99640625, 1.0, 0.875625, 1.0, 0.971875, 0.995625, 0.97859375, 0.93, 0.9415625, 0.898125, 1.0, 0.929375, 1.0, 0.96625, 0.9046875, 1.0, 0.933125, 0.99484375, 0.9453125, 0.96390625, 0.97078125, 1.0, 1.0, 0.95953125, 0.9434375, 1.0, 1.0, 0.9715625, 1.0, 1.0, 0.85375, 1.0, 0.96515625, 0.84, 0.970625, 0.93734375, 0.94609375, 0.9853125, 0.9809375, 0.90265625, 1.0, 0.98078125, 0.96328125, 1.0, 0.9778125, 1.0, 0.998125, 1.0, 0.9196875, 1.0, 0.991875, 0.9771875, 0.86203125, 0.9221875, 1.0, 1.0, 1.0, 0.94125, 0.911875, 1.0, 0.99, 1.0, 1.0, 0.90203125, 0.84234375, 0.97984375, 0.86515625, 0.91984375, 1.0, 0.9315625, 0.97359375, 0.97703125, 0.9265625, 0.93484375, 1.0, 0.9590625, 0.9715625, 1.0, 0.98921875, 0.84453125, 0.9428125, 1.0, 0.8296875, 0.96921875, 0.83203125, 1.0, 0.98359375, 0.8359375, 0.94796875, 1.0, 0.98171875, 0.876875, 1.0, 1.0, 0.896875, 0.955625, 0.956875, 0.913125, 0.87203125, 0.97171875, 0.991875, 0.986875, 0.98515625, 0.99671875, 0.99, 0.87984375, 0.89734375, 0.986875, 0.98125, 0.99109375, 0.96734375, 1.0, 0.9359375, 1.0, 0.9065625, 0.9, 0.98328125, 0.9125, 0.99390625, 1.0, 0.94140625, 0.95125, 0.97921875, 0.93875, 0.974375, 0.98984375, 0.9434375, 0.91828125, 0.90515625, 0.95203125, 0.9903125, 1.0, 1.0, 0.90734375, 1.0, 0.89140625, 0.96015625, 0.9309375, 0.9990625, 0.9421875, 0.8753125, 0.8778125, 1.0, 0.9703125, 0.843125, 0.97609375, 1.0, 0.9315625, 0.99703125, 0.98125, 1.0, 0.99765625, 0.9925, 0.74125, 0.73421875, 1.0, 0.86328125, 0.93171875, 1.0, 1.0, 1.0, 0.88578125, 0.955, 1.0, 0.9903125, 0.9846875, 0.950625, 0.97546875, 0.90828125, 0.9909375, 1.0, 0.9678125, 0.93828125, 0.8315625, 0.999375, 0.87609375, 0.92421875, 0.91515625, 1.0, 1.0, 1.0, 0.8821875, 0.98921875, 0.9771875, 0.96953125, 0.9490625, 1.0, 0.89265625, 0.98609375, 0.9640625, 0.9346875, 0.9509375, 0.89765625, 0.97625, 1.0, 0.961875, 0.98390625, 0.9653125, 1.0, 0.930625, 0.99, 0.91140625, 0.92703125, 1.0, 0.95296875, 0.9140625, 0.95859375, 1.0, 1.0, 1.0, 0.97015625, 0.9371875, 0.9471875, 0.9578125, 0.9409375, 1.0, 0.9990625, 0.97921875, 1.0, 1.0, 0.9209375, 1.0, 0.9996875, 0.9275, 1.0, 0.90890625, 1.0, 1.0, 0.87578125, 1.0, 0.9496875, 0.92265625, 0.961875, 0.9896875, 0.9553125, 1.0, 0.90796875, 1.0, 0.990625, 0.9921875, 1.0, 1.0, 0.8909375, 0.9825, 1.0, 0.933125, 0.9959375, 0.98625, 0.98203125, 1.0, 0.9084375, 0.98453125, 0.955625, 0.98640625, 0.938125, 0.8865625, 0.93421875, 0.88359375, 0.9009375, 1.0, 0.95015625, 0.78359375, 0.97640625, 0.96265625, 0.96734375, 0.89828125, 1.0, 0.96140625, 0.96859375, 0.9915625, 0.99078125, 1.0, 0.88484375, 1.0, 0.79453125, 0.87796875, 1.0, 0.944375, 0.929375, 0.88515625, 1.0, 0.9365625, 0.9340625, 0.9771875, 0.9690625, 0.90625, 0.84578125, 0.9678125, 0.94703125, 0.97671875, 0.915625, 0.94734375, 1.0, 1.0, 0.99203125, 1.0, 1.0, 0.92875, 0.98515625, 0.99078125, 0.9921875, 0.889375, 0.94390625, 0.93140625, 1.0, 0.99890625, 0.8746875, 0.97375, 0.87484375, 0.97328125, 1.0, 1.0, 0.875625, 1.0, 1.0, 1.0, 0.99609375, 0.99234375, 0.99296875, 0.97171875, 0.94734375, 1.0, 0.960625, 0.8890625, 1.0, 0.845625, 0.99, 0.86265625, 0.97984375, 0.8746875, 1.0, 1.0, 0.96546875, 0.9753125, 0.90328125, 0.92109375, 0.9703125, 0.97828125, 1.0, 0.98984375, 0.98484375, 0.97375, 1.0, 0.873125, 1.0, 0.983125, 0.99890625, 0.99640625, 0.9684375, 0.9921875, 0.9971875, 0.8040625, 0.9409375, 0.99015625, 0.99921875, 0.9415625, 0.99015625, 0.950625, 0.883125, 0.95203125, 0.956875, 0.988125, 0.9096875, 0.92875, 0.9703125, 0.956875, 1.0, 1.0, 0.8734375, 0.965625, 0.92078125, 0.95984375, 0.93703125, 1.0, 0.99046875, 1.0, 1.0, 0.97109375, 0.94609375, 1.0, 0.9615625, 0.9534375, 0.97296875, 0.9759375, 1.0, 0.92265625, 0.99390625, 0.891875, 0.993125, 1.0, 0.93453125, 0.9315625, 0.96265625, 1.0, 1.0, 1.0, 1.0, 0.97484375, 0.96671875, 0.95359375, 1.0, 0.98390625, 0.96703125, 0.9421875, 0.99765625, 0.890625, 0.916875, 0.9840625, 0.9540625, 0.97515625, 1.0, 1.0, 0.9978125, 0.96421875, 0.90140625, 0.9575, 0.866875, 0.88609375, 0.87734375, 0.96734375, 0.82703125, 0.8678125, 0.844375, 0.89859375, 1.0, 0.83859375, 0.98671875, 0.94140625, 0.84828125, 1.0, 0.92453125, 0.9271875, 0.91265625, 0.994375, 0.95578125, 0.8809375, 0.986875, 1.0, 0.92234375, 0.99953125, 0.84640625, 0.995, 0.95953125, 1.0, 1.0, 0.97, 0.84796875, 0.94109375, 1.0, 0.9759375, 1.0, 0.99484375, 0.96625, 0.9465625, 0.88296875, 0.70875, 1.0, 0.92828125, 0.9328125, 0.91828125, 0.93296875, 0.96296875, 0.99046875, 0.94578125, 0.93953125, 0.90125, 0.9303125, 0.94828125, 0.87296875, 1.0, 0.95890625, 0.93078125, 0.9565625, 1.0, 0.91, 0.9871875, 0.99015625, 0.99109375, 0.89796875, 0.97765625, 0.95796875, 0.99609375, 1.0, 0.9659375, 1.0, 0.9978125, 0.98140625, 0.875625, 0.97015625, 0.98609375, 1.0, 0.98828125, 1.0, 0.9984375, 0.96125, 0.98546875, 0.9625, 0.9575, 0.99015625, 0.9921875, 0.91609375, 1.0, 0.92296875, 0.89671875, 0.85734375, 1.0, 1.0, 0.9375, 0.97609375, 0.9446875, 1.0, 0.8975, 0.883125, 0.8990625, 0.90953125, 0.87921875, 1.0, 0.9375, 1.0, 1.0, 0.8725, 0.93515625, 0.9653125, 0.9903125, 0.87734375, 0.97578125, 1.0, 1.0, 0.8653125, 1.0, 0.95421875, 0.83828125, 0.95921875, 0.90875, 0.9546875, 1.0, 1.0, 0.9434375, 0.98578125, 0.9465625, 0.9771875, 0.995625, 1.0, 0.9921875, 0.9921875, 0.9996875, 0.98296875, 0.89171875, 0.9509375, 0.989375, 0.96, 1.0, 0.981875, 0.97140625, 1.0, 0.9271875, 0.83703125, 0.90890625, 0.9953125, 0.94796875, 0.97890625, 0.98078125, 0.94515625, 0.90828125, 0.9015625, 0.891875, 0.990625, 0.96609375, 0.980625, 0.90796875, 0.94859375, 0.96484375, 0.9521875, 1.0, 1.0, 0.91703125, 1.0, 1.0, 0.95625, 0.95125, 0.93265625, 1.0, 0.9484375, 0.98328125, 0.95234375, 0.89453125, 0.85859375, 0.91640625, 0.984375, 0.93734375, 0.974375, 0.99375, 0.92859375, 0.9740625, 0.89109375, 0.964375, 0.94640625, 0.86953125, 0.96, 0.97671875, 0.87921875, 1.0, 0.906875, 0.94625, 0.971875, 1.0, 0.97875, 0.98984375, 0.85984375, 1.0, 1.0, 0.88640625, 0.93359375, 0.9203125, 0.9928125, 1.0, 0.9375, 0.9671875, 0.82890625, 0.9096875, 0.93125, 0.988125, 1.0, 0.92140625, 0.98734375, 0.99046875, 1.0, 1.0, 0.88765625, 0.986875, 1.0, 0.99078125, 0.9978125, 1.0, 1.0, 0.89375, 0.9853125, 0.969375, 0.9775, 1.0, 0.97046875, 0.9484375, 0.9259375, 1.0, 0.8690625, 0.8553125, 0.98953125, 1.0, 1.0, 0.97921875, 0.945, 0.92, 0.92734375, 0.95984375, 0.954375, 0.96328125, 0.9571875, 0.93453125, 1.0, 0.875625, 0.9946875, 0.92, 1.0, 0.9865625, 0.9403125, 1.0, 0.99453125, 0.9803125, 1.0, 0.94453125, 1.0, 1.0, 0.944375, 1.0, 0.90484375, 0.96484375, 0.9975, 0.96765625, 0.928125, 0.82703125, 0.985, 0.9821875, 0.9028125, 1.0, 0.93734375, 0.9678125, 0.99921875, 0.89125, 1.0, 0.98109375, 0.9434375, 0.92109375, 0.93578125, 0.85328125, 0.98984375, 1.0, 0.97265625, 1.0, 0.97953125, 0.936875, 0.91359375, 1.0, 0.99171875, 0.9428125, 1.0, 1.0, 0.97765625, 1.0, 1.0, 0.9978125, 0.96, 0.99640625, 0.96640625, 0.92015625, 0.98828125, 0.9975, 0.95109375, 0.956875, 0.969375, 0.88421875, 0.94796875, 0.9921875, 1.0, 0.821875, 0.98671875, 1.0, 0.98484375, 0.994375, 1.0, 1.0, 0.9715625, 0.8934375, 0.92515625, 1.0, 0.8840625, 0.966875, 0.7878125, 0.98359375, 0.9359375, 0.97703125, 0.95796875, 1.0, 0.86609375, 0.9590625, 0.98078125, 1.0, 0.9146875, 0.938125, 0.979375, 0.89296875, 0.983125, 0.9765625, 1.0, 1.0, 1.0, 0.8553125, 0.9634375, 0.98953125, 0.85796875, 0.96765625, 0.84703125, 1.0, 0.86421875, 0.93984375, 0.98140625, 0.94234375, 0.9959375, 0.94328125, 0.98609375, 0.91171875, 0.90546875, 0.98015625, 0.9534375, 0.89671875, 0.999375, 0.81703125, 0.935, 0.96390625, 0.9475, 0.95546875, 0.96734375, 0.95484375, 1.0, 1.0, 0.9790625, 0.99015625, 0.97171875, 1.0, 0.9696875, 0.97296875, 0.928125, 0.9234375, 0.90359375, 1.0, 1.0, 0.95296875, 0.884375, 0.98734375, 0.9571875, 0.860625, 1.0, 0.90796875, 0.95078125, 0.9540625, 0.98703125, 0.87875, 0.92328125, 0.975, 0.936875, 0.9696875, 0.87421875, 0.95890625, 0.9990625, 0.98796875, 1.0, 1.0, 0.96046875, 0.96140625, 1.0, 0.88953125, 1.0, 0.9509375, 0.90046875, 0.89265625, 0.9475, 0.98171875, 0.92109375, 1.0, 0.87140625, 1.0, 0.9965625, 0.95875, 0.89515625, 0.9965625, 1.0, 0.87625, 0.95921875, 0.9109375, 0.99984375, 1.0, 1.0, 0.9946875, 1.0, 0.99859375, 1.0, 0.87390625, 0.9446875, 1.0, 1.0, 0.93609375, 0.99328125, 0.95859375, 0.99921875, 0.98796875, 0.92671875, 0.91015625, 0.9859375, 0.9484375, 0.9703125, 0.9890625, 1.0, 0.95984375, 0.95953125, 0.99484375, 0.974375, 1.0, 0.9859375, 0.98484375, 0.90453125, 0.9571875, 1.0, 0.96578125, 0.923125, 0.95078125, 0.9515625, 1.0, 0.99859375, 0.97671875, 0.92796875, 0.95359375, 1.0, 0.99953125, 0.91296875, 0.91609375, 1.0, 0.96984375, 0.87703125, 0.99828125, 0.9353125, 0.91796875, 0.94734375, 0.98953125, 0.96734375, 1.0, 1.0, 1.0, 0.96703125, 0.94875, 0.965625, 0.9475, 0.91390625, 1.0, 0.9028125, 1.0, 0.95984375, 0.94875, 0.96984375, 1.0, 0.95109375, 0.88421875, 0.95265625, 0.9603125, 0.9703125, 0.97953125, 0.953125, 0.9109375, 0.94796875, 0.8871875, 1.0, 0.963125, 1.0, 1.0, 0.9940625, 0.9846875, 0.9259375, 1.0, 0.8853125, 0.8634375, 0.9884375, 1.0, 0.96, 0.900625, 0.87984375, 0.93296875, 0.9059375, 0.9675, 0.759375, 0.92328125, 0.99921875, 0.86625, 0.9984375, 1.0, 0.97421875, 0.96796875, 0.9778125, 1.0, 0.87328125, 0.98265625, 0.81828125, 0.95875, 0.99, 1.0, 1.0, 1.0, 0.99015625, 0.9365625, 1.0, 0.9678125, 1.0, 0.9471875, 1.0, 0.91046875, 0.9215625, 0.9215625, 1.0, 0.87828125, 1.0, 0.985, 0.96453125, 1.0, 0.9909375, 0.9459375, 0.87171875, 0.891875, 0.91640625, 0.959375, 0.960625, 0.994375, 0.9859375, 0.84671875, 0.9315625, 0.89390625, 0.82359375, 0.98140625, 1.0, 0.78359375, 1.0, 0.97, 0.97125, 0.9728125, 1.0, 0.89265625, 0.99234375, 1.0, 0.9596875, 0.87875, 0.99921875, 0.9275, 0.98625, 1.0, 0.98421875, 0.95515625, 1.0, 1.0, 0.9009375, 0.945625, 0.964375, 1.0, 0.92109375, 1.0, 0.93921875, 0.9075, 0.9171875, 0.87765625, 0.99953125, 0.99421875, 0.99609375, 0.97671875, 1.0, 0.85578125, 0.96140625, 0.9971875, 0.96078125, 0.96515625, 0.9246875, 0.97203125, 0.9125, 0.94546875, 0.92359375, 0.99890625, 0.928125, 0.985, 0.9790625, 1.0, 0.95625, 0.9890625, 0.996875, 0.9734375, 1.0, 0.9321875, 0.9415625, 0.896875, 0.93703125, 0.95734375, 0.85875, 0.98921875, 0.96, 0.90484375, 0.98984375, 1.0, 0.9084375, 0.97734375, 0.94703125, 0.881875, 0.9859375, 0.98359375, 0.91296875, 1.0, 0.91359375, 0.97140625, 0.97234375, 0.9284375, 0.9315625, 0.9434375, 0.98984375, 0.98359375, 0.94828125, 0.96953125, 0.88578125, 0.941875, 0.92796875, 0.97765625, 0.93140625, 0.88265625, 0.9884375, 0.9325, 0.8778125, 0.95546875, 0.91046875, 0.9528125, 0.90328125, 0.94078125, 1.0, 1.0, 0.91046875, 0.9753125, 0.999375, 0.90390625, 1.0, 1.0, 0.94546875, 0.96953125, 0.89171875, 1.0, 0.949375, 0.960625, 0.98640625, 1.0, 0.985, 0.9709375, 0.94640625, 0.939375, 1.0, 1.0, 1.0, 0.94828125, 1.0, 0.98765625, 0.90859375, 0.87578125, 0.98359375, 0.9721875, 0.99203125, 0.92015625, 0.92015625, 0.8290625, 0.89640625, 0.99296875, 0.99109375, 0.9021875, 1.0, 1.0, 0.90359375, 0.98546875, 0.888125, 0.9946875, 0.9975, 0.9684375, 0.91796875, 0.958125, 0.89546875, 0.92859375, 0.98234375, 1.0, 0.908125, 0.91265625, 0.96921875, 0.9671875, 0.9434375, 0.83, 0.91546875, 0.94453125, 1.0, 0.8853125, 0.99203125, 0.97453125, 1.0, 0.99671875, 0.97890625, 0.92796875, 0.89875, 0.89609375, 0.9740625, 0.95875, 0.9234375, 1.0, 0.9396875, 1.0, 0.94390625, 0.93515625, 0.97578125, 0.961875, 0.86875, 0.91375, 0.94828125, 0.971875, 0.974375, 0.7865625, 1.0, 1.0, 1.0, 0.9703125, 0.91953125, 0.92140625, 1.0, 0.9953125, 1.0, 0.94109375, 0.8846875, 0.90109375, 0.90703125, 0.968125, 0.95125, 0.92359375, 1.0, 0.92703125, 0.93765625, 0.8303125, 0.960625, 0.9528125, 0.984375, 0.9559375, 0.95328125, 0.9446875, 0.95859375, 1.0, 0.988125, 1.0, 0.93953125, 0.93125, 0.98453125, 1.0, 0.980625, 0.980625, 0.8875, 0.94125, 1.0, 0.979375, 0.9409375, 0.86515625, 1.0, 0.96703125, 0.8859375, 1.0, 0.99421875, 0.858125, 0.945, 0.995625, 1.0, 0.92359375, 0.96796875, 0.96828125, 1.0, 1.0, 0.91515625, 0.94609375, 0.9878125, 0.9203125, 1.0, 0.96125, 0.99171875, 0.899375, 1.0, 0.87921875, 0.92390625, 1.0, 1.0, 0.9425, 1.0, 0.9871875, 1.0, 0.9809375, 1.0, 0.8971875, 1.0, 0.9803125, 0.9778125, 0.95578125, 0.98109375, 1.0, 1.0, 0.99859375, 0.96703125, 0.895, 0.88890625, 1.0, 0.95875, 0.9990625, 0.99640625, 0.9503125, 1.0, 1.0, 1.0, 0.94140625, 0.98640625, 0.92421875, 0.8896875, 0.931875, 0.93265625, 1.0, 0.9603125, 0.971875, 0.95765625, 0.84078125, 0.9578125, 0.93796875, 1.0, 0.8421875, 1.0, 0.9509375, 1.0, 0.98546875, 0.96609375, 0.96546875, 0.82203125, 0.98296875, 0.98203125, 1.0, 0.95609375, 0.766875, 0.94390625, 1.0, 0.94890625, 0.9971875, 0.9034375, 0.91828125, 0.959375, 0.9534375, 0.998125, 0.9959375, 0.81015625, 1.0, 0.956875, 0.92296875, 0.886875, 0.86125, 0.9596875, 0.97859375, 0.99546875, 0.984375, 0.94109375, 0.95609375, 0.901875, 0.95890625, 0.973125, 1.0, 0.93828125, 0.935, 0.9871875, 0.90484375, 0.934375, 0.81953125, 0.946875, 0.95609375, 0.87765625, 0.9809375, 0.9878125, 0.99046875, 0.995, 0.8953125, 0.97875, 0.93171875, 0.9165625, 0.8571875, 0.98015625, 0.9525, 0.978125, 1.0, 0.873125, 0.98875, 0.999375, 0.844375, 0.98578125, 0.97359375, 0.91546875, 0.94875, 0.89328125, 0.95421875, 1.0, 1.0, 0.9609375, 0.98625, 0.9534375, 0.93125, 0.95234375, 1.0, 1.0, 0.95984375, 0.9784375, 0.88984375, 0.89921875, 0.835, 0.94796875, 0.96015625, 0.96140625, 0.8503125, 0.87921875, 0.840625, 0.991875, 0.903125, 0.9653125, 1.0, 0.9225, 1.0, 0.9528125, 0.98453125, 0.90671875, 0.93140625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.861875, 0.9075, 0.945, 0.929375, 0.92328125, 0.99375, 0.90515625, 0.94453125, 0.9753125, 1.0, 0.96203125, 0.8840625, 1.0, 0.93765625, 1.0, 1.0, 0.97375, 0.985625, 1.0, 1.0, 0.9503125, 1.0, 1.0, 0.93859375, 0.93046875, 0.91328125, 0.92765625, 1.0, 1.0, 0.9696875, 0.963125, 0.99609375, 0.9321875, 1.0, 0.94359375, 0.98515625, 0.8565625, 0.85625, 0.97796875, 0.96515625, 0.95859375, 0.96765625, 1.0, 1.0, 1.0, 0.9725, 1.0, 0.9228125, 1.0, 0.98625, 1.0, 0.933125, 0.87796875, 0.9659375, 0.93859375, 0.99734375, 0.96140625, 0.96671875, 0.9225, 0.9003125, 1.0, 0.9290625, 1.0, 0.97515625, 0.98359375, 0.989375, 0.96453125, 0.8475, 0.97421875, 0.901875, 1.0, 0.94390625, 0.96203125, 0.9990625, 1.0, 1.0, 0.9290625, 0.89359375, 0.7775, 0.891875, 1.0, 0.9165625, 0.97, 0.90515625, 1.0, 1.0, 0.8053125, 0.92828125, 0.8946875, 0.9765625, 0.8725, 0.96046875, 1.0, 0.974375, 1.0, 0.99515625, 0.96859375, 0.9371875, 0.93921875, 1.0, 0.98375, 1.0, 0.94875, 0.86546875, 0.9575, 0.89171875, 0.8878125, 0.9815625, 0.9465625, 1.0, 0.97109375, 0.916875, 0.96515625, 0.99953125, 0.88625, 1.0, 0.87609375, 1.0, 0.91515625, 0.98546875, 1.0, 1.0, 0.9821875, 0.8875, 1.0, 0.93484375, 0.969375, 0.9875, 1.0, 0.943125, 1.0, 0.956875, 1.0, 0.9675, 0.99046875, 0.9546875, 1.0, 0.909375, 0.9571875, 0.90484375, 0.90484375, 0.893125, 0.89015625, 0.91109375, 1.0, 0.91390625, 1.0, 1.0, 0.97109375, 0.9515625, 0.944375, 0.9846875, 0.97890625, 0.9075, 0.85515625, 0.97734375, 0.98625, 0.95921875, 0.97828125, 0.865625, 0.94265625, 1.0, 1.0, 0.9721875, 0.85953125, 1.0, 0.97609375, 1.0, 0.94234375, 0.91859375, 0.92734375, 0.86953125, 1.0, 0.9746875, 0.9415625, 0.88421875, 0.98703125, 0.93828125, 0.95140625, 1.0, 0.96421875, 0.988125, 0.97625, 0.9921875, 0.97359375, 0.89828125, 0.9771875, 1.0, 1.0, 0.983125, 0.99515625, 0.9465625, 0.9584375, 0.995, 0.9371875, 1.0, 0.8603125, 0.98921875, 0.960625, 0.86765625, 0.93296875, 0.97578125, 0.94859375, 0.87625, 0.95203125, 0.9853125, 1.0, 0.9884375, 0.92328125, 0.93625, 0.974375, 0.97875, 0.956875, 0.8953125, 0.96328125, 0.9871875, 0.94359375, 0.98078125, 0.9490625, 0.98234375, 0.97453125, 0.90796875, 0.93546875, 1.0, 0.8928125, 1.0, 0.88609375, 1.0, 1.0, 0.98328125, 0.9875, 0.92734375, 0.9278125, 1.0, 0.935, 1.0, 0.81125, 1.0, 1.0, 0.98125, 0.99734375, 0.984375, 0.94875, 1.0, 1.0, 0.97484375, 0.97390625, 0.9178125, 0.92125, 0.97140625, 0.98625, 0.8528125, 0.99875, 0.9915625, 1.0, 0.92125, 0.989375, 1.0, 0.95578125, 0.97921875, 0.99390625, 0.97453125, 0.93359375, 1.0, 0.95640625, 0.93546875, 0.97515625, 0.94203125, 0.925, 1.0, 0.9325, 1.0, 1.0, 0.9071875, 0.97765625, 0.985625, 1.0, 0.94734375, 1.0, 1.0, 0.92453125, 0.94125, 0.99953125, 0.9675, 0.9328125, 1.0, 0.88515625, 0.8928125, 0.97890625, 0.99421875, 1.0, 0.980625, 0.9625, 0.9396875, 0.9825, 1.0, 0.966875, 1.0, 0.93203125, 1.0, 0.70296875, 0.904375, 0.9828125, 0.97265625, 0.9340625, 0.97140625, 0.991875, 1.0, 1.0, 0.973125, 0.96484375, 0.8771875, 0.9703125, 0.95328125, 0.98328125, 1.0, 0.98296875, 0.9575, 1.0, 0.9425, 0.976875, 0.9540625, 1.0, 0.999375, 0.90625, 0.98765625, 0.9125, 0.96125, 1.0, 0.98828125, 0.93078125, 0.91765625, 0.999375, 0.94734375, 1.0, 0.8728125, 0.9765625, 0.9584375, 0.94859375, 0.9759375, 0.97, 0.865625, 0.91984375, 0.97421875, 0.981875, 0.97203125, 0.97328125, 0.9503125, 1.0, 1.0, 1.0, 1.0, 0.9921875, 1.0, 0.9509375, 0.89828125, 0.9821875, 0.985, 0.95140625, 0.996875, 0.9528125, 0.79421875, 0.8984375, 0.9934375, 0.97328125, 1.0, 0.98546875, 0.9978125, 0.97890625, 0.8696875, 0.95421875, 1.0, 0.90140625, 0.81953125, 0.9075, 0.89296875, 0.99109375, 1.0, 0.92828125, 1.0, 0.98515625, 0.9565625, 1.0, 0.995, 0.95421875, 1.0, 0.89859375, 0.94046875, 0.9009375, 1.0, 0.91140625, 1.0, 0.87328125, 0.96921875, 0.82484375, 0.97953125, 0.9796875, 0.89375, 0.8796875, 1.0, 0.84359375, 0.92984375, 0.92890625, 1.0, 0.945, 0.98953125, 1.0, 0.9521875, 0.98453125, 0.9284375, 0.98703125, 0.9196875, 0.9840625, 1.0, 0.999375, 1.0, 0.96, 1.0, 0.9646875, 0.98421875, 0.989375, 0.97578125, 0.9859375, 0.99796875, 1.0, 1.0, 0.9353125, 0.8375, 0.949375, 0.901875, 0.9359375, 0.94265625, 0.99125, 0.96234375, 0.90046875, 1.0, 0.9871875, 0.9878125, 1.0, 0.93328125, 0.91234375, 0.9934375, 1.0, 0.98125, 1.0, 1.0, 0.95421875, 0.86703125, 0.85203125, 0.941875, 0.965, 0.869375, 0.9903125, 1.0, 0.8359375, 1.0, 0.9946875, 0.98234375, 0.96703125, 0.98296875, 1.0, 1.0, 1.0, 1.0, 0.93953125, 0.97890625, 0.975, 0.87265625, 0.87046875, 0.90171875, 0.8921875, 0.9903125, 1.0, 0.91203125, 0.97328125, 0.993125, 0.99453125, 1.0, 0.83890625, 0.894375, 0.9559375, 0.9603125, 0.81328125, 0.96671875, 0.901875, 0.9865625, 0.923125, 0.93890625, 0.88890625, 0.96609375, 0.8709375, 1.0, 0.96578125, 0.9478125, 0.988125, 0.84421875, 0.92078125, 1.0, 0.8003125, 1.0, 0.94640625, 0.865625, 1.0, 0.98984375, 0.879375, 0.96484375, 0.9925, 0.98796875, 1.0, 0.939375, 1.0, 0.9396875, 1.0, 0.89640625, 0.955625, 1.0, 0.82234375, 1.0, 1.0, 0.89734375, 1.0, 0.95296875, 1.0, 0.97875, 0.9975, 0.87296875, 0.94625, 0.9521875, 0.86859375, 1.0, 1.0, 0.93421875, 0.97859375, 0.95859375, 0.9746875, 1.0, 0.9134375, 0.9609375, 0.90921875, 0.95953125, 1.0, 0.986875, 0.86703125, 0.82875, 1.0, 0.98390625, 0.99390625, 1.0, 0.97828125, 0.855, 0.926875, 0.95421875, 0.958125, 0.9440625, 0.95625, 0.96453125, 0.86578125, 1.0, 1.0, 1.0, 0.99578125, 1.0, 0.99328125, 1.0, 1.0, 1.0, 0.98703125, 0.97484375, 0.9878125, 0.9765625, 0.9209375, 1.0, 0.9190625, 0.8540625, 0.92765625, 0.99890625, 0.9896875, 0.9759375, 0.9540625, 0.98890625, 1.0, 1.0, 0.9165625, 0.97109375, 1.0, 1.0, 1.0, 0.94703125, 1.0, 1.0, 0.9559375, 0.895, 0.98390625, 0.9690625, 0.99671875, 0.993125, 1.0, 0.96734375, 1.0, 1.0, 0.958125, 0.94921875, 0.83671875, 0.8584375, 0.96203125, 0.93578125, 0.985, 0.94015625, 0.97046875, 0.9159375, 0.96828125, 0.98109375, 0.9675, 0.981875, 1.0, 0.9778125, 0.94796875, 0.9853125, 1.0, 0.9690625, 0.99, 0.99328125, 1.0, 0.98796875, 1.0, 1.0, 1.0, 0.90921875, 0.9709375, 0.91515625, 0.99984375, 0.9621875, 0.98828125, 0.9425, 1.0, 0.9540625, 0.96765625, 0.9021875, 0.91234375, 0.9496875, 0.95734375, 0.9534375, 0.99765625, 0.99953125, 0.9384375, 0.94109375, 0.91296875, 0.9471875, 1.0, 0.98265625, 0.92640625, 0.85453125, 0.92078125, 0.99875, 1.0, 1.0, 1.0, 1.0, 0.94734375, 0.9484375, 0.963125, 0.94890625, 0.95125, 0.9490625, 0.98734375, 1.0, 0.98703125, 1.0, 0.90140625, 0.97640625, 0.975625, 0.880625, 0.9884375, 0.9734375, 0.96859375, 0.9303125, 1.0, 0.97125, 0.9978125, 0.9003125, 0.98734375, 0.87515625, 0.85625, 0.97671875, 0.89921875, 1.0, 1.0, 1.0, 0.906875, 0.94359375, 0.9765625, 0.88015625, 0.9928125, 0.8096875, 0.89140625, 1.0, 0.96609375, 0.87796875, 1.0, 1.0, 0.9578125, 1.0, 1.0, 1.0, 1.0, 0.97828125, 0.73671875, 1.0, 0.966875, 0.95703125, 1.0, 0.9640625, 0.86125, 1.0, 1.0, 1.0, 0.91609375, 1.0, 0.8746875, 0.9425, 1.0, 0.990625, 0.9921875, 0.93328125, 0.95984375, 1.0, 0.9740625, 0.9903125, 0.95890625, 0.78015625, 0.8975, 1.0, 1.0, 0.98484375, 1.0, 0.98703125, 0.9396875, 0.94546875, 0.98109375, 0.9825, 0.98546875, 0.97828125, 1.0, 0.941875, 0.8825, 0.9675, 1.0, 0.9696875, 0.995625, 0.975625, 1.0, 1.0, 0.90296875, 0.96984375, 0.9634375, 0.9959375, 0.93234375, 0.96625, 0.9934375, 1.0, 0.99640625, 1.0, 1.0, 0.914375, 0.97015625, 0.92734375, 0.97421875, 1.0, 1.0, 0.88859375, 1.0, 0.943125, 0.93609375, 1.0, 0.8740625, 0.989375, 0.9284375, 0.894375, 1.0, 1.0, 0.9609375, 0.9653125, 0.97734375, 0.9303125, 0.8728125, 0.95921875, 0.93859375, 0.99640625, 0.77203125, 0.9621875, 0.8853125, 0.94109375, 0.92125, 1.0, 0.98765625, 0.8634375, 1.0, 0.8978125, 1.0, 0.999375, 0.99390625, 0.89796875, 1.0, 1.0, 0.9509375, 0.9771875, 0.9540625, 1.0, 0.9721875, 0.9121875, 0.97640625, 0.8296875, 0.93, 0.89625, 0.8596875, 0.908125, 1.0, 0.8946875, 0.9571875, 1.0, 0.9975, 0.96109375, 0.94171875, 0.941875, 0.8384375, 0.85375, 0.97296875, 0.959375, 0.96390625, 0.95453125, 1.0, 1.0, 0.93265625, 0.9615625, 1.0, 1.0, 0.90484375, 0.91078125, 1.0, 0.9384375, 0.99796875, 0.9896875, 0.9440625, 0.989375, 0.97265625, 0.9478125, 0.9821875, 0.956875, 1.0, 0.9309375, 1.0, 0.98375, 1.0, 0.99625, 0.95875, 0.99828125, 1.0, 0.93578125, 1.0, 1.0, 0.95828125, 0.9671875, 0.90015625, 0.9665625, 0.855625, 0.9209375, 0.9146875, 0.99484375, 0.8684375, 0.95515625, 0.97640625, 0.9790625, 0.9040625, 0.895, 0.898125, 0.9690625, 1.0, 1.0, 0.96546875, 0.9490625, 0.981875, 0.93046875, 1.0, 0.95109375, 0.854375, 0.986875, 0.97375, 1.0, 0.97265625, 0.87953125, 1.0, 0.9740625, 0.9946875, 1.0, 1.0, 0.97703125, 0.98625, 0.99234375, 0.9284375, 0.965, 0.9996875, 0.87515625, 0.9740625, 0.968125, 0.99296875, 0.96796875, 0.98328125, 0.87, 0.9259375, 0.88296875, 1.0, 0.920625, 1.0, 1.0, 0.96390625, 0.9034375, 1.0, 1.0, 0.9321875, 0.9, 0.97171875, 0.96109375, 0.9859375, 0.93875, 0.95, 0.95828125, 0.90296875, 0.93359375, 0.9603125, 1.0, 0.90328125, 0.88640625, 0.93171875, 0.896875, 0.9515625, 0.9546875, 0.9759375, 0.8634375, 0.93359375, 0.9765625, 0.97515625, 0.99265625, 0.956875, 0.9884375, 1.0, 0.93515625, 0.9696875, 0.87296875, 0.9315625, 0.99265625, 0.94359375, 1.0, 1.0, 0.976875, 1.0, 0.9615625, 0.9071875, 0.99765625, 0.97359375, 0.86546875, 1.0, 0.82828125, 0.86484375, 0.871875, 0.87078125, 0.91375, 1.0, 0.91796875, 0.93765625, 0.9146875, 0.89296875, 0.94125, 0.936875, 0.919375, 0.9559375, 0.8703125, 0.9628125, 1.0, 0.9803125, 1.0, 0.9259375, 0.9890625, 0.99578125, 0.9078125, 0.96296875, 0.9278125, 0.971875, 1.0, 1.0, 0.88015625, 1.0, 0.90703125, 0.9571875, 1.0, 0.925625, 0.88703125, 0.98078125, 0.97375, 0.87765625, 0.96828125, 0.89875, 0.91078125, 0.950625, 1.0, 0.97, 0.9821875, 0.96625, 0.9509375, 0.9928125, 0.993125, 0.92390625, 0.873125, 0.98765625, 0.950625, 0.978125, 0.99828125, 0.901875, 0.89875, 1.0, 1.0, 1.0, 0.99046875, 1.0, 0.97453125, 0.97921875, 1.0, 0.97875, 0.97765625, 0.98234375, 0.97140625, 0.85328125, 0.90625, 1.0, 0.97390625, 0.93140625, 0.97390625, 1.0, 0.95921875, 1.0, 0.96640625, 0.96796875, 0.8628125, 0.99390625, 1.0, 0.9640625, 0.9775, 0.9771875, 0.9478125, 0.91765625, 0.96421875, 0.9365625, 0.87421875, 0.86171875, 1.0, 0.999375, 0.96375, 0.980625, 0.944375, 0.86, 0.975625, 0.99, 0.98875, 0.9515625, 0.8875, 0.88609375, 0.9421875, 0.99921875, 0.985625, 1.0, 0.88546875, 0.986875, 0.95015625, 0.91953125, 0.93578125, 1.0, 0.97265625, 0.96390625, 0.94703125, 0.98484375, 0.899375, 0.92875, 0.8325, 1.0, 0.9028125, 0.96875, 0.87515625, 0.9178125, 1.0, 0.9059375, 0.955, 1.0, 0.8403125, 0.9846875, 0.9959375, 0.91671875, 1.0, 0.7925, 0.74984375, 0.96125, 0.8753125, 0.95984375, 0.9821875, 1.0, 0.99640625, 0.985625, 1.0, 0.95921875, 0.953125, 0.93421875, 0.97, 0.9528125, 0.88515625, 0.99515625, 0.990625, 0.96546875, 1.0, 0.9871875, 1.0, 0.96390625, 0.9715625, 0.999375, 0.96515625, 0.9571875, 0.99953125, 0.981875, 0.9571875, 0.93734375, 0.995, 0.84875, 0.86328125, 0.884375, 1.0, 1.0, 0.92765625, 1.0, 0.94484375, 0.98015625, 0.975625, 0.92703125, 0.98859375, 1.0, 0.97875, 1.0, 0.979375, 0.891875, 0.9828125, 0.9759375, 0.93421875, 1.0, 0.9590625, 0.93734375, 0.91171875, 0.9684375, 0.968125, 1.0, 0.91625, 1.0, 1.0, 1.0, 0.905625, 0.98921875, 1.0, 0.96875, 0.82984375, 0.92734375, 1.0, 0.96421875, 0.94640625, 1.0, 1.0, 0.998125, 0.99125, 0.943125, 0.98515625, 1.0, 0.99890625, 0.9578125, 0.94375, 0.99078125, 0.87703125, 0.92109375, 0.90296875, 1.0, 0.90140625, 1.0, 0.99484375, 1.0, 0.98671875, 0.88765625, 0.97625, 0.98078125, 1.0, 0.91953125, 0.92015625, 0.95640625, 1.0, 0.9984375, 0.92640625, 0.98578125, 0.95109375, 1.0, 0.95578125, 0.8896875, 1.0, 1.0, 0.98015625, 0.87046875, 0.91578125, 0.9996875, 0.99875, 1.0, 0.9378125, 0.94625, 1.0, 1.0, 0.96578125, 1.0, 0.88265625, 0.9228125, 0.98765625, 1.0, 0.9753125, 0.82140625, 0.99421875, 0.9571875, 0.889375, 0.86703125, 0.92890625, 1.0, 1.0, 0.88734375, 0.88921875, 1.0, 1.0, 0.9065625, 0.9846875, 0.93796875, 0.970625, 1.0, 1.0, 0.98921875, 1.0, 0.96546875, 1.0, 0.83625, 0.865, 0.9065625, 1.0, 1.0, 1.0, 1.0, 0.98015625, 0.9853125, 1.0, 0.91609375, 0.8246875, 0.90640625, 0.7753125, 0.9209375, 0.90046875, 0.98578125, 0.8959375, 1.0, 0.95546875, 0.97765625, 0.9209375, 1.0, 0.90359375, 0.94578125, 0.98890625, 0.98375, 0.995625, 1.0, 0.81109375, 0.9371875, 0.93265625, 0.93625, 1.0, 0.96828125, 0.93375, 0.93015625, 1.0, 0.86984375, 0.81359375, 0.8828125, 0.84015625, 0.9590625, 0.79453125, 0.9478125, 1.0, 0.885, 0.90484375, 0.89296875, 0.981875, 0.98859375, 0.9740625, 0.92953125, 1.0, 1.0, 0.9903125, 0.990625, 0.85625, 0.89546875, 0.92796875, 1.0, 0.95, 0.9828125, 0.99140625, 0.98265625, 0.99421875, 0.9396875, 1.0, 0.99125, 1.0, 0.8803125, 0.93546875, 0.84453125, 0.9365625, 0.90578125, 1.0, 0.91671875, 0.911875, 0.96171875, 0.955625, 0.9959375, 0.8484375, 0.92140625, 0.9975, 0.959375, 0.6978125, 1.0, 0.9615625, 0.8175, 0.93390625, 0.9203125, 0.955625, 0.90171875, 0.9340625, 0.86859375, 0.979375, 0.99921875, 1.0, 0.85265625, 0.971875, 0.9584375, 0.893125, 0.97046875, 0.97140625, 0.90328125, 0.9553125, 0.9578125, 0.96734375, 1.0, 0.89578125, 0.77859375, 0.99046875, 0.961875, 0.9946875, 1.0, 0.9790625, 0.85890625, 1.0, 0.9728125, 0.835, 0.96875, 0.9028125, 1.0, 0.96328125, 0.96296875, 0.97515625, 0.93046875, 0.9215625, 0.96765625, 0.826875, 0.9515625, 0.98375, 0.91078125, 1.0, 0.92875, 0.9275, 0.86234375, 1.0, 1.0, 0.884375, 0.95140625, 0.96390625, 0.9546875, 1.0, 0.92203125, 1.0, 0.915625, 0.973125, 1.0, 1.0, 0.9834375, 0.94734375, 0.9484375, 0.91703125, 0.9896875, 1.0, 1.0, 0.97375, 0.92265625, 0.9909375, 1.0, 0.7275, 0.908125, 0.9503125, 0.98296875, 0.89625, 1.0, 0.98140625, 0.93421875, 1.0, 0.8796875, 1.0, 0.95796875, 0.931875, 1.0, 0.95671875, 1.0, 1.0, 1.0, 1.0, 0.99328125, 1.0, 0.904375, 1.0, 0.88765625, 0.88109375, 0.9590625, 0.87609375, 0.96546875, 0.98734375, 0.81890625, 0.97765625, 1.0, 1.0, 0.86390625, 1.0, 0.9615625, 0.965625, 0.9759375, 0.945, 1.0, 0.9553125, 0.9315625, 0.999375, 1.0, 0.90390625, 0.99796875, 1.0, 0.94484375, 0.95984375, 0.98171875, 0.97859375, 0.98671875, 0.92875, 1.0, 1.0, 0.9, 1.0, 0.92546875, 0.99421875, 0.95515625, 0.94265625, 0.97125, 1.0, 0.91546875, 0.98890625, 0.9696875, 1.0, 0.97296875, 0.9159375, 0.990625, 0.86921875, 1.0, 1.0, 1.0, 0.9878125, 0.95421875, 1.0, 0.98265625, 0.93984375, 0.92890625, 0.9621875, 0.90765625, 0.90765625, 1.0, 0.88734375, 0.969375, 0.99015625, 0.92296875, 0.85609375, 0.8421875, 0.83765625, 1.0, 0.86734375, 0.87890625, 0.9409375, 0.831875, 1.0, 0.88, 0.98328125, 1.0, 0.98078125, 1.0, 0.97, 0.9878125, 0.89515625, 0.93703125, 0.8575, 1.0, 1.0, 0.99234375, 1.0, 0.98390625, 0.9209375, 1.0, 0.9778125, 1.0, 0.86078125, 0.9678125, 0.99875, 0.87109375, 0.9753125, 0.9653125, 0.9271875, 0.8784375, 0.98765625, 0.98296875, 0.96421875, 0.91140625, 1.0, 1.0, 0.99328125, 1.0, 0.99875, 0.92421875, 0.96265625, 1.0, 0.934375, 0.96359375, 0.8871875, 0.9225, 0.9903125, 1.0, 0.9771875, 1.0, 0.9871875, 1.0, 0.86796875, 0.94234375, 1.0, 0.89234375, 0.96796875, 0.93015625, 0.8240625, 0.99765625, 0.90765625, 0.9434375, 0.97421875, 0.95484375, 0.96, 0.994375, 1.0, 1.0, 0.9165625, 0.999375, 1.0, 1.0, 0.94921875, 0.99125, 0.9228125, 1.0, 1.0, 0.95984375, 0.9984375, 0.949375, 0.88828125, 0.9228125, 0.91453125, 0.95765625, 0.94328125, 0.8515625, 0.97671875, 0.99921875, 0.9515625, 0.93703125, 0.9909375, 0.995, 0.83046875, 0.99140625, 0.999375, 0.7475, 1.0, 0.88234375, 0.926875, 0.85765625, 1.0, 0.97375, 0.883125, 0.8884375, 1.0, 0.9646875, 0.9634375, 0.85078125, 0.985, 0.96734375, 0.9440625, 0.964375, 0.96390625, 0.96328125, 0.90234375, 0.97296875, 0.913125, 0.9946875, 0.93375, 0.99734375, 0.9759375, 0.88265625, 1.0, 0.964375, 1.0, 0.92359375, 1.0, 1.0, 1.0, 0.87828125, 0.998125, 0.91, 0.9103125, 0.979375, 1.0, 0.95125, 1.0, 0.9615625, 1.0, 0.955, 0.9478125, 0.9796875, 0.86375, 0.94953125, 1.0, 1.0, 0.99890625, 1.0, 0.9146875, 0.9428125, 1.0, 1.0, 0.98328125, 0.91453125, 0.89671875, 0.8971875, 1.0, 1.0, 1.0, 0.889375, 0.96828125, 1.0, 1.0, 0.9946875, 0.9990625, 0.965625, 0.93484375, 1.0, 0.89515625, 0.8925, 1.0, 0.98015625, 1.0, 0.92359375, 0.9728125, 0.9340625, 0.86921875, 0.99578125, 1.0, 1.0, 0.93421875, 0.9690625, 1.0, 0.98796875, 0.99109375, 0.960625, 1.0, 1.0, 0.988125, 0.95640625, 0.99078125, 0.921875, 1.0, 0.90140625, 0.9471875, 1.0, 0.94703125, 0.98046875, 0.9815625, 1.0, 0.72140625, 0.94828125, 0.8528125, 0.9465625, 1.0, 0.903125, 1.0, 0.95171875, 1.0, 0.90203125, 0.93859375, 0.91375, 0.99171875, 0.97546875, 0.94328125, 0.95265625, 0.90578125, 0.96953125, 0.86921875, 0.96859375, 1.0, 0.99171875, 0.9834375, 0.95703125, 0.916875, 1.0, 0.9409375, 0.96890625, 0.94578125, 0.9603125, 0.8559375, 0.84609375, 1.0, 1.0, 0.90828125, 0.94703125, 0.98453125, 0.98765625, 0.9803125, 1.0, 0.90875, 0.925, 0.9890625, 0.97140625, 0.86, 0.8646875, 0.80828125, 1.0, 0.95953125, 0.91953125, 0.936875, 0.97484375, 0.9996875, 0.9896875, 0.983125, 0.93171875, 0.9221875, 0.93390625, 0.99515625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.955625, 0.94359375, 0.8278125, 1.0, 1.0, 1.0, 0.96015625, 0.91015625, 1.0, 0.9996875, 0.84453125, 0.910625, 0.8928125, 0.98953125, 0.8871875, 0.9753125, 0.91734375, 0.9721875, 1.0, 0.99953125, 0.97421875, 1.0, 0.9921875, 1.0, 0.979375, 0.806875, 0.85234375, 0.9434375, 1.0, 0.83171875, 0.86734375, 1.0, 1.0, 0.92234375, 1.0, 1.0, 1.0, 0.8609375, 0.9590625, 0.91578125, 0.959375, 1.0, 0.915, 0.96390625, 1.0, 0.9975, 0.903125, 0.9728125, 1.0, 0.97671875, 0.985, 1.0, 0.995625, 1.0, 0.9578125, 0.9640625, 0.91765625, 0.8834375, 1.0, 0.983125, 0.8621875, 1.0, 0.87421875, 1.0, 0.8209375, 0.94078125, 0.944375, 0.90359375, 0.9909375, 0.9996875, 0.9775, 1.0, 1.0, 0.92, 0.90578125, 0.77796875, 0.99390625, 0.99046875, 0.968125, 0.92796875, 0.96640625, 0.91390625, 0.956875, 0.94109375, 0.98140625, 0.95703125, 1.0, 0.9659375, 0.975625, 0.86578125, 0.92109375, 0.9821875, 0.99859375, 0.97328125, 1.0, 0.8659375, 0.88921875, 1.0, 0.98703125, 0.89796875, 0.9965625, 0.93875, 0.9421875, 0.90109375, 0.953125, 0.99265625, 0.9709375, 0.92, 0.9090625, 1.0, 0.8865625, 0.93265625, 0.92359375, 0.9353125, 0.978125, 0.965625, 0.98890625, 0.9825, 0.92640625, 0.99796875, 0.89796875, 0.99171875, 0.881875, 0.94265625, 1.0, 1.0, 0.93296875, 0.96375, 0.981875, 0.99671875, 0.9215625, 0.93609375, 0.99671875, 0.86109375, 0.9809375, 1.0, 0.949375, 0.99765625, 0.91609375, 0.97625, 0.97734375, 0.9825, 0.91046875, 1.0, 0.87546875, 1.0, 0.97578125, 0.8921875, 0.965, 0.951875, 0.95203125, 1.0, 0.99484375, 0.95609375, 1.0, 0.93578125, 0.94421875, 1.0, 1.0, 0.92125, 0.96703125, 0.98046875, 1.0, 0.931875, 1.0, 1.0, 1.0, 0.96859375, 0.99921875, 0.885625, 0.93828125, 0.993125, 0.97796875, 0.9921875, 1.0, 0.88828125, 0.90359375, 1.0, 1.0, 0.831875, 1.0, 0.95296875, 0.99328125, 0.8990625, 0.9790625, 0.9371875, 0.95875, 0.92140625, 1.0, 0.93203125, 0.969375, 0.96234375, 1.0, 1.0, 0.974375, 0.96328125, 0.9821875, 0.998125, 0.85390625, 0.97671875, 0.9665625, 1.0, 0.989375, 0.99390625, 0.99578125, 0.9553125, 0.97875, 0.93609375, 0.97953125, 0.98296875, 0.8853125, 0.85078125, 0.93171875, 0.98265625, 0.9275, 0.9509375, 1.0, 0.95546875, 0.79640625, 0.959375, 0.9515625, 0.9884375, 0.97078125, 0.88609375, 0.92171875, 0.9790625, 0.99171875, 1.0, 0.94171875, 0.97109375, 1.0, 0.86234375, 1.0, 0.87015625, 1.0, 0.99796875, 0.89875, 0.97671875, 0.965, 0.91421875, 1.0, 0.9940625, 1.0, 0.99484375, 1.0, 0.9865625, 0.99359375, 0.94828125, 0.9871875, 0.94203125, 0.98046875, 0.94203125, 0.89515625, 0.97109375, 1.0, 0.86, 0.91875, 1.0, 0.97515625, 0.9140625, 1.0, 1.0, 0.97390625, 0.95796875, 0.9534375, 1.0, 0.96265625, 1.0, 0.96734375, 0.91203125, 1.0, 0.9159375, 0.89046875, 0.99, 0.9890625, 0.99265625, 1.0, 0.98140625, 0.96421875, 1.0, 0.98140625, 0.87625, 0.8928125, 0.91984375, 0.93171875, 1.0, 0.87859375, 0.9734375, 1.0, 0.9603125, 0.96015625, 0.90890625, 0.955625, 0.93703125, 0.9796875, 1.0, 0.98078125, 0.97546875, 0.95609375, 1.0, 0.8465625, 0.9565625, 0.98296875, 0.990625, 1.0, 0.9625, 0.99890625, 1.0, 0.95890625, 1.0, 0.869375, 0.97953125, 0.92453125, 1.0, 0.97109375, 0.9675, 1.0, 0.99671875, 0.8721875, 0.86296875, 0.9453125, 0.990625, 1.0, 0.9059375, 0.99796875, 0.9971875, 0.9528125, 0.97609375, 0.98953125, 0.98015625, 0.97625, 1.0, 1.0, 0.89765625, 0.96421875, 0.8484375, 0.9625, 0.87140625, 0.99671875, 1.0, 0.99234375, 1.0, 1.0, 0.9928125, 1.0, 0.99890625, 1.0, 1.0, 0.9434375, 0.99953125, 0.97765625, 0.99421875, 0.8965625, 1.0, 0.9978125, 1.0, 0.8775, 0.9421875, 0.9603125, 0.985, 0.91171875, 0.98609375, 0.9971875, 0.9840625, 0.911875, 0.86265625, 0.9303125, 1.0, 0.8525, 0.911875, 0.96609375, 0.933125, 0.99875, 1.0, 0.9725, 0.91328125, 0.96828125, 0.9796875, 0.9140625, 1.0, 0.90734375, 0.93359375, 1.0, 0.9953125, 0.97890625, 1.0, 0.96625, 0.86375, 1.0, 0.87921875, 0.97734375, 0.91015625, 0.97609375, 1.0, 0.98890625, 0.87515625, 0.81921875, 1.0, 0.97703125, 0.86171875, 0.994375, 1.0, 0.92484375, 0.9296875, 0.98203125, 0.8725, 1.0, 0.9084375, 0.84671875, 0.97296875, 0.9565625, 1.0, 0.9515625, 0.99984375, 0.91421875, 0.96609375, 1.0, 0.97140625, 0.99171875, 0.8175, 0.7765625, 1.0, 1.0, 0.94234375, 0.93625, 1.0, 0.97984375, 0.96515625, 0.9440625, 1.0, 0.98390625, 0.94015625, 0.96375, 0.98375, 0.95578125, 0.9909375, 1.0, 0.9828125, 0.9765625, 0.92296875, 1.0, 1.0, 0.9603125, 1.0, 0.84015625, 0.98328125, 0.92703125, 0.9475, 0.96203125, 0.91078125, 0.89125, 0.96703125, 0.97, 0.9628125, 1.0, 0.98140625, 0.92375, 0.9846875, 0.938125, 0.9634375, 0.98625, 1.0, 0.9528125, 0.93953125, 0.896875, 0.9928125, 1.0, 0.9275, 0.941875, 0.96484375, 0.98171875, 0.94078125, 0.98328125, 0.81640625, 0.9925, 0.97828125, 0.9578125, 1.0, 0.96703125, 0.98234375, 0.84625, 1.0, 0.99265625, 0.75953125, 1.0, 1.0, 0.9046875, 0.90421875, 0.70515625, 1.0, 0.91625, 0.87578125, 0.8340625, 0.989375, 1.0, 0.9925, 0.9396875, 0.90078125, 0.9696875, 0.9925, 1.0, 1.0, 1.0, 1.0, 0.985625, 0.94921875, 0.9840625, 0.980625, 0.931875, 0.9234375, 0.86515625, 0.97328125, 1.0, 0.999375, 0.79421875, 1.0, 0.95859375, 1.0, 1.0, 1.0, 0.925, 0.9015625, 0.96734375, 1.0, 0.91046875, 0.8425, 0.9909375, 0.9953125, 0.99078125, 1.0, 1.0, 0.953125, 1.0, 0.8775, 0.93515625, 1.0, 0.9634375, 0.8803125, 0.94328125, 0.94390625, 1.0, 1.0, 1.0, 0.971875, 0.96203125, 1.0, 1.0, 0.85171875, 0.90671875, 0.825, 0.9809375, 0.973125, 0.98109375, 0.93375, 0.93265625, 0.95265625, 0.9953125, 0.98015625, 0.9046875, 0.95765625, 0.9821875, 0.88, 1.0, 0.9315625, 0.99515625, 0.87734375, 0.993125, 0.9684375, 0.98359375, 1.0, 0.96265625, 0.95125, 0.98921875, 0.97203125, 0.99765625, 0.985, 0.94734375, 0.90578125, 0.9234375, 0.895, 0.94796875, 1.0, 1.0, 0.99953125, 1.0, 0.91640625, 0.8915625, 0.894375, 1.0, 0.98625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99875, 0.9521875, 1.0, 0.90390625, 0.9775, 0.9571875, 1.0, 1.0, 1.0, 0.86390625, 0.940625, 0.95046875, 1.0, 0.99125, 1.0, 0.84828125, 0.9609375, 0.95328125, 0.978125, 0.92484375, 0.9759375, 0.87671875, 0.8253125, 0.98765625, 1.0, 0.95484375, 1.0, 0.9525, 0.99703125, 0.97359375, 0.996875, 1.0, 1.0, 0.91640625, 0.93765625, 1.0, 0.910625, 1.0, 0.9121875, 1.0, 0.88390625, 0.87890625, 0.98984375, 0.96140625, 1.0, 0.97046875, 1.0, 0.9053125, 0.9875, 0.95765625, 0.9740625, 1.0, 0.98046875, 1.0, 0.91609375, 0.9746875, 0.9259375, 0.9271875, 0.9403125, 1.0, 0.953125, 0.99125, 0.9290625, 0.971875, 0.93328125, 0.88203125, 0.9790625, 0.95046875, 0.919375, 1.0, 0.90234375, 1.0, 0.85796875, 0.97109375, 0.9275, 1.0, 0.9075, 0.9678125, 0.89921875, 0.88859375, 0.99328125, 0.9003125, 1.0, 0.861875, 1.0, 0.97734375, 0.924375, 1.0, 0.8003125, 0.97890625, 0.98140625, 1.0, 0.94234375, 0.96359375, 1.0, 0.92640625, 0.97140625, 0.98359375, 0.99421875, 0.99328125, 0.94078125, 0.97375, 1.0, 0.87578125, 0.92765625, 0.98828125, 0.994375, 1.0, 0.97203125, 1.0, 0.99453125, 0.9453125, 1.0, 0.91296875, 0.89640625, 0.99859375, 0.991875, 0.97328125, 0.96484375, 1.0, 1.0, 0.9403125, 0.8821875, 1.0, 0.868125, 0.95328125, 0.98484375, 0.9878125, 0.97140625, 0.99734375, 0.745625, 1.0, 0.99734375, 0.87203125, 0.989375, 0.96046875, 0.968125, 0.99734375, 1.0, 0.83828125, 0.90890625, 0.98109375, 0.97203125, 0.9696875, 1.0, 0.99671875, 0.93875, 0.97625, 0.9125, 0.91046875, 0.9690625, 1.0, 0.96015625, 0.93265625, 1.0, 1.0, 1.0, 0.996875, 0.931875, 1.0, 0.96359375, 0.92421875, 1.0, 1.0, 0.9828125, 0.9565625, 0.93328125, 0.97109375, 0.92109375, 0.9784375, 1.0, 1.0, 0.900625, 0.991875, 0.94515625, 0.89265625, 0.9578125, 0.94671875, 0.95203125, 0.9328125, 1.0, 1.0, 1.0, 0.8665625, 0.97453125, 0.97046875, 1.0, 0.9590625, 0.97640625, 0.99875, 0.98359375, 1.0, 0.99171875, 1.0, 1.0, 1.0, 0.96109375, 0.9325, 0.91640625, 0.881875, 0.994375, 0.9975, 1.0, 0.939375, 0.88546875, 0.96171875, 0.95609375, 0.89703125, 0.91390625, 1.0, 0.99046875, 0.96140625, 0.97875, 1.0, 1.0, 0.95984375, 1.0, 1.0, 0.8378125, 1.0, 0.94640625, 0.92859375, 1.0, 0.858125, 0.89484375, 0.9165625, 1.0, 0.96453125, 1.0, 0.99125, 0.98796875, 0.9453125, 0.9615625, 0.9553125, 0.9615625, 1.0, 0.9875, 0.989375, 0.973125, 1.0, 1.0, 0.92453125, 0.9571875, 1.0, 0.925, 0.9934375, 1.0, 0.98375, 1.0, 0.93109375, 1.0, 0.88125, 1.0, 1.0, 1.0, 0.9484375, 0.96328125, 0.9784375, 1.0, 0.97171875, 0.97921875, 0.998125, 0.9325, 0.9503125, 0.89921875, 0.93671875, 0.924375, 0.7475, 0.89625, 1.0, 0.99640625, 1.0, 0.995625, 0.96671875, 0.8946875, 1.0, 0.999375, 1.0, 1.0, 0.951875, 0.9884375, 0.99671875, 0.931875, 1.0, 0.95875, 1.0, 0.985625, 0.97953125, 0.9746875, 0.9303125, 0.92078125, 0.97328125, 0.97828125, 0.9325, 0.95875, 0.90515625, 0.96, 0.955625, 0.901875, 0.96328125, 0.81765625, 0.9053125, 0.91859375, 0.96359375, 0.94796875, 0.98875, 0.91140625, 0.9715625, 0.99875, 1.0, 0.9975, 0.98078125, 1.0, 0.93265625, 0.89015625, 0.96328125, 0.89390625, 0.9475, 0.9578125, 0.88734375, 0.9896875, 0.9471875, 0.838125, 0.96390625, 0.90453125, 1.0, 0.9709375, 0.96453125, 0.95515625, 1.0, 1.0, 0.89046875, 0.94296875, 0.87421875, 0.91265625, 0.93390625, 1.0, 0.9953125, 0.910625, 0.94171875, 0.9696875, 0.966875, 0.97046875, 0.920625, 0.94796875, 0.89546875, 0.90171875, 0.95765625, 1.0, 0.9759375, 0.971875, 0.9375, 1.0, 0.94328125, 1.0, 0.8109375, 1.0, 1.0, 1.0, 0.9284375, 1.0, 1.0, 0.9709375, 0.95671875, 0.841875, 0.8165625, 0.96984375, 1.0, 0.97796875, 0.9675, 1.0, 0.9746875, 0.9546875, 0.97203125, 1.0, 0.976875, 0.97078125, 0.88203125, 1.0, 0.8171875, 0.96328125, 0.95796875, 0.98984375, 1.0, 0.98109375, 0.93203125, 0.89546875, 0.97109375, 0.98140625, 0.8890625, 1.0, 1.0, 0.9128125, 0.971875, 0.98, 0.9940625, 1.0, 0.9809375, 0.9053125, 0.88453125, 0.97890625, 0.98, 1.0, 0.968125, 0.89390625, 1.0, 1.0, 0.9303125, 0.920625, 0.91125, 0.95921875, 0.98515625, 0.9725, 0.9853125, 0.941875, 0.99921875, 1.0, 0.95375, 0.99140625, 0.97125, 0.9265625, 1.0, 1.0, 0.92609375, 0.99046875, 1.0, 1.0, 0.91125, 0.903125, 1.0, 1.0, 0.99890625, 1.0, 0.99046875, 0.99640625, 0.96375, 0.9309375, 0.88765625, 0.94109375, 0.964375, 1.0, 0.89140625, 0.92609375, 0.93890625, 1.0, 0.97828125, 1.0, 0.9278125, 0.95796875, 0.980625, 0.8409375, 0.891875, 0.96546875, 0.84921875, 0.9540625, 1.0, 0.97609375, 0.8815625, 0.9165625, 1.0, 1.0, 0.96578125, 1.0, 1.0, 0.9359375, 1.0, 1.0, 0.9353125, 0.9259375, 0.9575, 0.8584375, 0.99921875, 0.8646875, 0.9921875, 0.9821875, 0.99734375, 1.0, 1.0, 0.9865625, 0.9896875, 0.9509375, 0.89046875, 1.0, 0.94625, 0.9309375, 0.98421875, 0.9165625, 0.8696875, 0.9078125, 0.93671875, 0.8946875, 0.8871875, 0.95640625, 0.94109375, 0.77578125, 0.8975, 0.9909375, 0.98796875, 1.0, 0.95984375, 0.80046875, 0.97890625, 0.9965625, 0.99, 1.0, 0.95765625, 0.9721875, 0.91703125, 0.92140625, 0.97625, 0.9653125, 0.965, 0.9990625, 0.8246875, 0.9696875, 1.0, 0.985625, 1.0, 0.944375, 0.99640625, 0.90546875, 0.96015625, 0.948125, 0.94859375, 0.9328125, 0.9775, 0.94828125, 0.97015625, 0.9765625, 0.88671875, 0.90515625, 0.983125, 0.9871875, 1.0, 0.93828125, 0.92828125, 1.0, 1.0, 0.94296875, 0.876875, 1.0, 0.9265625, 0.84734375, 1.0, 1.0, 0.92203125, 0.9859375, 0.985625, 1.0, 0.94828125, 0.9503125, 0.961875, 1.0, 0.9759375, 0.95828125, 0.99703125, 0.99046875, 0.876875, 0.985625, 0.9140625, 0.9409375, 0.92640625, 1.0, 0.90390625, 0.93515625, 0.96609375, 0.9825, 0.92734375, 0.99484375, 0.98484375, 0.98984375, 0.9640625, 0.998125, 0.98796875, 0.99234375, 1.0, 0.82765625, 0.97421875, 0.9946875, 0.864375, 1.0, 0.9534375, 0.95140625, 0.850625, 0.97046875, 0.959375, 0.9659375, 0.960625, 0.97171875, 1.0, 0.901875, 1.0, 0.94859375, 0.91484375, 0.91171875, 0.99859375, 0.9159375, 0.98890625, 1.0, 0.9128125, 0.95359375, 0.94390625, 0.8946875, 1.0, 0.89234375, 0.83109375, 0.9625, 1.0, 0.9271875, 0.9728125, 0.8378125, 0.9821875, 0.96125, 0.81328125, 0.995, 0.915, 1.0, 0.9096875, 0.94796875, 1.0, 0.949375, 0.9003125, 1.0, 0.8134375, 0.89078125, 0.8003125, 0.978125, 1.0, 0.97953125, 0.9084375, 1.0, 0.9578125, 0.93421875, 0.97421875, 0.8853125, 0.944375, 1.0, 0.99375, 0.97625, 0.9653125, 0.939375, 0.98984375, 0.97234375, 0.83328125, 0.90921875, 0.9659375, 1.0, 1.0, 0.98609375, 1.0, 0.888125, 0.995625, 0.95859375, 0.9471875, 0.95328125, 1.0, 1.0, 0.98296875, 0.99609375, 1.0, 0.859375, 0.96328125, 0.9284375, 1.0, 0.9425, 0.89328125, 0.92359375, 1.0, 0.9625, 1.0, 0.92828125, 1.0, 0.855, 0.9303125, 0.96703125, 0.9728125, 0.968125, 0.80046875, 0.975625, 0.83984375, 0.98203125, 0.991875, 0.9946875, 0.94875, 0.96171875, 1.0, 0.9928125, 0.89671875, 0.92109375, 0.9415625, 1.0, 0.955625, 0.9659375, 0.8571875, 1.0, 0.95296875, 0.976875, 1.0, 1.0, 0.8659375, 1.0, 0.95171875, 1.0, 0.91203125, 0.98625, 1.0, 0.90484375, 0.95375, 0.959375, 0.9821875, 0.92, 0.90671875, 0.94625, 0.98796875, 0.9903125, 1.0, 0.8703125, 0.9684375, 0.9796875, 0.92796875, 0.96671875, 1.0, 0.94984375, 0.9765625, 0.9596875, 0.92640625, 1.0, 0.9746875, 0.88640625, 0.91234375, 1.0, 0.92234375, 1.0, 1.0, 0.98734375, 0.95484375, 1.0, 1.0, 0.985625, 1.0, 0.945, 0.99109375, 0.8890625, 0.91453125, 0.86875, 0.99375, 1.0, 0.9525, 0.99359375, 1.0, 0.93421875, 1.0, 0.989375, 1.0, 0.8584375, 0.92890625, 0.9878125, 0.98390625, 1.0, 0.95359375, 0.83921875, 0.98796875, 0.8946875, 0.941875, 0.9859375, 1.0, 0.95171875, 0.87140625, 1.0, 0.98859375, 0.99328125, 1.0, 1.0, 0.9275, 1.0, 0.950625, 0.98140625, 0.965, 1.0, 0.94, 0.96375, 0.88578125, 0.97375, 0.87078125, 0.97375, 1.0, 0.939375, 1.0, 0.9809375, 1.0, 0.9734375, 0.9959375, 1.0, 0.93125, 0.98015625, 0.985625, 0.86265625, 1.0, 0.8546875, 0.97234375, 0.9615625, 0.9246875, 0.8625, 1.0, 1.0, 1.0, 0.98109375, 0.93796875, 0.94453125, 1.0, 0.94890625, 1.0, 0.9703125, 0.94140625, 0.9690625, 1.0, 0.81328125, 0.956875, 0.96109375, 0.95625, 0.966875, 0.96015625, 1.0, 1.0, 0.93890625, 1.0, 0.99171875, 0.8665625, 0.96296875, 0.964375, 1.0, 0.9578125, 1.0, 0.973125, 1.0, 0.973125, 0.954375, 1.0, 0.9625, 0.944375, 0.91421875, 0.92796875, 0.929375, 1.0, 0.80625, 0.99609375, 0.94359375, 1.0, 0.96765625, 1.0, 1.0, 0.9971875, 0.98453125, 0.945, 0.93078125, 0.935, 0.99671875, 1.0, 0.98796875, 0.9096875, 0.8975, 0.97, 0.96125, 1.0, 0.96234375, 0.984375, 1.0, 0.96734375, 1.0, 0.97953125, 0.93203125, 0.99765625, 0.97203125, 0.9446875, 0.9309375, 0.92234375, 0.8721875, 1.0, 1.0, 0.96484375, 1.0, 0.98953125, 1.0, 0.9896875, 0.96796875, 0.95109375, 0.9915625, 1.0, 0.950625, 0.9946875, 0.96421875, 0.845, 0.9303125, 0.96125, 0.9421875, 0.88765625, 1.0, 0.9196875, 0.87359375, 1.0, 1.0, 0.99765625, 1.0, 0.80328125, 0.96984375, 1.0, 0.931875, 0.96921875, 1.0, 0.98328125, 1.0, 0.9625, 0.96953125, 0.98515625, 0.99796875, 0.8734375, 0.8659375, 0.93546875, 0.9490625, 0.991875, 0.95890625, 0.96984375, 0.9871875, 0.9865625, 0.8071875, 1.0, 0.97703125, 0.918125, 0.966875, 0.9784375, 0.954375, 0.94046875, 0.89953125, 0.99328125, 0.99734375, 1.0, 0.9353125, 1.0, 1.0, 0.91078125, 0.94265625, 0.88203125, 0.94109375, 1.0, 1.0, 1.0, 0.9840625, 0.85890625, 1.0, 0.85421875, 1.0, 0.8975, 0.98796875, 1.0, 0.92578125, 0.9575, 1.0, 1.0, 0.97140625, 1.0, 0.9253125, 0.97375, 0.979375, 1.0, 0.94359375, 0.92828125, 1.0, 1.0, 0.98828125, 0.9925, 0.96359375, 1.0, 1.0, 1.0, 0.9475, 1.0, 0.963125, 1.0, 0.92671875, 1.0, 0.90296875, 0.9215625, 0.91421875, 0.99890625, 0.87984375, 1.0, 1.0, 0.97625, 1.0, 0.98890625, 0.94765625, 0.99453125, 1.0, 1.0, 0.976875, 0.865, 0.9334375, 1.0, 0.9584375, 0.8940625, 0.99765625, 1.0, 0.936875, 1.0, 0.968125, 0.9059375, 1.0, 0.941875, 0.966875, 1.0, 0.98328125, 0.9753125, 1.0, 1.0, 1.0, 1.0, 0.99375, 0.973125, 1.0, 1.0, 1.0, 0.8728125, 0.9715625, 0.96703125, 0.91, 1.0, 1.0, 1.0, 0.75234375, 1.0, 1.0, 0.90296875, 0.94796875, 1.0, 0.98625, 1.0, 0.935, 0.89, 0.97921875, 0.990625, 0.8675, 1.0, 0.83171875, 0.95828125, 0.8178125, 0.871875, 0.9471875, 0.96953125, 0.92453125, 0.94609375, 0.78203125, 0.98390625, 0.97296875, 1.0, 1.0, 1.0, 0.76296875, 0.90484375, 0.91515625, 0.97, 0.99421875, 0.99046875, 0.9409375, 1.0, 0.98953125, 0.94734375, 0.9653125, 1.0, 1.0, 0.98140625, 1.0, 0.98296875, 1.0, 0.98078125, 0.9534375, 0.84703125, 0.96796875, 0.94390625, 0.98875, 0.9796875, 0.96640625, 0.8728125, 0.993125, 0.98828125, 0.94421875, 0.99234375, 0.8646875, 0.98953125, 0.95484375, 0.9371875, 0.843125, 0.98890625, 0.99421875, 1.0, 0.870625, 0.8690625, 0.99203125, 0.98796875, 0.9340625, 1.0, 0.92890625, 0.90390625, 1.0, 0.9959375, 1.0, 1.0, 0.9125, 1.0, 0.91578125, 0.8309375, 1.0, 0.970625, 0.93171875, 0.93359375, 0.8746875, 0.9584375, 1.0, 0.96609375, 0.9034375, 0.89453125, 0.9765625, 0.9646875, 0.9571875, 0.94515625, 1.0, 0.96640625, 0.99515625, 0.97296875, 1.0, 0.8546875, 0.9515625, 1.0, 1.0, 0.9484375, 0.97109375, 0.97234375, 0.99109375, 0.940625, 0.975625, 0.9571875, 1.0, 1.0, 0.99, 0.9225, 1.0, 0.89625, 0.93328125, 0.96640625, 0.975625, 0.999375, 1.0, 1.0, 0.99828125, 0.9684375, 1.0, 0.95671875, 1.0, 0.9625, 0.8825, 0.97921875, 0.910625, 0.9946875, 0.95421875, 0.93296875, 0.9825, 0.97609375, 0.99375, 0.95484375, 0.92890625, 0.97171875, 0.9628125, 0.945, 0.9475, 0.99515625, 0.933125, 0.90234375, 1.0, 0.805625, 1.0, 1.0, 0.90515625, 0.95265625, 0.8909375, 0.96734375, 0.8946875, 0.95796875, 0.98765625, 0.85515625, 1.0, 0.9284375, 0.75390625, 0.976875, 0.97984375, 0.9065625, 0.8021875, 1.0, 0.92265625, 1.0, 0.98359375, 0.98671875, 0.9134375, 0.95125, 0.93015625, 1.0, 0.95234375, 0.86515625, 0.95703125, 0.99921875, 0.98015625, 1.0, 1.0, 0.88046875, 0.99890625, 0.9796875, 0.93390625, 0.95859375, 1.0, 1.0, 1.0, 0.87921875, 0.9475, 1.0, 0.99265625, 0.95890625, 1.0, 0.9121875, 0.9984375, 0.98765625, 0.99015625, 0.9034375, 0.92078125, 1.0, 1.0, 0.98359375, 1.0, 0.9640625, 0.9896875, 0.94, 0.9934375, 0.85296875, 0.99625, 0.90140625, 0.89875, 1.0, 1.0, 1.0, 0.9990625, 0.9621875, 1.0, 0.9134375, 0.9571875, 0.98640625, 0.92625, 0.8090625, 0.9534375, 1.0, 0.90296875, 1.0, 0.7975, 0.965625, 0.92453125, 0.81671875, 1.0, 0.96109375, 0.88671875, 0.99671875, 0.91796875, 1.0, 0.97953125, 1.0, 1.0, 0.9259375, 0.96265625, 0.97984375, 0.92375, 0.93515625, 0.9, 0.99671875, 0.9153125, 0.87671875, 0.888125, 0.9775, 0.9828125, 0.988125, 0.97515625, 0.93765625, 1.0, 1.0, 0.95390625, 1.0, 0.958125, 0.98640625, 0.90265625, 0.83734375, 0.968125, 0.92828125, 0.936875, 0.96578125, 1.0, 0.9659375, 0.919375, 0.97984375, 0.9084375, 0.94, 1.0, 0.84546875, 0.84875, 0.90671875, 0.99734375, 0.97765625, 0.908125, 0.97046875, 0.99515625, 0.88703125, 0.98984375, 1.0, 0.93515625, 1.0, 0.94234375, 0.9975, 0.98875, 0.994375, 0.8990625, 0.84625, 0.9996875, 0.8996875, 0.869375, 0.920625, 1.0, 0.9784375, 0.9990625, 0.98328125, 0.9603125, 1.0, 0.9790625, 1.0, 0.97375, 0.9825, 0.9053125, 0.82484375, 1.0, 0.95859375, 0.97578125, 0.94515625, 0.9875, 0.9571875, 1.0, 0.99, 0.97125, 0.92796875, 0.903125, 0.96859375, 0.9909375, 0.97140625, 0.94234375, 0.98140625, 0.91796875, 0.9865625, 0.98703125, 1.0, 0.9275, 0.95859375, 1.0, 0.92234375, 1.0, 0.95203125, 0.9778125, 1.0, 0.974375, 0.898125, 1.0, 0.986875, 0.94671875, 0.9625, 0.949375, 0.98125, 0.958125, 1.0, 0.91609375, 0.9771875, 0.9734375, 0.92828125, 0.99953125, 1.0, 0.95484375, 1.0, 1.0, 0.8484375, 0.964375, 1.0, 1.0, 0.87875, 0.94421875, 0.8365625, 1.0, 0.9765625, 0.96890625, 0.98265625, 0.98828125, 1.0, 0.9925, 1.0, 1.0, 0.9296875, 0.9390625, 0.98625, 0.8425, 0.9559375, 1.0, 0.9896875, 0.9096875, 0.895625, 0.94109375, 0.990625, 1.0, 0.87390625, 0.93203125, 0.94203125, 0.96203125, 0.9496875, 1.0, 0.8559375, 1.0, 0.96359375, 0.98171875, 0.92734375, 1.0, 0.88765625, 0.96765625, 0.99, 0.90765625, 0.93203125, 0.95796875, 0.97953125, 0.980625, 0.91234375, 0.98484375, 0.96265625, 0.97453125, 0.9771875, 0.9971875, 0.78484375, 0.8971875, 1.0, 0.995625, 1.0, 0.97984375, 0.969375, 0.98828125, 0.9903125, 0.96875, 0.94171875, 0.94265625, 0.93953125, 0.97765625, 0.98421875, 1.0, 0.88421875, 0.989375, 0.98640625, 1.0, 0.9796875, 0.91046875, 0.88921875, 0.975625, 0.92875, 1.0, 0.97234375, 0.931875, 0.931875, 0.97578125, 1.0, 0.98703125, 1.0, 0.938125, 0.89765625, 1.0, 1.0, 1.0, 0.9021875, 0.97765625, 0.841875, 1.0, 0.98375, 1.0, 0.83078125, 0.93078125, 0.9121875, 0.95609375, 0.854375, 0.886875, 0.91203125, 0.92640625, 0.998125, 0.98765625, 0.8809375, 0.99125, 0.9828125, 0.886875, 0.9871875, 0.9978125, 1.0, 0.93703125, 0.92015625, 1.0, 0.9934375, 1.0, 0.89640625, 0.9440625, 0.96765625, 1.0, 1.0, 0.96453125, 0.838125, 0.86046875, 0.9390625, 0.96046875, 0.96984375, 1.0, 0.9296875, 0.9878125, 1.0, 0.95859375, 0.92015625, 0.90609375, 0.93421875, 0.915, 0.98765625, 0.94296875, 0.9478125, 0.99046875, 0.88515625, 0.869375, 1.0, 1.0, 0.90671875, 0.9959375, 0.9665625, 0.995, 0.9834375, 1.0, 0.93984375, 0.99515625, 0.87953125, 1.0, 0.93390625, 0.92921875, 0.9965625, 0.99734375, 0.85703125, 1.0, 1.0, 1.0, 0.976875, 0.82671875, 0.9453125, 0.97953125, 1.0, 0.9584375, 0.946875, 1.0, 1.0, 0.905, 1.0, 0.9903125, 0.97, 0.93625, 1.0, 1.0, 0.98796875, 0.9715625, 1.0, 0.86109375, 0.91421875, 1.0, 1.0, 0.93484375, 0.9728125, 0.98703125, 0.9725, 0.78265625, 0.90234375, 1.0, 1.0, 0.9734375, 0.9990625, 0.97984375, 0.8490625, 0.97765625, 0.96359375, 0.9509375, 0.87015625, 1.0, 0.77796875, 0.97984375, 0.99171875, 1.0, 1.0, 0.97046875, 0.97625, 0.989375, 0.92015625, 1.0, 0.9653125, 0.758125, 0.96203125, 0.93234375, 0.98828125, 1.0, 0.97984375, 1.0, 0.941875, 0.94640625, 1.0, 0.9771875, 1.0, 0.86078125, 0.95796875, 0.96671875, 0.9990625, 0.94515625, 0.95640625, 0.91125, 0.994375, 0.96546875, 0.9859375, 0.97, 0.99765625, 1.0, 0.97, 0.98765625, 0.9840625, 0.96796875, 1.0, 0.98421875, 0.8753125, 0.99734375, 0.97515625, 0.9175, 0.89515625, 1.0, 0.9765625, 0.98484375, 1.0, 0.98375, 0.8353125, 0.99484375, 1.0, 0.918125, 1.0, 0.956875, 1.0, 0.92, 1.0, 0.96171875, 0.96109375, 0.91703125, 0.9990625, 1.0, 0.946875, 0.9684375, 0.9875, 1.0, 0.95, 0.941875, 0.90515625, 0.94640625, 0.9859375, 0.961875, 0.93796875, 0.9596875, 0.9559375, 1.0, 1.0, 1.0, 0.93328125, 0.9409375, 0.940625, 1.0, 0.93953125, 0.75203125, 0.89796875, 0.8790625, 1.0, 0.8871875, 1.0, 0.8546875, 0.96859375, 0.9409375, 0.97046875, 1.0, 1.0, 0.94078125, 0.9353125, 0.9575, 0.9146875, 0.9375, 0.92984375, 0.98421875, 0.99796875, 0.91296875, 1.0, 1.0, 0.99625, 1.0, 0.98921875, 0.94703125, 1.0, 0.94140625, 0.9803125, 1.0, 0.98015625, 0.9603125, 0.9771875, 0.9021875, 1.0, 1.0, 0.96640625, 0.968125, 0.95078125, 1.0, 0.95296875, 0.94671875, 0.97265625, 0.9909375, 0.9771875, 0.91734375, 0.87890625, 0.81015625, 0.95359375, 0.83203125, 1.0, 0.97234375, 0.9165625, 0.9271875, 1.0, 0.94640625, 0.99640625, 0.96109375, 0.9165625, 1.0, 0.92140625, 0.98015625, 0.98578125, 0.88921875, 0.94859375, 0.995625, 0.7559375, 1.0, 0.954375, 0.9428125, 0.9803125, 0.9796875, 0.93765625, 0.973125, 0.92015625, 1.0, 0.92125, 0.971875, 0.98890625, 1.0, 0.99953125, 0.9425, 0.94984375, 1.0, 0.97046875, 1.0, 0.92890625, 1.0, 0.9721875, 0.9496875, 0.898125, 1.0, 1.0, 0.96953125, 0.8684375, 1.0, 1.0, 0.95609375, 0.9321875, 1.0, 0.98015625, 0.9971875, 0.89421875, 0.940625, 0.99953125, 0.95234375, 0.9909375, 0.965625, 1.0, 0.9890625, 0.9828125, 0.9215625, 0.925, 1.0, 0.98171875, 1.0, 0.97609375, 0.98640625, 0.99609375, 0.93140625, 0.934375, 0.88203125, 0.9196875, 1.0, 0.9153125, 0.99484375, 1.0, 0.97703125, 1.0, 1.0, 0.97640625, 1.0, 0.9721875, 0.9278125, 0.8659375, 0.9940625, 0.936875, 0.93171875, 0.96109375, 0.955625, 0.739375, 0.92359375, 0.97359375, 0.9346875, 0.89375, 0.97859375, 0.92390625, 1.0, 0.939375, 1.0, 1.0, 1.0, 0.989375, 0.98109375, 0.9809375, 1.0, 0.95296875, 0.98828125, 0.9084375, 0.95875, 0.92796875, 0.9471875, 1.0, 0.95546875, 0.8965625, 1.0, 0.930625, 0.906875, 0.971875, 0.756875, 0.9725, 0.78640625, 0.81609375, 0.95390625, 1.0, 0.88890625, 0.95953125, 0.93234375, 0.96484375, 0.9015625, 0.9778125, 1.0, 0.9453125, 0.910625, 0.9146875, 0.99921875, 0.95578125, 0.93765625, 0.949375, 0.89015625, 0.87578125, 0.9725, 0.86921875, 0.9653125, 0.97375, 0.94859375, 1.0, 0.924375, 0.99234375, 0.7971875, 0.998125, 0.91109375, 1.0, 0.8975, 0.8871875, 0.94078125, 0.988125, 0.97484375, 0.9603125, 0.8984375, 0.88765625, 0.94984375, 0.8625, 0.91515625, 0.9078125, 0.976875, 0.9846875, 0.91921875, 0.955, 0.95859375, 0.9309375, 0.96046875, 0.92484375, 0.99859375, 0.991875, 0.96890625, 0.9296875, 0.98046875, 0.82046875, 1.0, 0.98046875, 0.9684375, 0.9128125, 0.9565625, 0.9278125, 0.95609375, 0.85109375, 1.0, 0.9578125, 0.9775, 0.99140625, 1.0, 1.0, 1.0, 0.9940625, 0.9815625, 0.94890625, 0.954375, 1.0, 0.96, 0.9503125, 0.9909375, 0.976875, 0.98109375, 1.0, 0.93484375, 1.0, 1.0, 0.98421875, 0.97953125, 0.99734375, 0.98765625, 1.0, 0.91046875, 0.98640625, 1.0, 1.0, 1.0, 1.0, 0.9915625, 0.95234375, 0.95234375, 0.91203125, 0.94109375, 0.91140625, 1.0, 0.90421875, 0.9115625, 0.94359375, 1.0, 1.0, 0.8753125, 0.8575, 0.99625, 1.0, 0.97734375, 0.82578125, 0.97046875, 0.92984375, 0.92, 1.0, 0.943125, 1.0, 0.9928125, 0.9240625, 0.905, 0.92984375, 0.9290625, 1.0, 1.0, 0.91359375, 0.9984375, 0.90828125, 0.93953125, 0.95421875, 0.873125, 0.84921875, 0.99625, 1.0, 0.91625, 0.88140625, 0.87546875, 0.989375, 0.99140625, 0.97703125, 0.94625, 0.98625, 1.0, 0.9875, 0.99609375, 0.94546875, 0.9934375, 1.0, 0.986875, 0.991875, 0.99203125, 0.9953125, 1.0, 1.0, 1.0, 1.0, 0.9859375, 0.958125, 0.988125, 0.93140625, 1.0, 0.98046875, 0.8165625, 0.92578125, 0.98234375, 0.9653125, 1.0, 1.0, 0.88921875, 1.0, 1.0, 0.8875, 0.96328125, 0.8940625, 0.95328125, 0.9984375, 0.99296875, 0.9796875, 0.95140625, 0.91796875, 0.9725, 0.9034375, 0.9428125, 0.89953125, 0.85765625, 0.975, 0.913125, 1.0, 0.98515625, 0.93640625, 0.9078125, 0.97734375, 0.99453125, 0.8890625, 0.94875, 0.96328125, 0.99140625, 0.99578125, 0.9946875, 0.97515625, 0.8875, 0.991875, 0.99140625, 1.0, 0.97515625, 1.0, 0.90875, 0.9434375, 1.0, 0.9653125, 1.0, 0.9071875, 0.9703125, 0.97859375, 1.0, 0.99375, 1.0, 0.9896875, 0.8803125, 0.97109375, 0.96609375, 0.96625, 0.93625, 0.99453125, 0.963125, 0.974375, 0.910625, 0.97546875, 0.99078125, 0.96609375, 0.9421875, 0.89953125, 0.9346875, 0.9046875, 1.0, 0.9171875, 0.88640625, 1.0, 0.9753125, 0.96, 0.99390625, 0.94984375, 0.9496875, 0.94265625, 0.9984375, 0.96421875, 0.96421875, 0.84859375, 0.9628125, 1.0, 1.0, 1.0, 0.91328125, 1.0, 0.8753125, 0.95515625, 0.94421875, 1.0, 0.8765625, 0.97953125, 1.0, 0.9278125, 0.99875, 0.884375, 0.9584375, 1.0, 1.0, 0.9971875, 1.0, 0.99875, 0.9178125, 1.0, 0.9965625, 0.96265625, 0.93703125, 1.0, 1.0, 0.990625, 0.98984375, 0.96921875, 1.0, 0.99578125, 0.9246875, 0.98875, 0.97265625, 0.9721875, 0.95375, 0.975625, 0.97140625, 0.9896875, 0.9140625, 0.9815625, 0.88125, 0.8709375, 0.97640625, 1.0, 0.988125, 0.9453125, 1.0, 0.92609375, 0.951875, 0.99234375, 1.0, 0.905, 1.0, 0.99515625, 0.94, 1.0, 0.94296875, 0.96015625, 0.8753125, 0.8340625, 0.94859375, 0.95390625, 0.971875, 0.92140625, 0.9028125, 0.984375, 0.9421875, 0.96609375, 0.91546875, 0.88484375, 1.0, 1.0, 0.99765625, 0.9621875, 0.908125, 0.8921875, 0.8953125, 0.988125, 0.955, 0.9646875, 0.9003125, 0.96828125, 0.93953125, 0.99, 0.9596875, 0.84953125, 1.0, 0.8859375, 0.9465625, 0.9634375, 0.94375, 1.0, 0.9965625, 0.9109375, 0.87296875, 0.98140625, 0.9059375, 1.0, 0.96546875, 1.0, 0.89859375, 1.0, 0.9653125, 0.98671875, 0.9834375, 0.97078125, 0.99625, 1.0, 1.0, 0.98640625, 0.95328125, 0.99921875, 0.8928125, 0.933125, 0.86703125, 0.86765625, 0.9971875, 0.96953125, 0.9765625, 0.90125, 1.0, 0.951875, 1.0, 0.9665625, 0.97171875, 0.93203125, 0.96984375, 0.90140625, 0.976875, 0.998125, 0.89078125, 1.0, 1.0, 1.0, 1.0, 0.97140625, 0.8753125, 0.9734375, 0.99375, 0.9578125, 0.9421875, 0.9115625, 0.9740625, 1.0, 0.99359375, 0.87765625, 1.0, 0.93703125, 0.87671875, 0.98640625, 0.9284375, 0.9746875, 0.98453125, 0.99515625, 1.0, 0.96953125, 1.0, 1.0, 0.94859375, 0.98890625, 0.9978125, 0.88, 0.98359375, 0.99125, 0.93796875, 1.0, 0.909375, 0.9790625, 1.0, 0.99234375, 0.93171875, 0.959375, 0.9821875, 0.92, 0.99046875, 0.9775, 0.9925, 0.95765625, 0.98125, 1.0, 0.93703125, 0.9421875, 0.99296875, 0.97765625, 1.0, 1.0, 0.9165625, 0.8921875, 0.86359375, 1.0, 0.91890625, 0.88765625, 0.86671875, 0.88875, 0.94109375, 0.981875, 0.97828125, 0.98890625, 0.92703125, 0.8384375, 0.99640625, 0.98796875, 0.979375, 0.8740625, 0.949375, 0.97546875, 0.9478125, 0.99296875, 0.92953125, 0.97875, 0.9309375, 0.9284375, 0.88421875, 0.89578125, 0.905, 1.0, 0.86109375, 0.92109375, 0.8790625, 0.99671875, 0.86859375, 0.96640625, 1.0, 0.999375, 1.0, 0.9978125, 1.0, 0.865625, 0.97984375, 0.9990625, 1.0, 0.8659375, 0.985, 0.933125, 0.93015625, 0.97296875, 0.9559375, 0.9434375, 0.883125, 0.97921875, 0.90640625, 1.0, 0.94015625, 0.9225, 0.93296875, 0.97859375, 0.9315625, 0.93109375, 0.89953125, 0.9871875, 0.7325, 0.96984375, 0.94359375, 0.9990625, 0.9878125, 1.0, 1.0, 1.0, 0.99859375, 0.996875, 0.98671875, 1.0, 0.8965625, 1.0, 0.9653125, 0.97671875, 1.0, 1.0, 1.0, 0.919375, 1.0, 1.0, 0.93203125, 0.85609375, 0.96234375, 0.97515625, 0.98375, 0.97375, 0.74421875, 0.9878125, 0.99875, 0.93515625, 1.0, 0.94578125, 0.98765625, 1.0, 1.0, 0.95, 1.0, 0.913125, 1.0, 0.96703125, 0.8953125, 1.0, 0.98421875, 0.97515625, 0.97890625, 0.9415625, 0.89828125, 0.83265625, 0.99625, 1.0, 0.838125, 0.83953125, 0.9425, 1.0, 0.9875, 0.96078125, 0.9521875, 0.97890625, 0.9984375, 0.97140625, 1.0, 0.9978125, 0.8896875, 0.8715625, 0.98328125, 0.89625, 1.0, 0.936875, 0.958125, 0.82, 0.99671875, 0.99765625, 1.0, 0.9465625, 0.9159375, 0.97265625, 0.98390625, 0.9828125, 1.0, 0.93671875, 0.99609375, 0.93359375, 0.9671875, 1.0, 1.0, 0.88859375, 0.92875, 0.93734375, 1.0, 0.88421875, 0.96140625, 1.0, 1.0, 1.0, 0.99828125, 1.0, 0.98109375, 0.980625, 0.97828125, 1.0, 1.0, 0.8953125, 0.94828125, 0.98171875, 0.96421875, 1.0, 0.98875, 0.88109375, 1.0, 0.95875, 0.96109375, 1.0, 0.84640625, 1.0, 1.0, 1.0, 0.9634375, 1.0, 0.869375, 0.9996875, 0.9828125, 0.90328125, 0.969375, 1.0, 0.91953125, 0.9934375, 0.9925, 0.99015625, 0.91875, 0.875625, 1.0, 0.92796875, 0.93140625, 0.99484375, 0.9390625, 0.91484375, 0.9634375, 0.9784375, 0.9903125, 0.9953125, 1.0, 0.97546875, 1.0, 1.0, 0.9634375, 0.95640625, 1.0, 0.93125, 0.88, 0.96109375, 0.845625, 0.93515625, 0.99515625, 0.9928125, 0.96984375, 0.93421875, 0.98921875, 1.0, 0.88, 0.99015625, 1.0, 0.87953125, 0.909375, 0.9165625, 1.0, 0.95671875, 0.90875, 0.99875, 0.941875, 0.98859375, 0.99984375, 1.0, 1.0, 0.924375, 0.89328125, 0.94984375, 1.0, 0.93640625, 0.95859375, 0.9603125, 0.88875, 0.896875, 0.97875, 0.9478125, 0.9609375, 0.99171875, 0.95515625, 0.96109375, 0.94671875, 0.9903125, 0.94390625, 1.0, 0.9765625, 0.97171875, 1.0, 0.94109375, 1.0, 0.8971875, 1.0, 0.87140625, 1.0, 0.99515625, 1.0, 1.0, 0.94125, 1.0, 0.96375, 0.9321875, 0.93828125, 0.98921875, 1.0, 1.0, 1.0, 0.9596875, 0.86171875, 1.0, 1.0, 0.956875, 0.9475, 0.95625, 0.96359375, 0.95484375, 1.0, 1.0, 0.96109375, 0.87078125, 0.96390625, 1.0, 0.93640625, 0.95046875, 0.94578125, 0.84296875, 1.0, 1.0, 1.0, 0.94140625, 0.99296875, 0.97375, 0.98390625, 0.92125, 0.861875, 1.0, 0.94265625, 0.95390625, 0.9359375, 0.95203125, 0.869375, 1.0, 1.0, 0.99953125, 0.89390625, 0.8596875, 0.96734375, 0.99796875, 0.9371875, 1.0, 0.934375, 0.92, 0.87765625, 0.96625, 0.98125, 1.0, 0.86921875, 1.0, 0.91671875, 0.95296875, 0.99140625, 0.91359375, 1.0, 0.76859375, 0.98625, 1.0, 0.97734375, 1.0, 0.93015625, 0.95125, 0.94953125, 1.0, 0.9328125, 0.96953125, 0.9303125, 0.99, 0.9521875, 1.0, 0.92328125, 0.91546875, 0.965625, 0.96921875, 0.9284375, 0.9265625, 0.95734375, 0.9365625, 0.8409375, 1.0, 0.8865625, 0.97234375, 1.0, 1.0, 0.97859375, 0.93203125, 0.9475, 0.990625, 0.96140625, 1.0, 1.0, 1.0, 1.0, 0.97, 0.9459375, 0.9990625, 0.94859375, 0.91796875, 0.968125, 0.97265625, 0.96296875, 0.86828125, 0.98234375, 0.92109375, 0.97203125, 1.0, 1.0, 0.9928125, 1.0, 1.0, 0.93796875, 0.9659375, 0.99875, 0.95203125, 1.0, 0.96390625, 0.9709375, 1.0, 0.97125, 0.9140625, 0.980625, 0.91359375, 0.9553125, 0.8875, 1.0, 1.0, 1.0, 0.99, 0.9309375, 0.9334375, 1.0, 0.96625, 0.93625, 0.99296875, 1.0, 1.0, 1.0, 1.0, 0.94078125, 0.97609375, 0.99984375, 0.92578125, 0.95125, 0.93640625, 0.95640625, 0.84125, 0.9428125, 1.0, 1.0, 0.91484375, 0.90984375, 0.8478125, 0.9646875, 0.88875, 0.88640625, 0.77359375, 0.9978125, 0.8734375, 0.9915625, 0.96640625, 1.0, 0.9978125, 0.96984375, 0.89828125, 0.97015625, 0.889375, 0.9703125, 0.8946875, 0.8128125, 0.94921875, 0.8090625, 0.9390625, 0.79421875, 0.9815625, 0.969375, 1.0, 1.0, 0.96640625, 1.0, 0.9715625, 0.88484375, 0.8665625, 0.963125, 0.94046875, 0.86640625, 1.0, 0.9446875, 1.0, 1.0, 1.0, 0.94046875, 0.84015625, 0.9559375, 0.7990625, 0.95734375, 0.95328125, 0.98109375, 0.983125, 0.9809375, 0.9321875, 0.97109375, 0.935625, 0.8959375, 0.95890625, 0.94484375, 0.9821875, 0.87546875, 0.87359375, 1.0, 0.9253125, 0.92578125, 0.93484375, 0.93859375, 0.96, 1.0, 0.93234375, 1.0, 0.98140625, 1.0, 0.9365625, 0.816875, 0.99546875, 0.9140625, 0.9921875, 0.91515625, 0.99203125, 0.96515625, 0.88328125, 1.0, 1.0, 0.993125, 0.82796875, 1.0, 1.0, 0.971875, 0.97875, 0.98578125, 0.84203125, 0.8634375, 0.9959375, 0.9815625, 0.899375, 0.90609375, 0.94359375, 0.96828125, 1.0, 1.0, 0.895, 0.921875, 0.97359375, 0.9896875, 0.99859375, 0.86125, 1.0, 0.9903125, 1.0, 0.978125, 1.0, 0.8784375, 1.0, 0.9234375, 1.0, 0.89765625, 0.8928125, 0.90359375, 0.96671875, 0.88765625, 0.9790625, 1.0, 0.98984375, 0.94984375, 0.98484375, 1.0, 0.94921875, 0.91734375, 1.0, 1.0, 1.0, 0.95140625, 0.94359375, 0.8871875, 0.989375, 0.83078125, 0.99890625, 0.9403125, 0.92703125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9740625, 0.95375, 0.9490625, 0.99796875, 1.0, 0.91140625, 1.0, 0.98765625, 0.99578125, 0.9759375, 1.0, 0.88125, 1.0, 1.0, 0.9215625, 0.9934375, 0.8740625, 1.0, 1.0, 1.0, 0.921875, 0.9396875, 0.90234375, 0.965, 0.86953125, 0.93875, 0.98640625, 0.929375, 1.0, 1.0, 0.84640625, 1.0, 1.0, 1.0, 0.99265625, 0.8353125, 0.94609375, 1.0, 1.0, 1.0, 0.98875, 0.9559375, 0.96953125, 0.96015625, 0.96703125, 0.88765625, 0.98390625, 0.94984375, 1.0, 1.0, 0.9665625, 1.0, 0.99484375, 0.9509375, 0.97625, 0.99140625, 1.0, 1.0, 0.9778125, 1.0, 0.97703125, 0.916875, 1.0, 0.93171875, 1.0, 0.9440625, 0.916875, 0.97578125, 0.9859375, 1.0, 0.9884375, 0.913125, 0.78265625, 0.97234375, 0.92921875, 0.8903125, 0.97578125, 0.88953125, 0.944375, 0.84390625, 0.9828125, 0.97375, 0.9396875, 0.9915625, 0.968125, 1.0, 1.0, 0.90421875, 0.826875, 0.93359375, 1.0, 0.9540625, 0.98703125, 0.96265625, 0.98109375, 0.86109375, 0.879375, 0.80234375, 0.99484375, 0.94359375, 0.9959375, 0.9896875, 0.95765625, 0.91265625, 0.96453125, 0.9796875, 1.0, 1.0, 0.91703125, 0.9890625, 0.96984375, 0.9846875, 1.0, 0.94546875, 1.0, 0.9871875, 0.95796875, 0.98453125, 0.99484375, 0.966875, 1.0, 0.82515625, 0.93578125, 0.9953125, 0.9359375, 0.99125, 1.0, 0.98703125, 0.87828125, 0.9234375, 0.95703125, 0.925625, 1.0, 0.97859375, 0.9325, 0.85, 1.0, 1.0, 0.9334375, 0.9803125, 0.97984375, 0.97234375, 0.951875, 0.98796875, 0.988125, 1.0, 0.9521875, 0.96515625, 0.9253125, 1.0, 1.0, 0.96984375, 0.90390625, 0.9325, 1.0, 0.9453125, 0.92890625, 0.96421875, 0.96703125, 1.0, 0.91421875, 0.9234375, 0.920625, 0.92609375, 1.0, 0.92515625, 0.93578125, 1.0, 0.924375, 0.91625, 1.0, 0.95875, 0.99515625, 1.0, 0.9959375, 0.9690625, 1.0, 0.97421875, 0.99546875, 1.0, 0.81203125, 0.90140625, 0.98921875, 0.940625, 0.98171875, 0.98484375, 0.86375, 0.95484375, 0.98859375, 0.953125, 0.935625, 0.991875, 1.0, 0.9271875, 1.0, 0.9715625, 0.91265625, 0.94046875, 1.0, 0.94328125, 0.89609375, 0.961875, 1.0, 1.0, 0.96046875, 0.99921875, 1.0, 0.94390625, 0.90453125, 0.98484375, 0.84953125, 0.94984375, 0.9896875, 0.91078125, 0.9425, 0.9996875, 0.9646875, 0.90390625, 1.0, 0.8809375, 0.96859375, 0.9903125, 0.98484375, 1.0, 0.92234375, 1.0, 0.96359375, 0.8878125, 0.9628125, 0.95796875, 0.97953125, 1.0, 1.0, 0.99796875, 0.85171875, 0.89296875, 0.97578125, 0.9315625, 1.0, 0.998125, 0.8846875, 0.8803125, 0.96375, 1.0, 0.97078125, 0.8621875, 0.9953125, 1.0, 0.99296875, 1.0, 1.0, 0.9290625, 1.0, 0.998125, 0.90140625, 1.0, 1.0, 0.97578125, 0.88421875, 1.0, 0.96015625, 0.99171875, 0.86984375, 0.99640625, 0.9646875, 0.88765625, 0.91078125, 1.0, 0.984375, 1.0, 0.8225, 1.0, 0.9784375, 1.0, 1.0, 0.93984375, 0.99578125, 0.85890625, 0.90625, 1.0, 1.0, 0.89125, 0.99140625, 0.96390625, 0.844375, 0.90421875, 0.9121875, 0.97640625, 1.0, 0.96859375, 0.92484375, 0.923125, 0.99125, 1.0, 1.0, 0.94328125, 0.9796875, 0.9625, 0.9628125, 0.93375, 0.9675, 0.9796875, 1.0, 0.94421875, 0.88828125, 0.95, 0.846875, 0.98875, 1.0, 0.86140625, 1.0, 0.9934375, 0.9753125, 0.99484375, 1.0, 1.0, 1.0, 1.0, 0.98671875, 1.0, 0.98453125, 0.95921875, 0.9909375, 1.0, 0.99578125, 1.0, 0.96515625, 0.84515625, 0.845, 0.8878125, 0.8965625, 0.86984375, 0.9946875, 0.97625, 0.99453125, 1.0, 0.87125, 1.0, 0.84296875, 0.93375, 0.9103125, 0.875, 1.0, 0.9609375, 0.97796875, 0.89015625, 0.98640625, 0.90453125, 0.96046875, 1.0, 1.0, 0.93640625, 0.99765625, 0.9065625, 0.9959375, 0.8875, 0.98375, 0.9290625, 0.99484375, 1.0, 0.95078125, 0.96671875, 0.9, 0.9303125, 0.89734375, 0.95203125, 1.0, 0.92421875, 0.893125, 0.958125, 0.978125, 1.0, 0.93296875, 1.0, 0.91046875, 0.9340625, 0.9853125, 0.738125, 1.0, 1.0, 1.0, 0.98796875, 0.9946875, 0.99234375, 0.97265625, 0.9753125, 0.95765625, 0.94953125, 0.85921875, 0.9346875, 0.94609375, 0.9375, 0.9278125, 0.9384375, 0.99234375, 0.99875, 1.0, 0.9659375, 1.0, 0.990625, 0.94, 1.0, 1.0, 0.97046875, 0.9146875, 0.989375, 0.95265625, 0.9728125, 0.9553125, 1.0, 0.93765625, 0.94875, 0.966875, 0.89625, 0.9896875, 1.0, 0.9603125, 1.0, 0.92046875, 1.0, 0.98875, 0.98875, 0.98984375, 0.8240625, 0.96, 1.0, 0.966875, 0.919375, 0.9996875, 0.97078125, 0.99578125, 0.9134375, 0.970625, 1.0, 0.9840625, 0.99109375, 0.90203125, 0.985, 1.0, 0.99203125, 0.985, 0.92015625, 0.93859375, 1.0, 0.90828125, 0.82546875, 0.923125, 1.0, 0.9284375, 1.0, 0.9428125, 0.980625, 0.985625, 0.98359375, 0.85765625, 0.91390625, 0.825625, 0.945625, 0.94265625, 0.9421875, 0.9671875, 1.0, 0.9946875, 0.97375, 1.0, 0.99984375, 0.88921875, 1.0, 0.94125, 0.98953125, 0.96625, 0.9484375, 1.0, 0.97640625, 1.0, 0.89859375, 0.9734375, 0.899375, 0.9634375, 1.0, 0.93859375, 0.95171875, 0.885, 1.0, 0.98234375, 0.964375, 0.890625, 1.0, 0.924375, 0.99890625, 0.99203125, 0.9528125, 0.9371875, 0.99203125, 0.86953125, 1.0, 0.98703125, 0.93484375, 0.88828125, 0.9725, 0.936875, 0.9265625, 1.0, 0.96578125, 1.0, 0.96921875, 0.95515625, 1.0, 0.83421875, 1.0, 0.94921875, 0.99703125, 0.87640625, 0.921875, 1.0, 0.9975, 1.0, 0.97859375, 0.98796875, 0.9034375, 0.95453125, 0.934375, 0.858125, 1.0, 0.98421875, 0.8809375, 0.91265625, 0.98890625, 0.99375, 1.0, 1.0, 0.94421875, 0.96046875, 0.89234375, 0.9209375, 0.9978125, 1.0, 0.9471875, 0.951875, 0.8715625, 0.92015625, 0.98203125, 0.88328125, 0.9865625, 0.9940625, 0.89703125, 0.9175, 1.0, 1.0, 0.9128125, 0.9384375, 1.0, 0.96203125, 1.0, 0.9671875, 0.90734375, 1.0, 0.86125, 0.76796875, 1.0, 0.9009375, 0.9296875, 0.898125, 1.0, 0.98578125, 1.0, 0.91328125, 0.97125, 0.9403125, 0.99671875, 0.9578125, 0.97375, 0.98640625, 0.92609375, 1.0, 0.96796875, 0.9459375, 0.98359375, 0.9971875, 1.0, 1.0, 0.95625, 0.97171875, 0.955, 0.9478125, 0.89625, 0.93671875, 0.97703125, 1.0, 0.9984375, 1.0, 0.9328125, 0.894375, 1.0, 1.0, 0.85, 1.0, 0.98875, 0.93109375, 0.9596875, 0.96828125, 1.0, 0.853125, 0.9553125, 0.94359375, 0.83859375, 0.9753125, 0.97984375, 0.9890625, 0.94953125, 1.0, 0.91625, 0.961875, 0.99953125, 0.87421875, 0.88375, 0.93015625, 0.99609375, 0.95640625, 0.95671875, 0.91359375, 0.934375, 1.0, 0.965625, 0.9753125, 0.9421875, 1.0, 0.9240625, 0.8815625, 1.0, 0.9090625, 0.9884375, 0.8065625, 0.851875, 0.86640625, 0.92390625, 1.0, 0.9915625, 0.94234375, 0.96875, 0.878125, 0.9721875, 1.0, 0.93203125, 1.0, 1.0, 0.91375, 0.91234375, 1.0, 0.9728125, 0.9284375, 0.8759375, 0.98765625, 0.93046875, 0.895, 0.94359375, 0.99875, 1.0, 0.9428125, 0.90109375, 0.9353125, 0.89640625, 0.85515625, 0.765625, 0.99140625, 0.9890625, 1.0, 0.89625, 0.97546875, 0.88890625, 0.98984375, 0.9984375, 0.9090625, 0.90328125, 0.883125, 0.95265625, 0.96859375, 0.94890625, 1.0, 0.95453125, 0.96984375, 0.7784375, 0.92, 0.95890625, 1.0, 0.929375, 0.8534375, 0.9540625, 1.0, 0.98890625, 1.0, 0.87109375, 1.0, 0.9215625, 1.0, 1.0, 0.98953125, 0.89875, 0.8553125, 0.824375, 0.84515625, 0.87375, 1.0, 0.90515625, 0.9153125, 0.97078125, 0.9559375, 0.99265625, 0.9475, 0.9328125, 1.0, 0.958125, 0.97984375, 0.99390625, 1.0, 0.84859375, 0.87703125, 0.7846875, 0.911875, 1.0, 0.98875, 0.96890625, 0.8596875, 0.981875, 0.93609375, 0.93859375, 0.9784375, 1.0, 1.0, 0.99671875, 0.91671875, 0.915625, 0.93796875, 0.9484375, 0.99875, 0.95171875, 0.86015625, 1.0, 0.90859375, 1.0, 0.9096875, 0.97, 0.9903125, 0.99859375, 0.9628125, 0.96375, 0.834375, 0.893125, 0.984375, 0.92, 0.970625, 0.909375, 0.98859375, 0.89875, 0.94125, 1.0, 0.98984375, 0.9990625, 0.969375, 0.99203125, 0.9921875, 0.8290625, 0.9184375, 0.933125, 1.0, 1.0, 1.0, 0.93734375, 0.975, 0.97765625, 1.0, 1.0, 0.8915625, 0.9190625, 0.96265625, 0.9728125, 0.876875, 0.94703125, 0.875, 0.99125, 0.93203125, 0.97125, 0.9090625, 0.9715625, 0.80015625, 1.0, 0.9203125, 0.92578125, 0.88421875, 0.93046875, 0.90734375, 0.96015625, 0.960625, 0.92609375, 0.97671875, 0.9390625, 0.99640625, 0.96875, 1.0, 0.99078125, 0.8834375, 1.0, 1.0, 0.96796875, 0.90609375, 0.989375, 0.89625, 0.9040625, 0.97859375, 0.95625, 0.95921875, 1.0, 0.96890625, 0.98734375, 0.913125, 0.98640625, 0.886875, 0.9734375, 0.92421875, 0.96828125, 1.0, 0.95703125, 0.918125, 0.96578125, 0.981875, 0.99671875, 1.0, 0.94078125, 0.91328125, 1.0, 0.9365625, 0.9925, 0.9325, 0.90625, 0.97015625, 1.0, 0.99296875, 0.96453125, 1.0, 0.931875, 0.97578125, 1.0, 0.96, 0.895625, 0.92203125, 0.9425, 0.99625, 1.0, 1.0, 1.0, 1.0, 0.99140625, 1.0, 0.81296875, 0.89765625, 0.94671875, 1.0, 0.983125, 0.9446875, 1.0, 0.994375, 0.95125, 0.911875, 0.99375, 0.92671875, 1.0, 0.999375, 0.98421875, 0.928125, 0.9903125, 0.9928125, 0.861875, 0.93578125, 0.983125, 0.918125, 0.99484375, 0.96171875, 0.96859375, 0.825, 0.905, 0.98453125, 0.80015625, 1.0, 1.0, 1.0, 0.9484375, 0.9996875, 0.99734375, 0.82140625, 1.0, 0.99375, 0.9759375, 1.0, 1.0, 0.9659375, 0.9334375, 1.0, 1.0, 0.86984375, 0.994375, 0.87453125, 0.94359375, 0.90359375, 0.96015625, 0.97140625, 0.97609375, 0.84828125, 0.955, 0.9840625, 1.0, 0.9903125, 0.8428125, 1.0, 1.0, 0.9740625, 0.999375, 0.9575, 0.93078125, 0.999375, 1.0, 0.96859375, 1.0, 1.0, 0.966875, 0.90265625, 0.83078125, 0.9490625, 1.0, 1.0, 0.94890625, 0.9334375, 0.9896875, 0.95671875, 0.94515625, 1.0, 0.89828125, 0.92953125, 0.986875, 0.8578125, 0.97265625, 0.8125, 0.9740625, 1.0, 0.95875, 0.97625, 1.0, 0.97, 0.85640625, 0.995625, 0.97484375, 0.97140625, 0.94515625, 0.98, 0.96328125, 0.8840625, 0.97078125, 0.95828125, 0.96203125, 0.92828125, 0.9803125, 0.90515625, 0.92359375, 0.9703125, 0.966875, 0.94578125, 0.90859375, 1.0, 0.9240625, 0.9771875, 0.9690625, 0.98453125, 0.9790625, 0.93890625, 1.0, 0.87984375, 0.96359375, 0.79875, 1.0, 0.87453125, 0.98640625, 0.98734375, 0.94921875, 1.0, 0.9440625, 0.96375, 0.99, 0.89109375, 0.9640625, 0.94546875, 1.0, 0.94703125, 0.93484375, 1.0, 1.0, 0.97328125, 0.951875, 0.9175, 0.88578125, 1.0, 0.95828125, 0.8203125, 0.87828125, 0.94484375, 0.919375, 0.9825, 1.0, 0.92234375, 0.8603125, 1.0, 0.94171875, 0.9959375, 0.95859375, 0.980625, 1.0, 1.0, 0.87078125, 0.96015625, 0.98625, 0.861875, 0.9109375, 0.9496875, 0.97234375, 0.97171875, 1.0, 1.0, 0.9984375, 1.0, 0.95828125, 1.0, 0.95609375, 0.97359375, 0.95625, 0.9221875, 0.9284375, 0.8815625, 0.96, 1.0, 0.88640625, 0.96734375, 0.93578125, 0.94890625, 0.974375, 1.0, 1.0, 1.0, 0.909375, 0.89578125, 0.96828125, 0.8953125, 1.0, 0.94890625, 1.0, 0.90359375, 0.96921875, 0.939375, 0.9675, 0.915625, 1.0, 0.9796875, 0.925, 0.994375, 1.0, 0.949375, 1.0, 0.9771875, 0.96796875, 0.8646875, 0.91140625, 0.98921875, 0.99890625, 0.9871875, 0.9525, 0.98421875, 0.93390625, 0.96046875, 1.0, 0.9903125, 0.93796875, 0.91984375, 0.84125, 0.983125, 0.94375, 0.89984375, 0.92078125, 0.946875, 0.95921875, 0.98921875, 0.99734375, 0.9209375, 0.8915625, 1.0, 1.0, 1.0, 0.89984375, 1.0, 0.9090625, 0.97515625, 0.84234375, 0.9715625, 1.0, 1.0, 0.99984375, 0.959375, 0.90984375, 0.96546875, 0.845, 1.0, 0.89453125, 0.9915625, 0.97078125, 1.0, 0.97609375, 0.89328125, 0.90703125, 0.92609375, 0.98328125, 0.97734375, 0.96625, 0.93671875, 1.0, 1.0, 0.93546875, 0.9578125, 0.97609375, 0.9665625, 0.946875, 0.950625, 0.98703125, 0.883125, 1.0, 0.950625, 1.0, 0.94140625, 0.92171875, 0.91046875, 0.9796875, 0.919375, 0.910625, 0.93875, 0.9640625, 0.96265625, 1.0, 0.96484375, 0.98203125, 0.99984375, 0.95484375, 0.99765625, 1.0, 0.950625, 0.99765625, 0.98109375, 0.970625, 0.975, 0.9378125, 0.83765625, 1.0, 0.98703125, 0.84546875, 0.88390625, 0.9684375, 0.9134375, 0.99015625, 0.97328125, 0.98140625, 0.975, 0.8803125, 1.0, 0.89171875, 0.88109375, 0.99859375, 0.99140625, 1.0, 0.85921875, 0.9696875, 0.95890625, 0.98546875, 0.90046875, 0.90171875, 1.0, 0.99953125, 0.9575, 0.95171875, 0.936875, 0.8371875, 0.9765625, 0.8746875, 0.96140625, 1.0, 0.97546875, 0.98046875, 0.88578125, 0.94296875, 0.93203125, 0.96046875, 0.8746875, 0.94890625, 1.0, 0.893125, 0.98234375, 0.99453125, 0.9815625, 0.896875, 0.960625, 0.7546875, 0.98390625, 0.96078125, 1.0, 0.97234375, 0.9428125, 0.974375, 0.9790625, 0.8684375, 0.92578125, 1.0, 0.92609375, 1.0, 1.0, 0.945625, 0.979375, 0.96578125, 0.92, 0.9246875, 1.0, 0.94078125, 0.97375, 0.944375, 0.98890625, 1.0, 0.99640625, 0.99234375, 1.0, 1.0, 0.991875, 0.9725, 0.97296875, 0.9134375, 0.9234375, 0.90515625, 1.0, 0.8271875, 0.91125, 0.99046875, 0.935, 0.94, 0.92109375, 0.881875, 1.0, 0.89375, 0.97890625, 0.9978125, 0.9975, 1.0, 1.0, 1.0, 0.924375, 1.0, 0.94359375, 0.9278125, 0.99859375, 0.88234375, 0.951875, 0.843125, 0.93984375, 1.0, 0.91125, 0.9409375, 1.0, 0.960625, 0.8915625, 0.94546875, 0.955, 0.97640625, 0.99953125, 0.97, 0.794375, 0.934375, 1.0, 0.93171875, 1.0, 1.0, 1.0, 0.92796875, 0.94734375, 1.0, 0.97390625, 0.9834375, 1.0, 0.860625, 0.9965625, 0.9396875, 1.0, 0.961875, 1.0, 1.0, 0.9609375, 0.9584375, 1.0, 0.92953125, 1.0, 1.0, 0.92203125, 0.92625, 0.89515625, 0.9959375, 0.9778125, 1.0, 0.99296875, 0.94234375, 0.9684375, 0.98734375, 0.93015625, 0.97765625, 0.86453125, 0.95953125, 0.85734375, 0.97328125, 0.9175, 0.9975, 1.0, 0.91546875, 0.9690625, 0.96359375, 0.97703125, 0.90125, 0.943125, 0.9778125, 0.91390625, 1.0, 0.92578125, 0.9265625, 0.939375, 0.9921875, 0.95890625, 1.0, 0.99375, 0.8834375, 0.94859375, 0.966875, 0.983125, 0.91625, 0.98453125, 0.89265625, 0.99484375, 0.93390625, 0.8890625, 0.931875, 0.8678125, 0.90484375, 1.0, 0.945, 0.85796875, 1.0, 0.989375, 0.95671875, 0.9225, 0.9725, 0.96390625, 1.0, 0.86875, 0.966875, 0.905625, 0.8740625, 0.9503125, 0.9859375, 0.93828125, 1.0, 1.0, 0.9584375, 0.965625, 1.0, 0.98484375, 1.0, 0.995625, 0.94703125, 0.9184375, 1.0, 0.9990625, 1.0, 0.92140625, 0.91, 0.9821875, 0.9890625, 0.68125, 0.95265625, 0.96875, 0.84546875, 0.95828125, 1.0, 0.96609375, 0.94140625, 0.95703125, 0.995, 1.0, 0.9534375, 1.0, 0.955, 0.9946875, 1.0, 0.97671875, 0.965625, 0.96609375, 0.9403125, 0.97578125, 0.96234375, 0.94984375, 0.98046875, 1.0, 0.98875, 0.8, 0.96546875, 0.986875, 0.9540625, 0.919375, 0.98015625, 1.0, 0.93734375, 0.924375, 0.93859375, 0.9603125, 1.0, 0.9725, 0.90625, 0.964375, 0.991875, 1.0, 1.0, 0.98390625, 0.89796875, 0.98921875, 0.95375, 0.9428125, 0.964375, 0.95359375, 0.8921875, 0.9003125, 0.97671875, 1.0, 0.9284375, 0.9259375, 0.97515625, 0.9859375, 0.92796875, 0.9371875, 0.95078125, 0.956875, 1.0, 0.92484375, 0.910625, 0.91484375, 0.89453125, 0.99203125, 0.9853125, 0.94859375, 0.89796875, 0.9746875, 1.0, 0.97671875, 0.99078125, 0.95328125, 0.8546875, 0.89859375, 1.0, 0.8296875, 1.0, 0.95921875, 0.92046875, 0.803125, 0.98546875, 0.9584375, 1.0, 0.94734375, 0.9615625, 0.91875, 0.96671875, 0.85265625, 0.93796875, 0.9296875, 0.84171875, 0.9628125, 0.90828125, 0.9046875, 1.0, 0.95859375, 0.9865625, 0.98234375, 1.0, 0.97703125, 0.8640625, 0.99734375, 0.8678125, 1.0, 0.88265625, 0.92140625, 0.9875, 0.8840625, 0.94796875, 0.95984375, 0.9946875, 0.99171875, 0.985, 0.953125, 0.96796875, 0.92734375, 1.0, 0.9690625, 0.89046875, 0.986875, 0.9021875, 0.91421875, 1.0, 0.9334375, 0.966875, 0.9403125, 0.86703125, 0.89515625, 0.89546875, 0.9740625, 0.93703125, 0.97875, 0.96671875, 1.0, 1.0, 0.81265625, 0.87640625, 0.85171875, 0.9003125, 0.8965625, 0.8178125, 0.95875, 0.9871875, 0.97671875, 0.92296875, 0.948125, 0.9565625, 1.0, 0.9496875, 0.96765625, 0.9665625, 0.9803125, 1.0, 0.95125, 1.0, 0.97703125, 1.0, 0.90828125, 0.98546875, 0.873125, 0.98359375, 0.9946875, 0.9996875, 0.93359375, 0.88125, 0.91140625, 0.98640625, 1.0, 1.0, 0.96484375, 0.9859375, 0.9521875, 0.97046875, 0.99421875, 0.9634375, 1.0, 1.0, 1.0, 1.0, 0.9015625, 0.91359375, 0.98640625, 0.89359375, 0.97984375, 0.9096875, 1.0, 0.92765625, 0.78875, 1.0, 0.99359375, 0.8678125, 0.96765625, 0.95265625, 0.928125, 0.9328125, 0.99546875, 0.976875, 0.93015625, 0.9971875, 0.934375, 0.92109375, 0.9059375, 0.86828125, 0.97109375, 1.0, 0.89265625, 0.89265625, 0.88875, 0.7609375, 1.0, 0.981875, 0.94171875, 0.85953125, 0.8928125, 0.89828125, 0.9353125, 0.94140625, 0.96015625, 0.9721875, 0.9884375, 0.91265625, 0.88, 0.9996875, 1.0, 0.92640625, 0.98484375, 1.0, 0.9840625, 0.9590625, 0.930625, 1.0, 0.96859375, 0.96296875, 1.0, 0.88890625, 1.0, 0.915, 1.0, 0.9909375, 0.88953125, 0.87578125, 0.99765625, 0.9975, 0.96046875, 0.95359375, 0.92484375, 1.0, 0.97828125, 0.98453125, 0.97578125, 1.0, 0.920625, 1.0, 0.916875, 0.96140625, 0.96390625, 0.93, 0.995, 0.90875, 0.97484375, 0.90578125, 0.90234375, 0.9978125, 1.0, 0.99359375, 0.89984375, 0.7103125, 0.9859375, 0.993125, 0.82421875, 0.9740625, 0.844375, 1.0, 0.946875, 0.991875, 0.94140625, 0.94203125, 0.85296875, 0.99625, 1.0, 0.8878125, 1.0, 0.95671875, 1.0, 0.97234375, 0.87515625, 0.91421875, 0.9715625, 0.90640625, 0.98734375, 0.94109375, 0.99734375, 0.8946875, 1.0, 0.92953125, 1.0, 0.97171875, 0.9303125, 0.97015625, 1.0, 0.99703125, 1.0, 0.8590625, 0.99078125, 1.0, 1.0, 0.91921875, 0.92078125, 0.993125, 0.98578125, 0.81125, 0.91640625, 0.96015625, 0.90875, 1.0, 1.0, 1.0, 0.9478125, 0.9709375, 0.99203125, 0.9521875, 0.99890625, 0.94703125, 0.92578125, 0.98078125, 1.0, 1.0, 1.0, 0.965, 0.95390625, 0.9909375, 0.97, 0.97015625, 0.96125, 1.0, 0.9990625, 1.0, 0.94109375, 1.0, 1.0, 0.8740625, 1.0, 1.0, 0.91546875, 0.92890625, 1.0, 0.93234375, 0.9803125, 0.9703125, 0.96859375, 0.9071875, 0.96625, 0.92984375, 0.99859375, 0.97796875, 0.84421875, 1.0, 1.0, 0.9290625, 0.94203125, 0.94265625, 0.88359375, 0.8575, 0.98015625, 0.998125, 0.96171875, 0.85859375, 0.79828125, 0.95125, 0.88515625, 1.0, 0.87, 0.966875, 1.0, 1.0, 0.985, 0.99296875, 0.86390625, 0.97671875, 0.780625, 0.9671875, 0.98859375, 0.9625, 0.97984375, 1.0, 1.0, 0.98234375, 0.9359375, 0.93234375, 0.95390625, 1.0, 0.9621875, 0.9815625, 0.91828125, 0.9440625, 0.879375, 0.89734375, 0.9153125, 0.9725, 0.93140625, 0.93609375, 0.93546875, 0.98640625, 1.0, 0.925, 1.0, 1.0, 0.98890625, 1.0, 0.90953125, 1.0, 0.85703125, 0.95015625, 1.0, 0.97859375, 0.96171875, 0.91671875, 0.90921875, 0.94546875, 0.864375, 0.94890625, 0.94140625, 0.9303125, 0.968125, 0.9534375, 0.9459375, 0.98453125, 0.9815625, 1.0, 1.0, 0.99734375, 0.85890625, 1.0, 1.0, 0.9134375, 0.9315625, 0.92046875, 0.930625, 0.92875, 0.94625, 1.0, 0.96015625, 0.83125, 0.99640625, 0.8453125, 0.98015625, 1.0, 0.985, 0.8915625, 0.87546875, 0.980625, 0.91625, 0.88484375, 0.968125, 1.0, 1.0, 0.9325, 0.95453125, 1.0, 0.9728125, 0.88265625, 0.9653125, 0.99671875, 0.91671875, 0.95078125, 1.0, 1.0, 1.0, 0.93734375, 0.969375, 0.93234375, 1.0, 1.0, 1.0, 0.99109375, 0.98125, 0.92015625, 0.87078125, 0.955625, 0.95359375, 0.9471875, 0.97125, 0.91953125, 1.0, 1.0, 0.8628125, 1.0, 1.0, 0.98140625, 0.8628125, 0.87296875, 1.0, 1.0, 1.0, 0.8703125, 1.0, 0.9609375, 1.0, 1.0, 0.9084375, 0.9940625, 0.96484375, 0.9909375, 0.99265625, 0.9434375, 0.9165625, 0.94140625, 1.0, 1.0, 0.96171875, 0.96390625, 0.98390625, 1.0, 1.0, 0.9878125, 0.86109375, 1.0, 0.92640625, 0.9396875, 0.85859375, 1.0, 0.95265625, 1.0, 0.986875, 0.97921875, 0.96765625, 0.91625, 0.94640625, 1.0, 0.86734375, 1.0, 0.93390625, 0.99328125, 0.93296875, 0.9640625, 0.91640625, 1.0, 0.9978125, 1.0, 0.90078125, 0.91953125, 0.90328125, 0.88171875, 0.87625, 0.9790625, 0.98328125, 0.9296875, 1.0, 0.95640625, 0.939375, 0.92703125, 0.95125, 0.9371875, 0.96546875, 0.99890625, 0.9359375, 0.98359375, 0.97875, 0.979375, 0.92671875, 0.9925, 1.0, 1.0, 0.91484375, 0.90484375, 0.98125, 1.0, 0.91296875, 0.88421875, 0.9834375, 1.0, 0.9459375, 0.961875, 0.94484375, 0.9696875, 1.0, 0.88859375, 1.0, 0.94421875, 0.9734375, 1.0, 0.99578125, 0.8490625, 0.9125, 0.84453125, 0.87140625, 0.95890625, 1.0, 1.0, 0.93140625, 0.85015625, 0.92859375, 0.96296875, 0.926875, 0.97140625, 0.86640625, 1.0, 0.99109375, 0.92984375, 0.9584375, 1.0, 0.9109375, 1.0, 0.9659375, 1.0, 0.8584375, 0.975625, 0.95546875, 0.9509375, 0.97671875, 0.95515625, 1.0, 0.97265625, 0.95296875, 1.0, 0.97390625, 0.9725, 0.89578125, 0.984375, 1.0, 0.94828125, 0.98453125, 1.0, 1.0, 1.0, 1.0, 0.97625, 1.0, 0.99859375, 0.9965625, 0.92234375, 0.99546875, 0.895625, 0.9815625, 1.0, 0.9578125, 0.93140625, 1.0, 0.93765625, 1.0, 0.93359375, 0.88578125, 0.87359375, 0.98046875, 0.97984375, 1.0, 1.0, 0.99671875, 0.94265625, 0.9996875, 0.93078125, 0.95296875, 0.9328125, 0.8734375, 1.0, 0.83875, 1.0, 0.95265625, 1.0, 0.9871875, 1.0, 0.9571875, 0.955, 0.86640625, 1.0, 0.97265625, 0.98078125, 0.97171875, 1.0, 0.99953125, 1.0, 1.0, 0.9928125, 0.9040625, 0.99765625, 0.95375, 0.91171875, 0.9984375, 0.95453125, 0.97609375, 0.9959375, 0.9421875, 0.98515625, 0.98859375, 0.96671875, 0.84671875, 0.9165625, 0.98296875, 1.0, 0.89390625, 0.98328125, 1.0, 0.98125, 0.9728125, 0.96421875, 0.95796875, 1.0, 0.89203125, 0.94484375, 0.9478125, 0.98609375, 0.9925, 1.0, 1.0, 1.0, 0.99234375, 1.0, 1.0, 0.98421875, 0.9028125, 0.893125, 1.0, 0.98015625, 0.8540625, 0.99296875, 0.92515625, 0.95625, 0.99671875, 1.0, 0.93484375, 0.9325, 0.91203125, 0.99421875, 0.97015625, 1.0, 0.9846875, 0.95234375, 1.0, 0.94140625, 0.96109375, 1.0, 0.98265625, 0.93578125, 0.9634375, 0.995, 0.95296875, 1.0, 0.96265625, 0.8859375, 1.0, 0.868125, 1.0, 0.95125, 0.99796875, 0.9965625, 1.0, 1.0, 0.933125, 0.9721875, 0.9746875, 1.0, 1.0, 0.9028125, 0.97015625, 0.90046875, 0.9290625, 0.91, 0.9328125, 1.0, 1.0, 0.891875, 0.86203125, 0.8971875, 0.96015625, 0.9515625, 0.9953125, 1.0, 0.9403125, 1.0, 1.0, 0.99421875, 0.9034375, 0.86328125, 0.95046875, 0.9671875, 0.9725, 0.97890625, 0.94203125, 0.971875, 0.8696875, 0.91453125, 0.96796875, 0.9865625, 0.95109375, 0.96765625, 0.96328125, 0.98375, 0.9159375, 0.96015625, 0.88953125, 0.91078125, 0.860625, 0.98515625, 0.67796875, 0.965625, 0.8821875, 0.96046875, 0.98359375, 1.0, 1.0, 1.0, 0.99921875, 1.0, 1.0, 0.9653125, 1.0, 0.96703125, 0.985, 0.95609375, 1.0, 0.83609375, 0.8490625, 0.91046875, 0.9709375, 0.9834375, 0.80953125, 0.86140625, 0.9271875, 1.0, 0.893125, 0.9503125, 0.90921875, 0.948125, 0.97375, 0.93328125, 1.0, 0.99390625, 0.921875, 1.0, 0.9659375, 1.0, 1.0, 0.930625, 1.0, 0.9471875, 0.94015625, 0.92515625, 0.95515625, 1.0, 0.96515625, 0.99375, 0.9909375, 0.97484375, 0.9378125, 1.0, 1.0, 0.91234375, 0.911875, 1.0, 0.8215625, 1.0, 0.9428125, 0.88640625, 0.9871875, 0.89328125, 1.0, 0.9853125, 1.0, 0.9325, 0.9590625, 1.0, 0.9478125, 0.9775, 1.0, 0.988125, 0.9290625, 0.87515625, 1.0, 0.98796875, 0.98453125, 0.9940625, 0.95984375, 0.98203125, 0.91625, 1.0, 0.91453125, 0.96046875, 0.9978125, 1.0, 1.0, 1.0, 0.89859375, 0.980625, 1.0, 0.968125, 0.89734375, 0.97578125, 0.9915625, 1.0, 1.0, 0.979375, 1.0, 0.92828125, 0.97109375, 0.92125, 0.8759375, 0.9259375, 0.936875, 0.93578125, 0.86234375, 0.98046875, 0.87109375, 0.98875, 0.9746875, 0.98828125, 1.0, 0.9771875, 0.99359375, 1.0, 0.97, 1.0, 0.91734375, 0.964375, 0.98328125, 0.9175, 0.97421875, 0.9940625, 0.92703125, 0.9021875, 1.0, 1.0, 0.91828125, 0.93484375, 1.0, 0.99015625, 0.97734375, 0.9325, 0.94265625, 0.9265625, 0.98390625, 0.9878125, 0.895, 0.90109375, 1.0, 0.93453125, 0.9440625, 0.95296875, 0.975625, 0.80265625, 1.0, 0.89828125, 0.89390625, 0.95546875, 1.0, 0.96140625, 0.921875, 1.0, 0.83984375, 0.94625, 0.9075, 1.0, 0.95140625, 0.925, 0.94796875, 1.0, 1.0, 0.99875, 0.96, 0.91765625, 0.92671875, 0.92546875, 1.0, 0.94421875, 0.95859375, 0.99078125, 0.96890625, 0.91859375, 0.980625, 0.9821875, 0.9725, 0.99203125, 0.9575, 1.0, 0.94734375, 1.0, 0.9746875, 0.88890625, 0.958125, 0.98046875, 1.0, 0.9621875, 0.99375, 0.9703125, 0.92171875, 0.94328125, 0.88578125, 0.95203125, 0.8959375, 1.0, 0.9103125, 1.0, 1.0, 1.0, 0.96953125, 1.0, 0.96265625, 0.99375, 0.91859375, 0.95765625, 1.0, 1.0, 1.0, 1.0, 0.89328125, 0.98484375, 0.98265625, 1.0, 1.0, 0.9703125, 0.9328125, 0.96921875, 1.0, 1.0, 0.9646875, 0.9828125, 0.92859375, 0.981875, 0.9278125, 0.958125, 0.85359375, 0.77296875, 0.929375, 0.85203125, 0.9790625, 0.9884375, 0.99390625, 0.93515625, 0.94484375, 0.97875, 0.93078125, 0.90015625, 0.96578125, 1.0, 0.85015625, 0.88140625, 1.0, 1.0, 0.91953125, 0.99015625, 0.9775, 0.80703125, 0.969375, 0.96515625, 1.0, 0.88125, 0.86265625, 1.0, 0.95390625, 1.0, 0.86859375, 0.923125, 0.974375, 0.9678125, 0.8525, 0.9984375, 1.0, 0.90796875, 0.8265625, 0.92109375, 0.824375, 0.90921875, 0.91046875, 0.97140625, 0.94375, 1.0, 0.8903125, 1.0, 0.9971875, 0.79828125, 0.96796875, 0.84703125, 1.0, 1.0, 0.9878125, 1.0, 0.9453125, 1.0, 0.86265625, 0.86359375, 0.94265625, 0.8146875, 1.0, 0.96359375, 0.928125, 0.990625, 0.96859375, 0.833125, 0.99703125, 0.88765625, 0.88703125, 1.0, 1.0, 1.0, 0.87203125, 0.94578125, 0.9253125, 0.9821875, 0.8890625, 0.970625, 1.0, 1.0, 0.91078125, 1.0, 1.0, 1.0, 1.0, 0.87421875, 1.0, 1.0, 0.93984375, 1.0, 0.96734375, 1.0, 0.87140625, 0.98125, 0.910625, 1.0, 0.9115625, 0.9453125, 0.90390625, 0.93171875, 0.96578125, 0.88640625, 0.954375, 0.83203125, 0.97453125, 0.86328125, 1.0, 1.0, 0.9621875, 0.89453125, 0.9965625, 0.95453125, 0.93828125, 0.98796875, 0.995, 0.99625, 0.84390625, 0.89171875, 0.9853125, 1.0, 0.993125, 0.875, 0.8796875, 0.816875, 0.93625, 0.953125, 0.91609375, 0.98234375, 0.97046875, 0.9115625, 0.87859375, 1.0, 0.93140625, 0.95796875, 0.955625, 0.86421875, 0.85, 0.89765625, 1.0, 1.0, 0.914375, 0.95359375, 0.85046875, 0.995, 0.99953125, 0.8975, 1.0, 0.95, 0.95234375, 0.98265625, 0.9475, 0.98625, 1.0, 0.945, 0.86140625, 0.94546875, 0.95734375, 1.0, 1.0, 0.89828125, 0.9578125, 0.99859375, 0.95078125, 0.92265625, 0.9384375, 0.9371875, 0.94171875, 0.95328125, 0.88, 0.935625, 0.92109375, 1.0, 0.958125, 0.950625, 0.98328125, 1.0, 0.85109375, 0.98390625, 0.99984375, 1.0, 0.8875, 1.0, 0.85125, 0.96859375, 0.92, 0.98703125, 1.0, 0.9909375, 0.93171875, 0.9590625, 1.0, 0.97671875, 1.0, 0.9940625, 0.996875, 0.94375, 0.9940625, 0.80359375, 0.9075, 0.97578125, 1.0, 0.97625, 0.9065625, 1.0, 0.92296875, 0.92296875, 0.98765625, 1.0, 1.0, 0.9821875, 1.0, 0.83390625, 0.91140625, 1.0, 0.918125, 0.97359375, 0.939375, 0.9578125, 0.9775, 0.94671875, 0.85421875, 0.9746875, 0.9740625, 0.873125, 0.93296875, 0.93453125, 0.95921875, 1.0, 0.945, 0.92859375, 0.92609375, 0.981875, 0.87859375, 1.0, 0.9853125, 0.99078125, 0.9921875, 0.975, 1.0, 0.970625, 0.9471875, 0.9921875, 0.9428125, 0.945625, 0.81, 0.913125, 1.0, 0.99390625, 0.9203125, 0.705, 0.96453125, 1.0, 0.99578125, 0.9290625, 0.9534375, 0.9840625, 1.0, 0.96125, 0.99109375, 1.0, 0.8825, 0.92609375, 0.94734375, 0.9734375, 0.77296875, 1.0, 0.8665625, 1.0, 0.92140625, 0.9971875, 1.0, 0.965625, 0.9721875, 0.9665625, 0.98875, 0.976875, 1.0, 0.984375, 0.9653125, 0.9446875, 0.95671875, 1.0, 0.87640625, 0.88921875, 1.0, 1.0, 0.99390625, 0.9775, 0.93203125, 0.91171875, 0.97578125, 0.974375, 0.97953125, 0.97875, 1.0, 0.91546875, 0.99234375, 0.94640625, 1.0, 0.88078125, 0.864375, 0.80921875, 0.97015625, 1.0, 1.0, 0.981875, 0.93703125, 0.9603125, 0.89609375, 0.86671875, 0.98140625, 0.99515625, 0.96984375, 0.9178125, 0.98375, 0.88328125, 0.9228125, 0.9115625, 0.86953125, 0.88578125, 1.0, 0.99171875, 0.92625, 0.99703125, 0.9615625, 1.0, 0.99765625, 0.96890625, 0.9540625, 0.99671875, 0.98875, 0.9171875, 0.9909375, 1.0, 1.0, 0.88125, 0.98125, 1.0, 1.0, 0.91515625, 0.99796875, 1.0, 0.87546875, 1.0, 0.97328125, 1.0, 0.88796875, 1.0, 0.9496875, 0.90046875, 1.0, 1.0, 0.9984375, 0.96390625, 1.0, 0.96734375, 1.0, 0.9878125, 0.93890625, 0.82921875, 1.0, 1.0, 0.88921875, 1.0, 1.0, 0.91609375, 0.87359375, 1.0, 0.9615625, 0.95859375, 0.9665625, 0.99640625, 0.9521875, 0.85828125, 0.956875, 0.9209375, 1.0, 1.0, 1.0, 0.96171875, 0.97671875, 0.9071875, 0.98875, 0.97671875, 0.91703125, 0.9515625, 0.88515625, 0.925, 0.90546875, 0.97328125, 0.88, 0.89140625, 0.945, 0.87765625, 1.0, 0.9565625, 0.94046875, 0.8321875, 0.94796875, 0.84015625, 0.9953125, 0.96515625, 1.0, 1.0, 0.92421875, 0.891875, 0.9796875, 1.0, 0.93703125, 1.0, 0.99796875, 0.91734375, 0.926875, 0.96859375, 0.93515625, 0.9565625, 0.866875, 0.97515625, 0.93046875, 1.0, 0.98234375, 0.97078125, 0.99734375, 0.94265625, 0.99328125, 0.98796875, 1.0, 0.96046875, 0.873125, 0.95859375, 0.93328125, 1.0, 0.989375, 0.98125, 0.97234375, 0.94421875, 0.99453125, 0.8965625, 0.99265625, 0.90671875, 1.0, 1.0, 0.96984375, 0.99984375, 0.94390625, 0.9584375, 0.9115625, 1.0, 0.9709375, 1.0, 0.96078125, 0.984375, 1.0, 0.9928125, 0.8734375, 0.88515625, 0.9278125, 0.99625, 0.9353125, 0.8809375, 0.9975, 0.9540625, 0.9753125, 1.0, 0.9825, 0.96828125, 1.0, 1.0, 1.0, 0.99796875, 0.88390625, 0.9534375, 0.99984375, 0.97421875, 1.0, 0.99578125, 0.91625, 1.0, 0.97359375, 0.91984375, 0.96671875, 1.0, 0.955, 0.98015625, 0.9665625, 0.98109375, 0.94265625, 0.94921875, 0.91359375, 0.96625, 0.9009375, 0.8134375, 0.960625, 0.98515625, 0.97234375, 0.96578125, 1.0, 0.9309375, 0.92046875, 1.0, 0.97625, 0.98078125, 0.8821875, 0.98296875, 0.94109375, 0.93453125, 1.0, 1.0, 0.92046875, 0.98625, 0.9725, 0.93921875, 0.97859375, 1.0, 0.918125, 1.0, 0.911875, 0.95859375, 0.98734375, 0.8909375, 0.9940625, 0.90859375, 0.976875, 0.96328125, 0.89875, 0.97515625, 1.0, 0.93640625, 0.9528125, 0.91875, 0.889375, 0.95484375, 0.9578125, 1.0, 0.92140625, 0.93171875, 0.97875, 0.941875, 0.9215625, 1.0, 0.9875, 0.8828125, 1.0, 1.0, 0.9778125, 1.0, 0.87890625, 0.8884375, 1.0, 0.98703125, 1.0, 1.0, 1.0, 0.93015625, 1.0, 0.9825, 0.95171875, 1.0, 0.95640625, 0.97875, 0.94390625, 0.9053125, 0.92125, 1.0, 1.0, 0.98859375, 0.90234375, 0.83765625, 0.974375, 0.9478125, 0.98703125, 1.0, 0.99125, 1.0, 1.0, 1.0, 0.97015625, 0.97859375, 0.9396875, 0.9290625, 1.0, 0.94921875, 0.98359375, 0.9075, 0.92859375, 1.0, 0.86046875, 1.0, 1.0, 0.88921875, 0.8534375, 1.0, 0.9228125, 0.9084375, 0.97734375, 0.93015625, 0.99640625, 1.0, 0.884375, 0.8378125, 1.0, 0.99734375, 0.98265625, 0.93109375, 0.97921875, 1.0, 1.0, 0.90421875, 0.88640625, 0.9253125, 1.0, 1.0, 0.9946875, 0.85328125, 0.89265625, 0.98421875, 0.9853125, 0.9990625, 0.979375, 0.93828125, 0.98453125, 1.0, 0.98375, 0.84203125, 0.931875, 0.8975, 1.0, 0.95671875, 0.94734375, 0.9153125, 0.95671875, 1.0, 0.89734375, 0.98921875, 0.9821875, 0.9178125, 1.0, 0.895625, 0.91109375, 0.97546875, 0.8953125, 0.94125, 0.98515625, 0.9396875, 0.9184375, 0.9690625, 0.9778125, 1.0, 0.96984375, 0.979375, 1.0, 0.86875, 1.0, 0.9171875, 0.9584375, 1.0, 0.9684375, 0.89390625, 0.90765625, 1.0, 0.961875, 1.0, 0.981875, 0.9671875, 0.97234375, 1.0, 0.98890625, 0.96828125, 0.87734375, 1.0, 0.95375, 1.0, 1.0, 0.96984375, 0.87609375, 0.95421875, 1.0, 0.9625, 0.8265625, 0.99546875, 0.9715625, 0.99546875, 0.99359375, 0.88640625, 1.0, 0.9309375, 0.97265625, 0.8953125, 1.0, 0.93546875, 0.95390625, 0.9640625, 0.92859375, 0.953125, 1.0, 0.9721875, 0.98078125, 0.99890625, 0.946875, 1.0, 0.94078125, 1.0, 0.95703125, 1.0, 0.9928125, 1.0, 0.990625, 0.80828125, 1.0, 0.9071875, 0.9384375, 0.9615625, 0.9121875, 0.8940625, 0.9771875, 0.90921875, 1.0, 0.91515625, 1.0, 0.9128125, 0.98828125, 0.86625, 0.923125, 0.98125, 0.96796875, 0.936875, 0.92765625, 0.91703125, 0.9140625, 1.0, 0.9628125, 0.87578125, 0.8625, 0.85015625, 0.93796875, 0.965, 1.0, 1.0, 0.91328125, 1.0, 0.9128125, 0.9825, 0.97859375, 0.98203125, 0.8096875, 0.996875, 0.86875, 0.8609375, 0.9834375, 0.9153125, 0.9359375, 0.9603125, 0.9053125, 1.0, 1.0, 0.97546875, 0.99625, 0.98, 0.94515625, 0.96796875, 0.84421875, 1.0, 0.94203125, 0.95625, 1.0, 0.98265625, 0.9659375, 1.0, 0.96609375, 0.99875, 0.95125, 0.86625, 0.899375, 0.946875, 0.97625, 1.0, 1.0, 0.97453125, 0.9778125, 0.98640625, 0.98296875, 0.93875, 0.91625, 1.0, 0.9884375, 0.93625, 0.938125, 0.94609375, 1.0, 0.9403125, 0.95171875, 1.0, 0.95265625, 0.875, 0.9659375, 0.94921875, 0.99203125, 1.0, 0.92125, 0.88796875, 0.96890625, 1.0, 0.9065625, 0.95421875, 0.98921875, 1.0, 0.92421875, 0.97875, 0.9596875, 0.96109375, 0.90625, 0.980625, 1.0, 0.95859375, 0.9953125, 0.9853125, 1.0, 0.925625, 0.87171875, 0.93734375, 0.953125, 1.0, 0.92671875, 0.96625, 0.96296875, 0.9784375, 0.98640625, 0.9375, 0.96265625, 1.0, 0.9728125, 1.0, 0.91703125, 0.93703125, 1.0, 0.99484375, 1.0, 0.94546875, 0.99203125, 0.97265625, 0.933125, 1.0, 1.0, 0.9690625, 0.87828125, 1.0, 0.9703125, 0.9621875, 0.9315625, 1.0, 1.0, 0.88015625, 0.964375, 0.96796875, 0.7575, 0.8975, 0.84828125, 0.964375, 1.0, 0.92046875, 0.964375, 0.9346875, 1.0, 0.993125, 0.88265625, 0.8334375, 0.921875, 0.903125, 0.90546875, 1.0, 0.966875, 0.8853125, 0.99390625, 0.89625, 0.946875, 0.9584375, 0.98671875, 0.846875, 1.0, 0.98515625, 1.0, 0.9803125, 0.91953125, 1.0, 1.0, 0.98546875, 0.89859375, 0.93328125, 0.959375, 0.8725, 0.99578125, 0.915625, 0.8575, 0.97953125, 0.8571875, 1.0, 1.0, 1.0, 1.0, 0.9384375, 1.0, 0.941875, 0.9559375, 1.0, 0.96765625, 0.99578125, 0.89421875, 0.9475, 0.95484375, 0.9284375, 0.89515625, 0.97375, 0.9634375, 0.97703125, 0.88984375, 0.83671875, 1.0, 0.9484375, 0.88046875, 1.0, 0.886875, 0.87734375, 1.0, 1.0, 1.0, 0.9803125, 1.0, 0.93125, 0.964375, 0.9009375, 0.956875, 1.0, 0.94578125, 0.97984375, 0.96765625, 1.0, 0.98671875, 0.7575, 0.93515625, 0.95171875, 0.891875, 0.87203125, 0.95609375, 0.99515625, 0.98796875, 0.97078125, 0.981875, 0.82296875, 1.0, 0.979375, 0.90796875, 0.99140625, 0.98859375, 0.9371875, 0.98765625, 1.0, 0.938125, 0.9675, 0.96796875, 1.0, 0.925625, 0.935625, 1.0, 0.92875, 0.893125, 0.99140625, 1.0, 1.0, 0.82015625, 0.96109375, 1.0, 1.0, 0.9584375, 0.9815625, 0.96984375, 0.9390625, 0.874375, 0.9425, 0.96875, 0.92984375, 0.9921875, 1.0, 0.85546875, 0.9296875, 0.9515625, 0.79203125, 0.99859375, 0.8921875, 0.9284375, 0.97609375, 0.95453125, 0.996875, 0.99546875, 0.94578125, 1.0, 0.976875, 0.94171875, 0.97359375, 0.961875, 0.99125, 0.8759375, 0.87453125, 1.0, 0.91796875, 0.97796875, 0.95203125, 0.8959375, 1.0, 0.9871875, 1.0, 1.0, 0.99, 0.98125, 0.92140625, 1.0, 0.97421875, 0.9721875, 0.84234375, 0.7421875, 0.8540625, 1.0, 0.9746875, 1.0, 0.96625, 0.98765625, 0.943125, 0.99546875, 0.884375, 1.0, 0.99453125, 0.93734375, 0.96734375, 1.0, 0.944375, 1.0, 0.99171875, 0.9715625, 0.90296875, 0.9915625, 0.98671875, 0.999375, 1.0, 1.0, 0.98828125, 1.0, 0.88546875, 0.92515625, 1.0, 0.94734375, 0.9471875, 0.9859375, 0.95890625, 0.9446875, 1.0, 0.98890625, 0.97625, 1.0, 0.97890625, 0.844375, 0.8790625, 1.0, 0.9790625, 0.98734375, 0.880625, 0.94015625, 1.0, 0.9221875, 1.0, 0.88640625, 0.97765625, 1.0, 0.7965625, 0.95625, 0.9540625, 0.93359375, 0.97328125, 1.0, 0.92625, 1.0, 1.0, 1.0, 1.0, 0.97984375, 1.0, 1.0, 0.9571875, 0.97515625, 0.803125, 0.90234375, 0.9428125, 0.9425, 1.0, 1.0, 0.95203125, 0.97734375, 1.0, 1.0, 0.85765625, 1.0, 0.9825, 0.97640625, 0.90703125, 1.0, 0.85453125, 1.0, 0.979375, 0.9, 0.95078125, 0.991875, 0.955625, 0.90046875, 0.98234375, 0.951875, 0.98859375, 0.99921875, 0.8875, 0.8140625, 0.8496875, 0.99796875, 0.93015625, 0.96875, 0.9740625, 0.98765625, 0.83890625, 1.0, 0.8471875, 0.99046875, 1.0, 0.98984375, 0.96359375, 0.87640625, 0.93984375, 0.82953125, 0.98640625, 0.9834375, 0.97421875, 0.98765625, 0.97140625, 0.98765625, 0.86359375, 1.0, 0.98890625, 0.9525, 0.96171875, 0.89234375, 0.95421875, 0.8840625, 0.98484375, 0.99703125, 0.9803125, 0.98859375, 0.916875, 0.9909375, 1.0, 0.92609375, 0.99625, 0.88890625, 0.65109375, 1.0, 0.95, 0.8646875, 0.8975, 0.964375, 0.93640625, 0.95921875, 0.9575, 1.0, 0.9571875, 0.9340625, 0.981875, 0.8765625, 0.965625, 0.88390625, 1.0, 0.8909375, 0.95546875, 0.9775, 0.99796875, 1.0, 0.95140625, 0.975625, 0.9696875, 0.88265625, 0.85953125, 1.0, 1.0, 0.93546875, 0.89578125, 0.9828125, 0.9775, 0.9490625, 1.0, 0.8434375, 0.953125, 0.9971875, 0.92171875, 0.8759375, 0.82125, 0.91625, 1.0, 0.7721875, 0.83921875, 0.94859375, 0.8534375, 1.0, 0.95078125, 0.965625, 1.0, 0.880625, 1.0, 0.95171875, 0.93484375, 0.954375, 0.93703125, 0.91140625, 0.95859375, 0.95859375, 0.9665625, 1.0, 0.97484375, 0.92078125, 0.90421875, 0.9853125, 1.0, 0.88, 0.96453125, 0.9415625, 0.9771875, 0.9621875, 0.93265625, 0.983125, 0.96234375, 1.0, 1.0, 0.963125, 0.94421875, 1.0, 1.0, 0.94921875, 1.0, 0.91171875, 1.0, 0.92703125, 1.0, 0.9578125, 1.0, 1.0, 1.0, 0.96734375, 0.9890625, 0.94796875, 0.8990625, 1.0, 1.0, 0.978125, 0.9675, 0.98796875, 1.0, 0.9475, 0.97078125, 0.950625, 1.0, 0.96375, 1.0, 0.9471875, 0.99421875, 0.7328125, 1.0, 0.96609375, 0.84265625, 1.0, 0.9015625, 0.9525, 0.9496875, 0.91390625, 0.800625, 0.96453125, 1.0, 1.0, 0.88, 1.0, 0.85390625, 0.97796875, 1.0, 0.9284375, 0.9871875, 1.0, 0.98734375, 1.0, 0.99, 0.72015625, 0.9671875, 0.8121875, 0.99, 0.98359375, 1.0, 0.95546875, 0.9384375, 0.92015625, 0.878125, 0.95453125, 1.0, 0.918125, 0.9575, 1.0, 0.99765625, 1.0, 0.873125, 0.98390625, 0.95234375, 0.9825, 0.95765625, 0.9221875, 1.0, 0.935, 0.97703125, 0.9975, 0.94953125, 0.8965625, 0.99671875, 1.0, 0.95609375, 1.0, 0.9209375, 0.95953125, 0.85328125, 0.98625, 0.89984375, 0.90640625, 0.93890625, 0.96953125, 0.8746875, 0.97984375, 1.0, 0.99765625, 0.90234375, 0.97609375, 0.96921875, 0.988125, 1.0, 0.99, 1.0, 0.9390625, 0.8059375, 0.98671875, 0.90328125, 0.94484375, 1.0, 0.9621875, 0.96671875, 1.0, 1.0, 0.9003125, 0.92390625, 1.0, 0.94625, 0.93140625, 1.0, 0.97421875, 1.0, 0.873125, 0.96671875, 0.9978125, 0.9084375, 0.91390625, 0.98296875, 0.9678125, 0.914375, 0.93203125, 0.9703125, 1.0, 0.98484375, 0.9409375, 0.9790625, 0.9728125, 0.93203125, 0.9553125, 1.0, 0.96375, 0.993125, 0.93390625, 1.0, 0.9884375, 0.8621875, 0.90375, 0.88421875, 0.981875, 0.94046875, 0.865, 0.91953125, 1.0, 1.0, 0.9425, 1.0, 0.87890625, 0.9221875, 1.0, 0.898125, 0.92734375, 0.9478125, 0.9609375, 0.92875, 1.0, 1.0, 1.0, 0.9340625, 0.98765625, 0.89140625, 0.92921875, 0.99015625, 1.0, 0.94171875, 0.97296875, 0.97078125, 0.84109375, 0.97421875, 0.951875, 0.9615625, 0.915625, 0.98125, 1.0, 0.96875, 0.9921875, 0.87140625, 0.92640625, 0.9728125, 0.874375, 0.9975, 0.96109375, 0.96375, 0.90390625, 0.9121875, 0.98953125, 0.96625, 0.9109375, 1.0, 0.94859375, 1.0, 0.92640625, 0.958125, 0.993125, 0.88703125, 0.99625, 0.81390625, 0.9740625, 0.92109375, 0.92328125, 0.95234375, 0.9353125, 1.0, 0.90734375, 0.97453125, 1.0, 1.0, 0.94984375, 0.91515625, 0.9678125, 0.91515625, 0.93171875, 0.94625, 0.9959375, 0.9071875, 0.956875, 0.89640625, 0.99328125, 0.90078125, 0.9671875, 0.9, 0.91359375, 0.96546875, 0.9878125, 1.0, 0.95890625, 1.0, 0.91046875, 1.0, 0.95265625, 0.9784375, 0.8578125, 0.871875, 0.87234375, 0.99953125, 0.96171875, 0.82390625, 0.9421875, 1.0, 0.995, 0.83359375, 1.0, 0.904375, 0.92765625, 0.97609375, 1.0, 1.0, 0.99515625, 0.94015625, 0.96609375, 1.0, 0.95203125, 0.878125, 0.8303125, 0.8903125, 0.885625, 0.95, 1.0, 0.9396875, 0.901875, 0.931875, 1.0, 0.8765625, 1.0, 1.0, 0.90515625, 0.9721875, 0.89671875, 1.0, 0.93109375, 0.9575, 0.93609375, 1.0, 1.0, 1.0, 1.0, 0.88109375, 0.95640625, 0.96734375, 0.92859375, 0.84625, 0.88640625, 0.994375, 0.99734375, 1.0, 1.0, 0.95625, 0.93640625, 0.95984375, 0.97921875, 1.0, 0.885625, 1.0, 0.9946875, 0.99796875, 0.98765625, 1.0, 0.9509375, 1.0, 0.9909375, 1.0, 0.88375, 0.9790625, 1.0, 0.9315625, 0.98671875, 1.0, 0.97734375, 0.97359375, 0.82453125, 0.8271875, 0.9646875, 0.9309375, 1.0, 0.87171875, 0.9590625, 1.0, 0.96296875, 0.88125, 1.0, 1.0, 0.9621875, 0.92640625, 0.9884375, 0.8809375, 1.0, 1.0, 0.881875, 0.9028125, 0.97234375, 0.9740625, 0.9890625, 1.0, 0.91, 1.0, 1.0, 1.0, 1.0, 0.99609375, 0.95953125, 0.89921875, 0.97734375, 0.97734375, 0.99296875, 1.0, 1.0, 1.0, 0.9134375, 0.9990625, 0.99609375, 0.95015625, 0.96390625, 0.99171875, 0.995, 0.9284375, 0.965625, 0.97109375, 0.865, 0.96328125, 0.9665625, 0.971875, 1.0, 0.95, 1.0, 0.9690625, 1.0, 1.0, 1.0, 1.0, 0.951875, 0.83203125, 0.94078125, 0.92890625, 0.97328125, 1.0, 0.9228125, 1.0, 0.9740625, 1.0, 0.99375, 0.9234375, 0.98015625, 0.9790625, 0.88421875, 0.98203125, 0.95765625, 1.0, 0.91578125, 0.98875, 0.94921875, 0.96921875, 0.98859375, 0.83625, 0.90265625, 0.9078125, 0.9146875, 1.0, 0.9684375, 0.96546875, 0.94125, 0.98765625, 0.9990625, 0.9878125, 0.98484375, 0.95828125, 1.0, 0.92046875, 0.97, 1.0, 0.93625, 0.89625, 1.0, 0.9934375, 0.91984375, 0.91390625, 0.90125, 0.90078125, 0.92140625, 0.99265625, 0.8534375, 0.9659375, 0.9959375, 0.98015625, 0.95015625, 0.99359375, 0.93890625, 0.87875, 0.941875, 0.98265625, 0.90953125, 1.0, 0.89875, 0.9640625, 0.973125, 0.9475, 0.94953125, 0.9934375, 0.994375, 0.970625, 0.95390625, 0.93, 0.85296875, 1.0, 0.8790625, 0.99921875, 0.995625, 0.93796875, 0.888125, 1.0, 0.99265625, 0.8925, 0.94921875, 0.99671875, 0.98109375, 1.0, 0.84421875, 0.97515625, 0.93625, 0.96625, 0.93375, 1.0, 0.97328125, 0.87625, 0.98703125, 0.99125, 1.0, 0.9490625, 0.93953125, 0.9303125, 0.97, 0.98265625, 0.98046875, 1.0, 1.0, 0.9025, 1.0, 0.9425, 1.0, 0.95921875, 0.976875, 0.96078125, 0.90046875, 1.0, 0.8975, 1.0, 0.95546875, 0.93609375, 1.0, 0.99703125, 0.9215625, 0.916875, 0.95734375, 0.92, 0.96125, 1.0, 0.9571875, 0.94515625, 0.98375, 0.9409375, 1.0, 0.9875, 0.88640625, 0.9578125, 0.9715625, 1.0, 1.0, 1.0, 0.96703125, 0.989375, 0.875625, 0.9425, 0.91296875, 0.978125, 0.90140625, 1.0, 0.97703125, 0.85265625, 1.0, 0.9471875, 0.9840625, 0.973125, 0.990625, 0.98, 0.9934375, 1.0, 0.9615625, 0.9490625, 0.8190625, 1.0, 0.99546875, 0.92765625, 1.0, 0.97015625, 0.990625, 0.93015625, 1.0, 0.97484375, 1.0, 0.9303125, 0.985625, 0.954375, 0.9334375, 0.9725, 0.89734375, 0.935625, 1.0, 0.9965625, 0.9509375, 0.8778125, 0.9771875, 0.8003125, 0.88484375, 1.0, 0.98390625, 0.941875, 0.96625, 1.0, 1.0, 0.945625, 0.9646875, 0.95421875, 0.968125, 0.99046875, 1.0, 1.0, 0.971875, 0.915, 0.986875, 0.83875, 0.96265625, 0.97953125, 0.96890625, 1.0, 0.7290625, 0.94828125, 0.99953125, 0.996875, 0.97171875, 0.8628125, 1.0, 0.96671875, 0.954375, 0.96828125, 0.960625, 0.9809375, 0.9521875, 1.0, 0.9375, 1.0, 0.946875, 0.98234375, 0.996875, 1.0, 1.0, 1.0, 0.91390625, 0.92515625, 0.9925, 1.0, 0.87609375, 0.99546875, 0.918125, 0.9359375, 1.0, 0.9109375, 0.90921875, 0.925625, 0.92125, 0.851875, 0.986875, 0.9490625, 0.88484375, 1.0, 1.0, 0.9203125, 0.98921875, 1.0, 0.926875, 0.98765625, 0.9740625, 1.0, 0.88234375, 0.9671875, 0.9846875, 0.95421875, 1.0, 1.0, 0.96953125, 0.9634375, 0.84765625, 0.89703125, 1.0, 0.98453125, 0.9525, 0.9678125, 0.959375, 1.0, 0.94578125, 0.9903125, 0.97734375, 0.909375, 1.0, 1.0, 0.9103125, 0.9628125, 0.96875, 0.97078125, 1.0, 0.939375, 0.9784375, 0.82375, 0.98328125, 0.97296875, 0.9515625, 1.0, 0.97859375, 0.90734375, 0.98828125, 0.91984375, 0.905625, 1.0, 0.98703125, 1.0, 0.975625, 1.0, 0.96515625, 1.0, 0.94578125, 0.91859375, 0.880625, 0.95984375, 0.9571875, 0.98796875, 0.97890625, 0.92625, 1.0, 0.96734375, 0.9378125, 0.93515625, 0.9903125, 0.9671875, 1.0, 0.96015625, 0.84671875, 0.945625, 0.9821875, 1.0, 0.965625, 0.94703125, 1.0, 0.96375, 0.96875, 0.72296875, 0.97265625, 0.9959375, 0.9490625, 1.0, 0.98015625, 1.0, 1.0, 1.0, 1.0, 0.93578125, 0.85140625, 0.9059375, 0.95015625, 1.0, 0.928125, 0.98890625, 1.0, 1.0, 0.91296875, 0.85, 1.0, 0.986875, 0.86203125, 0.88140625, 0.901875, 1.0, 1.0, 0.97453125, 1.0, 0.899375, 0.85328125, 1.0, 1.0, 0.98375, 0.998125, 0.97921875, 0.9684375, 0.9971875, 0.97171875, 0.90203125, 0.8196875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9396875, 0.97546875, 1.0, 0.97359375, 0.94671875, 1.0, 0.95359375, 0.9325, 0.915625, 0.926875, 0.8378125, 0.96734375, 1.0, 0.98609375, 0.929375, 0.871875, 0.96625, 0.91890625, 0.87046875, 0.96421875, 1.0, 0.919375, 1.0, 0.96046875, 0.9821875, 0.9253125, 0.96875, 0.98109375, 0.8490625, 0.9821875, 1.0, 0.974375, 0.97421875, 0.955625, 1.0, 0.83359375, 0.9803125, 0.956875, 0.9415625, 0.87609375, 0.9565625, 0.9475, 0.921875, 0.99359375, 1.0, 0.9975, 0.9409375, 1.0, 0.9746875, 0.964375, 0.97359375, 0.94328125, 0.9215625, 0.93234375, 0.9521875, 0.94421875, 0.8953125, 0.976875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94359375, 0.89953125, 0.9928125, 0.91125, 1.0, 0.9940625, 0.9784375, 0.99359375, 0.99125, 1.0, 0.99578125, 0.90140625, 0.99015625, 1.0, 0.9765625, 0.86765625, 0.95671875, 1.0, 0.96375, 1.0, 0.95421875, 1.0, 0.999375, 0.91953125, 0.91453125, 0.96546875, 0.8721875, 0.96515625, 0.859375, 0.92859375, 0.86984375, 1.0, 0.9690625, 0.99046875, 1.0, 0.80828125, 0.99078125, 0.9721875, 0.9953125, 1.0, 1.0, 0.951875, 0.88296875, 0.91890625, 0.89765625, 0.89390625, 0.966875, 0.84140625, 1.0, 0.9359375, 0.96296875, 1.0, 0.8903125, 1.0, 0.9528125, 1.0, 1.0, 0.94640625, 0.96015625, 0.96890625, 0.99796875, 1.0, 0.916875, 0.95390625, 0.96375, 1.0, 1.0, 0.9725, 1.0, 1.0, 0.98734375, 0.985, 0.93515625, 1.0, 1.0, 0.99078125, 0.96984375, 1.0, 0.88296875, 1.0, 1.0, 0.88671875, 0.98046875, 0.99890625, 0.99140625, 1.0, 0.921875, 1.0, 0.995625, 0.79015625, 1.0, 1.0, 1.0, 0.995625, 0.9071875, 1.0, 0.96078125, 0.98234375, 0.97484375, 1.0, 0.99125, 0.9246875, 0.9346875, 0.96734375, 1.0, 0.92234375, 1.0, 1.0, 0.9821875, 0.964375, 0.9415625, 0.92421875, 0.87484375, 0.953125, 1.0, 0.91015625, 0.99640625, 0.9978125, 0.8790625, 1.0, 1.0, 1.0, 0.8878125, 0.9546875, 0.999375, 0.9228125, 0.81875, 0.984375, 0.96734375, 0.9521875, 0.95078125, 1.0, 0.97140625, 0.91484375, 1.0, 0.92171875, 0.97578125, 0.974375, 0.9571875, 0.96046875, 0.878125, 1.0, 0.8734375, 1.0, 1.0, 0.99625, 0.9403125, 0.811875, 0.98359375, 1.0, 0.9646875, 0.98359375, 0.885, 0.85515625, 0.94765625, 0.96734375, 1.0, 0.97125, 0.904375, 1.0, 0.97109375, 0.93453125, 1.0, 0.9309375, 0.99453125, 0.77390625, 0.85265625, 0.991875, 0.88015625, 1.0, 0.99078125, 0.9184375, 0.9884375, 0.9190625, 0.93890625, 1.0, 0.95171875, 0.983125, 0.966875, 0.9075, 0.99046875, 1.0, 0.980625, 1.0, 0.92171875, 0.896875, 0.916875, 0.88515625, 1.0, 0.94796875, 0.91203125, 1.0, 1.0, 1.0, 0.87828125, 0.88390625, 1.0, 1.0, 1.0, 0.92640625, 1.0, 0.994375, 0.81453125, 0.78484375, 1.0, 1.0, 0.993125, 0.95859375, 1.0, 1.0, 0.94296875, 1.0, 0.95734375, 0.9996875, 0.97359375, 0.8771875, 0.96171875, 0.97625, 0.97640625, 1.0, 0.9965625, 0.985, 0.95671875, 1.0, 1.0, 0.94734375, 0.9665625, 0.9859375, 1.0, 0.91140625, 1.0, 0.855625, 0.92859375, 1.0, 0.97296875, 0.906875, 0.99671875, 1.0, 0.96640625, 0.86625, 0.97484375, 0.9284375, 0.95015625, 0.99375, 0.9696875, 0.933125, 0.95015625, 0.9740625, 1.0, 0.97421875, 0.904375, 0.88109375, 0.9603125, 0.86109375, 0.79140625, 0.90421875, 0.9025, 0.97390625, 1.0, 0.89234375, 0.9734375, 0.95, 1.0, 0.985625, 0.94484375, 0.98296875, 0.93109375, 0.866875, 0.9934375, 0.900625, 0.97296875, 1.0, 0.9609375, 0.98546875, 0.9875, 1.0, 0.96765625, 0.9784375, 0.84390625, 0.955, 0.98359375, 0.9203125, 1.0, 0.9353125, 1.0, 1.0, 0.83390625, 0.96265625, 1.0, 0.9415625, 0.9825, 1.0, 0.97171875, 1.0, 0.95578125, 0.95984375, 0.99421875, 0.9453125, 1.0, 1.0, 0.83765625, 0.9640625, 0.97515625, 0.95796875, 0.98046875, 0.98609375, 0.9725, 0.8515625, 1.0, 0.96609375, 1.0, 1.0, 0.9940625, 0.8675, 0.9209375, 0.9803125, 0.99890625, 0.9884375, 0.87171875, 0.9671875, 0.95734375, 1.0, 0.9771875, 1.0, 0.95171875, 0.9546875, 0.91453125, 0.953125, 1.0, 1.0, 0.97375, 0.9771875, 0.90796875, 0.86109375, 0.99578125, 1.0, 0.94953125, 0.89390625, 1.0, 0.96578125, 0.96953125, 1.0, 0.99, 0.78609375, 0.94234375, 0.92125, 0.98109375, 0.94734375, 1.0, 0.971875, 0.86578125, 0.97984375, 0.94453125, 0.98, 0.90328125, 0.87609375, 0.9865625, 1.0, 1.0, 0.98859375, 0.9025, 1.0, 0.89109375, 0.99421875, 0.93390625, 1.0, 0.863125, 0.99171875, 0.9790625, 0.9465625, 0.963125, 1.0, 0.99390625, 1.0, 0.94453125, 1.0, 0.97703125, 0.99859375, 1.0, 0.909375, 0.968125, 0.96328125, 0.89984375, 0.9665625, 0.9825, 0.85671875, 0.98515625, 0.97015625, 0.93609375, 0.89421875, 0.9978125, 0.89, 0.936875, 0.99359375, 1.0, 0.92890625, 0.86, 0.9628125, 0.89984375, 0.9996875, 0.9446875, 0.90671875, 0.94953125, 0.9978125, 0.99, 0.97125, 0.9840625, 1.0, 1.0, 0.99609375, 1.0, 0.92578125, 1.0, 1.0, 0.9103125, 0.98234375, 0.77609375, 0.96234375, 0.97765625, 0.87265625, 0.90796875, 0.9134375, 0.9684375, 1.0, 0.878125, 0.87265625, 0.9421875, 0.998125, 0.98921875, 1.0, 0.94796875, 0.93, 1.0, 0.860625, 0.95015625, 0.9, 1.0, 0.9503125, 1.0, 1.0, 1.0, 0.9996875, 0.9565625, 0.92953125, 0.9796875, 0.90390625, 0.9896875, 0.9690625, 0.9778125, 1.0, 0.99421875, 0.9571875, 0.86, 0.97046875, 0.888125, 0.948125, 0.91734375, 0.8596875, 1.0, 1.0, 1.0, 1.0, 0.90171875, 0.8575, 0.99140625, 0.94453125, 0.9884375, 0.95640625, 0.91703125, 1.0, 0.92703125, 0.9328125, 1.0, 0.7253125, 0.95140625, 0.97828125, 0.96421875, 0.97234375, 0.92109375, 0.93546875, 1.0, 0.9584375, 0.99328125, 0.91125, 0.97015625, 0.93125, 1.0, 1.0, 0.99765625, 0.990625, 0.94125, 1.0, 1.0, 1.0, 0.96109375, 0.891875, 0.8571875, 0.90671875, 0.99015625, 0.92953125, 0.9784375, 1.0, 0.9728125, 0.8934375, 0.8671875, 0.96921875, 0.96828125, 0.95703125, 0.8003125, 0.9625, 0.986875, 0.93109375, 0.91, 0.9690625, 0.98109375, 0.99140625, 0.91359375, 1.0, 0.94875, 0.873125, 1.0, 0.99671875, 0.9696875, 0.9371875, 0.98328125, 1.0, 0.79328125, 0.96296875, 0.99484375, 0.86734375, 0.97140625, 0.96171875, 0.89765625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9846875, 0.993125, 1.0, 0.9478125, 0.98484375, 1.0, 0.87921875, 0.83953125, 1.0, 0.90921875, 0.9878125, 1.0, 0.97921875, 1.0, 0.9965625, 0.98453125, 0.92734375, 0.8921875, 0.85796875, 0.9715625, 0.97203125, 0.97296875, 0.826875, 0.94671875, 0.99328125, 0.99671875, 1.0, 0.9153125, 1.0, 1.0, 0.9259375, 0.905, 1.0, 0.964375, 0.9775, 1.0, 0.906875, 1.0, 0.9771875, 1.0, 0.98921875, 1.0, 0.92984375, 0.93109375, 0.984375, 0.89859375, 1.0, 0.98578125, 0.9578125, 0.983125, 0.91484375, 0.86703125, 0.9659375, 0.9575, 0.95984375, 0.93171875, 0.945, 1.0, 0.9790625, 1.0, 1.0, 0.99875, 0.90953125, 0.9978125, 0.97515625, 0.99484375, 0.96703125, 0.975625, 0.93796875, 0.981875, 0.9928125, 0.89265625, 0.86328125, 0.96546875, 0.95359375, 0.96015625, 0.94859375, 0.98234375, 0.9540625, 0.980625, 1.0, 0.95640625, 0.95671875, 0.89890625, 0.89875, 0.936875, 0.93171875, 1.0, 0.9196875, 0.99171875, 0.9484375, 0.79953125, 0.87328125, 0.99796875, 0.9740625, 0.9134375, 0.91796875, 0.841875, 1.0, 1.0, 0.9203125, 0.90640625, 0.86984375, 0.9909375, 0.99328125, 1.0, 0.965, 0.92984375, 0.979375, 0.99546875, 1.0, 1.0, 0.985, 1.0, 0.8609375, 0.9859375, 0.95390625, 0.9425, 0.97015625, 1.0, 0.99515625, 0.988125, 0.8746875, 0.84984375, 0.9371875, 0.98625, 0.86484375, 1.0, 0.9978125, 0.8159375, 0.845, 1.0, 0.958125, 1.0, 0.9840625, 0.99671875, 0.95734375, 0.949375, 0.8409375, 0.89734375, 0.891875, 0.95828125, 0.83703125, 0.9103125, 1.0, 1.0, 1.0, 0.9146875, 0.89359375, 1.0, 0.95203125, 0.8628125, 0.98390625, 1.0, 0.9090625, 1.0, 0.94859375, 0.97015625, 0.98, 0.96265625, 0.93859375, 0.9234375, 0.9059375, 0.865, 0.93375, 1.0, 1.0, 0.94953125, 1.0, 0.8628125, 0.95078125, 0.93484375, 1.0, 0.99875, 0.9528125, 0.9578125, 1.0, 0.96734375, 1.0, 1.0, 0.9875, 1.0, 0.969375, 0.98078125, 0.99734375, 0.878125, 0.98515625, 0.973125, 0.96125, 0.9625, 0.97, 0.90203125, 1.0, 0.9496875, 0.96765625, 1.0, 1.0, 1.0, 0.98796875, 0.8915625, 0.92875, 0.9853125, 0.9684375, 1.0, 0.91578125, 0.9796875, 1.0, 0.99609375, 0.99703125, 0.9890625, 0.9571875, 0.94796875, 0.9259375, 0.986875, 1.0, 0.96078125, 1.0, 0.90125, 0.92953125, 1.0, 0.94828125, 0.961875, 0.96953125, 0.9946875, 1.0, 1.0, 0.84734375, 0.995625, 1.0, 0.9653125, 0.998125, 0.99828125, 0.99578125, 1.0, 1.0, 0.9784375, 0.98671875, 0.90578125, 0.82640625, 0.990625, 0.946875, 1.0, 1.0, 0.9896875, 1.0, 0.88140625, 0.97890625, 0.90375, 0.93890625, 1.0, 1.0, 1.0, 1.0, 0.90515625, 0.9490625, 0.97921875, 0.988125, 1.0, 0.849375, 0.9365625, 1.0, 0.94625, 1.0, 0.97890625, 0.978125, 0.90765625, 0.875625, 0.898125, 0.98015625, 1.0, 0.96953125, 0.91734375, 0.8390625, 1.0, 0.96859375, 1.0, 1.0, 0.9628125, 0.97359375, 1.0, 1.0, 0.99390625, 1.0, 0.9121875, 1.0, 0.886875, 0.96765625, 0.88390625, 0.94484375, 1.0, 0.8996875, 1.0, 0.9875, 0.9871875, 0.97859375, 1.0, 0.96375, 0.90765625, 0.8725, 1.0, 0.883125, 0.98546875, 0.95515625, 0.9803125, 0.91984375, 0.97953125, 0.95546875, 0.96546875, 0.81859375, 0.93125, 0.9815625, 0.98625, 0.875625, 0.91953125, 1.0, 0.98484375, 0.97703125, 0.96640625, 0.9371875, 0.9784375, 0.87421875, 0.965, 0.96546875, 0.9940625, 0.996875, 0.97015625, 0.9875, 0.959375, 0.9809375, 1.0, 1.0, 1.0, 0.99421875, 0.9865625, 0.84609375, 0.84640625, 0.9171875, 0.88109375, 1.0, 0.9690625, 1.0, 0.92734375, 0.98375, 1.0, 0.8871875, 0.97953125, 0.92015625, 0.989375, 0.9440625, 0.8934375, 1.0, 0.95828125, 0.8496875, 0.9734375, 0.92359375, 0.953125, 1.0, 0.99453125, 0.999375, 0.97, 0.90375, 1.0, 0.95546875, 0.97828125, 0.9096875, 0.8675, 0.98125, 0.9209375, 1.0, 1.0, 0.92578125, 0.94515625, 0.86625, 0.92453125, 1.0, 0.99265625, 0.99265625, 0.9775, 0.925, 0.9740625, 0.96953125, 0.97140625, 0.8475, 0.84125, 0.95359375, 0.97421875, 0.9784375, 1.0, 0.96859375, 0.9803125, 0.9515625, 0.92578125, 0.9659375, 0.91953125, 0.98171875, 0.934375, 1.0, 1.0, 1.0, 1.0, 0.85203125, 1.0, 0.89921875, 0.94734375, 0.9846875, 0.90046875, 1.0, 0.9628125, 0.9696875, 0.9946875, 1.0, 0.9215625, 1.0, 0.9853125, 1.0, 0.95671875, 0.97796875, 0.91625, 0.955625, 1.0, 1.0, 0.96765625, 0.92578125, 1.0, 0.99953125, 0.91453125, 0.90328125, 0.90984375, 0.99296875, 0.87984375, 0.9815625, 0.9878125, 0.94328125, 0.9715625, 0.99546875, 0.90671875, 0.93328125, 1.0, 0.96265625, 0.99765625, 1.0, 0.99890625, 0.99375, 0.989375, 0.98765625, 1.0, 0.983125, 0.999375, 0.9771875, 1.0, 1.0, 0.85640625, 0.9803125, 1.0, 0.93703125, 0.94, 0.96390625, 1.0, 0.99078125, 1.0, 0.89328125, 0.868125, 1.0, 1.0, 0.98453125, 1.0, 0.98734375, 0.955, 0.95671875, 0.965625, 0.91453125, 0.8721875, 0.8984375, 0.93390625, 0.98453125, 0.98953125, 0.9228125, 0.91953125, 1.0, 0.97515625, 0.99, 1.0, 0.975625, 1.0, 0.8946875, 0.95140625, 1.0, 0.9996875, 0.860625, 0.995, 0.945625, 0.82140625, 0.94875, 0.98578125, 0.91234375, 0.96078125, 0.99, 1.0, 0.87484375, 0.9878125, 0.896875, 0.99359375, 1.0, 0.939375, 0.92125, 1.0, 0.88734375, 0.94421875, 0.968125, 0.9746875, 0.96390625, 0.97078125, 0.96703125, 0.94265625, 0.94515625, 1.0, 0.9309375, 0.89984375, 0.82640625, 1.0, 1.0, 0.91, 0.98484375, 1.0, 0.923125, 0.910625, 0.96609375, 1.0, 0.94953125, 0.94875, 0.83953125, 0.97828125, 0.99375, 1.0, 0.96296875, 1.0, 0.9546875, 0.89734375, 0.9615625, 1.0, 1.0, 0.98703125, 1.0, 0.96484375, 0.97140625, 0.924375, 0.9784375, 0.95578125, 0.894375, 1.0, 1.0, 0.94484375, 1.0, 0.8821875, 0.70890625, 0.98171875, 0.8153125, 0.90265625, 1.0, 1.0, 0.90828125, 0.9553125, 0.951875, 0.87875, 0.89734375, 1.0, 0.92375, 1.0, 1.0, 0.99359375, 1.0, 1.0, 0.9715625, 1.0, 0.94109375, 1.0, 0.865, 1.0, 0.88734375, 0.9240625, 0.98125, 0.96484375, 0.8890625, 0.95484375, 0.96015625, 0.97828125, 0.9825, 0.9765625, 0.97625, 0.91640625, 0.86984375, 0.95890625, 1.0, 1.0, 0.99828125, 1.0, 0.9409375, 1.0, 0.99765625, 0.92609375, 0.80953125, 1.0, 0.98703125, 1.0, 0.96390625, 1.0, 1.0, 0.9546875, 0.90578125, 0.9621875, 0.98125, 1.0, 0.99171875, 0.97546875, 0.985625, 0.835, 1.0, 0.8678125, 0.96765625, 0.9903125, 1.0, 0.99453125, 0.953125, 0.98453125, 0.96421875, 0.9340625, 0.9846875, 0.963125, 0.96390625, 1.0, 0.983125, 0.87375, 1.0, 0.8678125, 0.9571875, 0.898125, 0.95203125, 0.97328125, 0.9325, 0.9178125, 1.0, 0.95609375, 0.93140625, 0.99640625, 0.95875, 0.96609375, 0.87328125, 1.0, 1.0, 0.96734375, 0.946875, 1.0, 0.9528125, 0.988125, 0.906875, 0.98125, 0.9659375, 0.96890625, 0.9490625, 0.8678125, 1.0, 0.79828125, 0.94140625, 0.950625, 1.0, 0.9775, 0.9753125, 1.0, 0.97484375, 0.9846875, 0.95734375, 0.98578125, 0.9925, 1.0, 1.0, 0.911875, 1.0, 0.975625, 0.91484375, 0.95609375, 0.96015625, 0.8984375, 0.94828125, 0.89203125, 0.9728125, 1.0, 0.89859375, 1.0, 0.99625, 0.9721875, 0.9346875, 0.9615625, 0.95859375, 0.86453125, 0.9503125, 0.99078125, 0.9928125, 0.9971875, 0.9828125, 0.9046875, 0.988125, 0.91, 0.91484375, 1.0, 0.9653125, 0.973125, 0.99640625, 1.0, 0.96734375, 0.95125, 0.91609375, 0.83109375, 0.88546875, 0.9434375, 1.0, 0.99140625, 0.97796875, 0.9521875, 0.91953125, 0.99359375, 0.946875, 1.0, 0.98828125, 0.92421875, 1.0, 0.99984375, 0.92609375, 1.0, 0.9215625, 0.95359375, 0.953125, 1.0, 0.90625, 0.9728125, 0.97140625, 1.0, 0.874375, 0.949375, 1.0, 0.94796875, 0.8453125, 0.94265625, 0.995625, 1.0, 0.969375, 0.97125, 0.986875, 0.8915625, 0.95890625, 0.84125, 0.920625, 0.9834375, 0.93953125, 0.995625, 0.9034375, 0.9590625, 0.82546875, 0.9365625, 0.93625, 0.89640625, 0.95078125, 1.0, 0.965, 0.956875, 1.0, 0.9971875, 0.89296875, 0.9225, 1.0, 1.0, 0.98859375, 0.976875, 0.9921875, 0.95265625, 0.97078125, 0.90546875, 1.0, 1.0, 0.9415625, 1.0, 0.95375, 0.9671875, 0.98984375, 0.99484375, 1.0, 0.9921875, 0.92953125, 1.0, 0.810625, 1.0, 0.94796875, 0.95171875, 0.90140625, 0.9946875, 0.9065625, 0.96140625, 0.90859375, 0.9628125, 0.90140625, 0.89609375, 0.97109375, 0.82015625, 0.97, 1.0, 0.97765625, 0.98515625, 0.86640625, 0.879375, 0.9284375, 0.95625, 0.954375, 0.94375, 1.0, 0.9334375, 0.97546875, 0.9940625, 0.9153125, 0.85265625, 0.99796875, 0.9490625, 0.9646875, 0.9184375, 0.99234375, 0.9984375, 0.95765625, 0.88140625, 0.9978125, 0.91984375, 0.95625, 0.95875, 0.96796875, 0.9409375, 0.856875, 0.97234375, 0.96796875, 1.0, 0.92484375, 0.815625, 0.9046875, 1.0, 0.98078125, 0.89390625, 0.97015625, 0.9025, 0.92625, 0.95234375, 0.98015625, 0.8209375, 0.9759375, 0.99796875, 0.9646875, 0.9815625, 0.98875, 0.9734375, 0.84515625, 0.92984375, 0.97875, 0.78390625, 0.99078125, 0.98078125, 0.85453125, 0.93765625, 0.99234375, 0.92640625, 0.87203125, 0.9921875, 1.0, 0.968125, 0.9840625, 0.93, 0.905, 1.0, 0.98640625, 1.0, 0.96765625, 0.97734375, 0.9503125, 0.98296875, 0.96921875, 0.98515625, 0.915, 0.9828125, 0.86890625, 1.0, 0.99015625, 0.92359375, 1.0, 0.92921875, 0.94828125, 0.99390625, 0.93765625, 0.89234375, 0.9490625, 1.0, 0.87203125, 0.899375, 0.9346875, 1.0, 0.8128125, 0.94859375, 0.991875, 0.915625, 1.0, 0.981875, 0.96359375, 0.8809375, 1.0, 0.9903125, 0.9675, 0.9946875, 0.92625, 0.971875, 0.9725, 0.93015625, 1.0, 0.95515625, 1.0, 0.99265625, 1.0, 0.98734375, 0.936875, 1.0, 0.98296875, 0.954375, 1.0, 0.94359375, 0.96578125, 0.93765625, 0.8996875, 1.0, 0.97015625, 0.96375, 1.0, 1.0, 0.96125, 0.95171875, 0.9240625, 0.961875, 0.9403125, 0.9478125, 0.9865625, 1.0, 0.92859375, 1.0, 1.0, 1.0, 0.93625, 0.91359375, 1.0, 1.0, 0.883125, 0.97953125, 1.0, 1.0, 0.90921875, 0.89828125, 0.9790625, 0.96421875, 0.9509375, 0.96625, 0.83765625, 1.0, 0.98734375, 0.864375, 0.9290625, 0.92125, 1.0, 0.9775, 0.94875, 0.9871875, 0.96890625, 0.92171875, 1.0, 0.98765625, 0.98359375, 1.0, 0.925625, 0.928125, 0.93515625, 0.889375, 0.95609375, 0.9671875, 0.96921875, 0.98578125, 0.98234375, 0.9146875, 0.98140625, 0.95265625, 1.0, 0.9640625, 0.9609375, 0.98921875, 0.980625, 1.0, 1.0, 1.0, 1.0, 0.95359375, 0.9540625, 0.99421875, 0.98296875, 1.0, 1.0, 1.0, 0.94546875, 0.95203125, 0.8390625, 0.90203125, 1.0, 0.93515625, 0.998125, 1.0, 0.90609375, 0.79875, 0.92921875, 1.0, 0.9859375, 0.81953125, 0.9465625, 0.88890625, 0.97421875, 0.97234375, 0.94359375, 0.915, 0.96046875, 1.0, 0.965, 1.0, 0.998125, 1.0, 0.96328125, 0.87421875, 0.91828125, 0.99125, 0.9146875, 0.92765625, 0.91, 0.8384375, 0.9521875, 1.0, 0.795, 0.995, 0.96109375, 1.0, 1.0, 0.93859375, 0.971875, 0.88328125, 0.978125, 0.9115625, 0.96140625, 0.94703125, 0.99578125, 0.99171875, 0.8678125, 0.8896875, 0.9903125, 0.9584375, 0.96875, 1.0, 0.9690625, 0.92609375, 0.87984375, 0.97171875, 1.0, 0.96953125, 0.8846875, 0.99078125, 0.97984375, 1.0, 0.91234375, 0.86734375, 0.88890625, 0.969375, 0.999375, 0.87171875, 1.0, 1.0, 0.93546875, 0.98125, 0.95671875, 0.90046875, 1.0, 0.97453125, 0.9615625, 0.99671875, 0.99578125, 0.99078125, 0.9553125, 0.8953125, 0.95953125, 0.96296875, 0.9590625, 1.0, 0.9659375, 0.9121875, 0.9803125, 0.92984375, 0.99265625, 1.0, 0.93125, 0.7890625, 0.89609375, 0.9946875, 0.8903125, 0.99203125, 0.9896875, 0.98359375, 0.97375, 0.9409375, 0.93765625, 1.0, 0.948125, 0.9259375, 0.94421875, 0.94234375, 0.96828125, 0.9884375, 1.0, 1.0, 1.0, 0.985, 0.8846875, 0.92265625, 0.92734375, 1.0, 0.90515625, 0.96546875, 0.9634375, 1.0, 0.92046875, 0.92921875, 0.91546875, 1.0, 0.9715625, 0.85328125, 0.9065625, 0.99265625, 1.0, 0.9009375, 0.9725, 0.9440625, 1.0, 0.9959375, 0.96171875, 1.0, 0.938125, 0.95734375, 0.89390625, 0.95078125, 1.0, 0.88421875, 0.93953125, 0.95921875, 0.98140625, 1.0, 0.96125, 0.9325, 0.98359375, 0.94640625, 1.0, 0.82203125, 0.9365625, 0.96859375, 0.97140625, 1.0, 1.0, 0.9240625, 0.973125, 0.9259375, 0.9609375, 0.95640625, 0.91828125, 0.930625, 0.936875, 0.98828125, 0.91390625, 1.0, 1.0, 0.97328125, 0.99390625, 0.9659375, 1.0, 0.97609375, 0.80640625, 0.99609375, 0.9396875, 0.96953125, 0.99546875, 0.92140625, 1.0, 0.8153125, 0.9509375, 1.0, 1.0, 0.9440625, 0.99296875, 1.0, 0.97390625, 0.82921875, 0.98359375, 0.90859375, 0.99171875, 0.90015625, 0.94515625, 0.92546875, 0.9909375, 0.92671875, 0.99171875, 1.0, 0.9190625, 1.0, 0.99, 0.95125, 1.0, 0.8921875, 0.853125, 0.92265625, 0.8659375, 0.9396875, 0.99578125, 0.995, 1.0, 0.91375, 1.0, 0.993125, 0.91234375, 0.9725, 0.9659375, 1.0, 0.9421875, 0.96328125, 0.999375, 0.8865625, 0.90125, 0.96140625, 0.9803125, 0.97640625, 0.97953125, 0.9496875, 0.9703125, 0.97734375, 0.9209375, 0.9903125, 0.93796875, 0.96859375, 0.98359375, 0.9578125, 0.966875, 0.93109375, 0.9828125, 0.9834375, 1.0, 0.8346875, 1.0, 0.978125, 0.95953125, 0.9271875, 0.94765625, 1.0, 1.0, 1.0, 0.9490625, 0.97140625, 0.873125, 1.0, 0.9465625, 0.9803125, 1.0, 0.97828125, 0.88484375, 1.0, 1.0, 0.96671875, 1.0, 1.0, 1.0, 1.0, 0.9615625, 0.95546875, 0.9996875, 0.94265625, 0.93703125, 0.9453125, 0.98328125, 0.9725, 1.0, 1.0, 0.97828125, 0.996875, 0.95984375, 0.9728125, 0.9709375, 0.9334375, 0.96609375, 0.9509375, 1.0, 0.99421875, 0.961875, 0.969375, 0.99296875, 0.948125, 0.9159375, 1.0, 0.9484375, 0.92328125, 1.0, 1.0, 0.97234375, 1.0, 1.0, 0.99390625, 0.98984375, 0.98875, 0.973125, 0.95890625, 0.9465625, 0.9046875, 0.83625, 0.970625, 1.0, 0.9790625, 0.98375, 0.90515625, 0.91375, 1.0, 0.9609375, 0.98703125, 0.9921875, 1.0, 0.88890625, 0.99140625, 0.98546875, 1.0, 0.96375, 0.86703125, 1.0, 0.96, 0.92296875, 0.94359375, 0.9615625, 1.0, 0.95359375, 0.938125, 1.0, 0.92078125, 0.88296875, 1.0, 0.83703125, 0.97640625, 0.853125, 1.0, 1.0, 0.984375, 1.0, 0.9534375, 1.0, 0.931875, 0.93796875, 0.95671875, 0.93609375, 0.88109375, 0.8853125, 1.0, 0.93453125, 0.86015625, 0.96640625, 0.96796875, 1.0, 0.9984375, 0.94796875, 0.89484375, 0.9390625, 0.97578125, 0.87609375, 1.0, 1.0, 1.0, 0.8553125, 1.0, 0.93484375, 0.97859375, 1.0, 0.9375, 0.97046875, 0.94234375, 0.9725, 0.9959375, 0.96171875, 0.91609375, 0.91890625, 0.8990625, 1.0, 0.9290625, 1.0, 0.9865625, 0.95921875, 0.8934375, 0.92703125, 0.89546875, 0.96546875, 0.98421875, 0.9446875, 0.99125, 0.9228125, 0.925, 0.9746875, 0.87921875, 0.9690625, 0.968125, 0.94015625, 0.99125, 1.0, 0.96046875, 0.93859375, 0.873125, 0.939375, 1.0, 1.0, 0.99890625, 1.0, 0.9953125, 1.0, 0.99625, 0.9753125, 0.9009375, 0.9753125, 0.87765625, 1.0, 0.8846875, 0.87421875, 0.949375, 0.90359375, 0.95171875, 0.85171875, 0.995625, 0.89078125, 0.970625, 0.99109375, 0.834375, 0.94234375, 0.97359375, 0.97203125, 0.9928125, 0.97125, 1.0, 0.990625, 0.9109375, 0.9825, 1.0, 1.0, 0.85859375, 0.96265625, 0.91140625, 0.96671875, 0.9546875, 0.83671875, 0.91828125, 1.0, 0.9540625, 0.97296875, 0.985625, 0.9284375, 0.991875, 0.990625, 0.98171875, 0.95375, 0.998125, 1.0, 0.920625, 0.89328125, 0.96046875, 0.97109375, 0.99953125, 0.97109375, 1.0, 0.9721875, 0.98296875, 0.97609375, 0.8671875, 0.91515625, 1.0, 0.9171875, 0.884375, 0.945, 0.9125, 0.885, 0.9909375, 0.95515625, 0.94609375, 0.96484375, 1.0, 0.99265625, 0.996875, 0.941875, 1.0, 0.95890625, 0.8115625, 0.949375, 1.0, 0.999375, 0.9684375, 0.99234375, 0.9403125, 0.93421875, 0.96703125, 0.9834375, 0.95328125, 0.98625, 0.9815625, 0.89625, 0.88875, 1.0, 0.98484375, 0.99515625, 0.99171875, 0.9971875, 1.0, 0.93875, 0.98859375, 0.95625, 0.95140625, 0.85921875, 0.96609375, 0.95234375, 1.0, 0.84890625, 1.0, 1.0, 0.92015625, 0.92078125, 0.92359375, 0.99125, 0.985625, 0.96296875, 0.9865625, 0.9403125, 0.9703125, 0.895, 0.95171875, 0.86578125, 0.955625, 0.96609375, 0.93625, 1.0, 0.93578125, 1.0, 1.0, 0.98078125, 0.9315625, 0.9553125, 0.95390625, 0.90375, 0.98265625, 0.98078125, 0.97328125, 0.91796875, 1.0, 0.93859375, 1.0, 0.97109375, 1.0, 0.94671875, 1.0, 0.97734375, 1.0, 1.0, 0.95234375, 0.87765625, 1.0, 0.9009375, 1.0, 0.97265625, 1.0, 0.9859375, 0.99328125, 0.971875, 0.9096875, 0.8975, 0.99109375, 1.0, 0.85875, 0.99765625, 0.9846875, 0.99421875, 0.94078125, 0.85953125, 0.915, 0.879375, 0.999375, 0.9553125, 0.86765625, 0.90671875, 0.98015625, 0.91828125, 0.96359375, 0.94625, 0.87328125, 1.0, 0.82484375, 0.90078125, 0.9309375, 0.87578125, 0.89140625, 0.976875, 0.92703125, 1.0, 0.97078125, 0.88078125, 0.94671875, 0.9421875, 0.9634375, 1.0, 1.0, 0.92703125, 1.0, 0.96328125, 0.92, 0.96515625, 0.96421875, 1.0, 0.98921875, 1.0, 0.87265625, 0.9375, 1.0, 0.9425, 0.891875, 0.95765625, 0.9615625, 1.0, 1.0, 1.0, 0.99515625, 1.0, 0.88390625, 0.97984375, 0.979375, 1.0, 0.97921875, 0.9690625, 0.86703125, 1.0, 0.9953125, 0.90984375, 0.909375, 0.8771875, 0.9934375, 1.0, 0.974375, 0.99125, 0.9615625, 0.95359375, 0.9659375, 0.97078125, 0.89890625, 0.94671875, 1.0, 0.91265625, 0.87078125, 0.940625, 0.97671875, 0.85703125, 0.95734375, 1.0, 0.88984375, 0.9434375, 0.94359375, 0.9503125, 1.0, 0.94421875, 0.91296875, 0.97015625, 0.96296875, 1.0, 0.95796875, 0.92765625, 0.97640625, 1.0, 0.97625, 1.0, 1.0, 1.0, 0.9290625, 0.9125, 1.0, 0.84953125, 1.0, 0.91046875, 0.92171875, 0.93484375, 1.0, 0.9396875, 1.0, 1.0, 1.0, 0.9853125, 0.97078125, 0.9128125, 0.9534375, 0.94125, 1.0, 0.79953125, 0.8778125, 0.94890625, 1.0, 0.95453125, 0.9896875, 0.8321875, 0.89328125, 1.0, 0.88078125, 0.9209375, 1.0, 0.970625, 0.908125, 0.97, 0.99875, 0.8934375, 0.99015625, 0.91515625, 0.8875, 1.0, 0.97734375, 0.99125, 0.9478125, 0.97484375, 1.0, 0.97359375, 0.89859375, 0.966875, 1.0, 0.883125, 1.0, 0.949375, 0.93109375, 0.9365625, 1.0, 0.9896875, 0.975, 0.828125, 0.97265625, 0.9784375, 0.97640625, 0.94328125, 0.95765625, 0.96609375, 0.98796875, 0.87265625, 0.9825, 0.8496875, 0.9190625, 1.0, 1.0, 1.0, 0.97265625, 0.97078125, 1.0, 0.83484375, 0.99875, 0.9634375, 0.9790625, 0.95125, 0.98, 0.9934375, 1.0, 1.0, 0.97640625, 1.0, 0.899375, 0.98265625, 1.0, 1.0, 0.96921875, 0.9125, 0.9721875, 0.99375, 0.90609375, 0.93359375, 0.94375, 0.99671875, 0.96953125, 0.994375, 1.0, 0.98671875, 0.966875, 0.97890625, 0.96609375, 1.0, 0.90484375, 0.90828125, 0.92453125, 0.88109375, 1.0, 0.9175, 0.968125, 0.948125, 0.97484375, 0.969375, 1.0, 0.9615625, 0.89953125, 0.9575, 1.0, 1.0, 0.94125, 0.98265625, 0.981875, 0.874375, 0.9509375, 0.988125, 0.9740625, 0.91046875, 1.0, 0.99203125, 1.0, 0.94375, 0.97, 0.95921875, 0.995, 0.9846875, 0.941875, 0.90359375, 0.891875, 0.920625, 1.0, 0.96578125, 0.95015625, 1.0, 1.0, 0.93828125, 0.99421875, 0.99671875, 1.0, 0.986875, 0.9203125, 1.0, 1.0, 0.980625, 0.99734375, 1.0, 0.9559375, 0.9721875, 0.9678125, 0.98484375, 1.0, 1.0, 0.93703125, 0.9940625, 0.99546875, 0.81640625, 1.0, 0.8571875, 1.0, 0.906875, 1.0, 0.98859375, 0.980625, 0.9409375, 0.9, 0.94515625, 0.8990625, 0.9721875, 0.98375, 0.978125, 0.90328125, 0.99203125, 0.97984375, 0.95234375, 1.0, 0.890625, 0.98640625, 1.0, 0.9984375, 1.0, 0.8646875, 0.90953125, 0.91078125, 0.9521875, 1.0, 0.98015625, 1.0, 0.910625, 0.8984375, 0.9978125, 0.9440625, 0.9571875, 0.9865625, 0.9309375, 0.8990625, 1.0, 1.0, 0.9665625, 1.0, 0.85421875, 0.964375, 0.960625, 0.9878125, 0.92328125, 0.94265625, 0.9065625, 0.95875, 0.9940625, 0.8903125, 0.9125, 0.97546875, 1.0, 0.89921875, 0.99, 1.0, 0.98125, 0.89765625, 0.91140625, 0.884375, 0.99390625, 0.92625, 1.0, 0.99921875, 0.96484375, 0.89546875, 0.91859375, 1.0, 0.8265625, 0.99125, 1.0, 0.84875, 0.98203125, 0.9675, 0.9034375, 0.935625, 0.979375, 0.9909375, 0.9915625, 1.0, 0.99109375, 0.9540625, 1.0, 0.98015625, 0.88265625, 1.0, 0.92375, 0.963125, 0.96390625, 0.93953125, 0.981875, 0.99640625, 0.985, 0.99671875, 0.92953125, 0.940625, 0.9621875, 0.99859375, 1.0, 0.95265625, 0.9540625, 0.889375, 0.99609375, 1.0, 1.0, 1.0, 1.0, 0.88484375, 0.9759375, 0.96203125, 1.0, 0.93640625, 0.9246875, 0.96421875, 0.9728125, 0.88109375, 0.91859375, 0.97671875, 0.98671875, 0.96296875, 0.94421875, 0.985625, 0.99984375, 1.0, 1.0, 0.895, 0.98890625, 0.93765625, 0.96265625, 1.0, 0.908125, 1.0, 0.95921875, 1.0, 0.91078125, 0.89296875, 1.0, 1.0, 1.0, 0.98171875, 0.87296875, 1.0, 0.95015625, 0.9559375, 1.0, 1.0, 1.0, 0.931875, 0.8640625, 0.91109375, 1.0, 0.90203125, 0.8584375, 0.94796875, 0.92890625, 1.0, 0.918125, 1.0, 0.94265625, 1.0, 0.90453125, 0.976875, 0.97625, 0.95609375, 0.99921875, 0.90921875, 1.0, 0.89078125, 0.8790625, 1.0, 1.0, 0.97234375, 0.89109375, 0.96859375, 0.98234375, 0.98125, 0.8615625, 0.980625, 0.97640625, 1.0, 0.97765625, 0.99046875, 0.99828125, 1.0, 0.99765625, 0.84484375, 1.0, 0.9990625, 0.99703125, 1.0, 0.95109375, 0.9965625, 0.95265625, 1.0, 0.84171875, 0.974375, 0.9578125, 0.9603125, 1.0, 0.9909375, 0.91046875, 1.0, 1.0, 1.0, 1.0, 0.9425, 0.99921875, 0.93078125, 1.0, 0.99703125, 0.96890625, 0.98484375, 0.9575, 0.94546875, 0.95734375, 0.85, 1.0, 1.0, 1.0, 0.995625, 1.0, 0.958125, 0.8809375, 0.895625, 1.0, 0.945625, 0.8490625, 0.9859375, 0.96515625, 1.0, 0.98546875, 1.0, 0.9603125, 0.93015625, 0.9734375, 0.93125, 0.96609375, 0.98125, 0.97515625, 0.9053125, 0.99046875, 0.93515625, 0.9978125, 1.0, 0.863125, 0.8846875, 0.89234375, 1.0, 1.0, 0.8478125, 1.0, 0.96828125, 0.96015625, 1.0, 0.90890625, 0.95125, 0.7671875, 0.98140625, 1.0, 1.0, 0.99109375, 0.9859375, 0.881875, 0.90453125, 0.99953125, 0.96875, 1.0, 0.994375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91484375, 0.939375, 0.95453125, 0.9125, 0.99515625, 0.9928125, 0.86296875, 1.0, 0.9046875, 1.0, 0.9775, 1.0, 0.9859375, 0.965, 0.98546875, 0.94078125, 0.8828125, 0.97703125, 0.92625, 0.9525, 0.90703125, 1.0, 0.87359375, 0.8896875, 0.93890625, 1.0, 0.91703125, 1.0, 1.0, 0.92828125, 1.0, 1.0, 1.0, 0.955625, 1.0, 0.87046875, 1.0, 0.92171875, 0.9621875, 0.97875, 0.98125, 0.9665625, 0.999375, 0.9996875, 0.9765625, 0.9234375, 0.9996875, 0.97484375, 0.93703125, 0.95875, 0.92609375, 0.9203125, 1.0, 1.0, 0.99203125, 0.90671875, 0.9665625, 0.89796875, 0.94765625, 0.89484375, 1.0, 0.95375, 0.954375, 0.98625, 0.926875, 0.97578125, 0.95578125, 0.97484375, 1.0, 0.95546875, 0.97828125, 1.0, 0.97578125, 1.0, 0.99609375, 0.85140625, 0.9853125, 1.0, 0.94171875, 1.0, 0.97140625, 0.84828125, 0.8534375, 0.86015625, 0.985625, 0.97125, 0.94640625, 0.993125, 0.94859375, 0.88484375, 0.92203125, 0.96625, 0.97625, 0.90296875, 1.0, 0.92609375, 0.99984375, 0.99828125, 1.0, 0.97453125, 1.0, 0.96359375, 0.96828125, 0.99984375, 0.9875, 1.0, 1.0, 1.0, 0.9315625, 1.0, 0.970625, 0.99515625, 0.9375, 0.92375, 0.8725, 0.98, 0.97140625, 0.96625, 1.0, 0.91609375, 0.97765625, 1.0, 0.9415625, 0.85890625, 0.96484375, 1.0, 0.99828125, 0.92203125, 1.0, 0.9225, 0.94671875, 0.9728125, 0.88890625, 0.91140625, 1.0, 0.9784375, 0.98375, 0.9340625, 1.0, 0.9375, 0.9859375, 1.0, 0.98234375, 0.96140625, 0.9334375, 0.94140625, 0.91484375, 0.9990625, 0.8753125, 0.9046875, 0.919375, 0.96, 1.0, 0.8734375, 0.9609375, 0.9671875, 0.9834375, 0.95515625, 0.8784375, 0.996875, 1.0, 0.9715625, 1.0, 0.96625, 0.97265625, 0.951875, 0.82703125, 0.9178125, 1.0, 1.0, 0.98125, 0.9071875, 1.0, 0.9903125, 0.98234375, 0.96234375, 0.9965625, 0.89421875, 0.98890625, 0.97625, 1.0, 1.0, 0.9371875, 0.98890625, 1.0, 0.99796875, 0.981875, 0.94515625, 0.99703125, 1.0, 1.0, 0.944375, 0.8990625, 1.0, 0.94875, 0.96828125, 0.9778125, 0.89453125, 0.86765625, 0.978125, 0.8415625, 0.97015625, 0.9625, 0.9684375, 0.99484375, 1.0, 0.97265625, 0.91703125, 0.89671875, 0.9, 0.89765625, 0.96625, 0.97078125, 0.94265625, 0.9671875, 0.91875, 0.9721875, 0.98578125, 1.0, 0.9984375, 0.9065625, 1.0, 0.9475, 0.90921875, 1.0, 1.0, 1.0, 0.8965625, 0.94515625, 0.88484375, 0.99984375, 0.97890625, 0.94390625, 1.0, 1.0, 0.78171875, 0.96578125, 0.8553125, 0.953125, 1.0, 0.966875, 0.8896875, 0.925, 0.96265625, 1.0, 0.92625, 0.8853125, 0.95265625, 0.8746875, 0.969375, 1.0, 0.98390625, 0.8609375, 0.988125, 0.9115625, 1.0, 0.91015625, 0.98796875, 0.89640625, 0.9865625, 1.0, 0.99859375, 0.9275, 0.993125, 0.93515625, 1.0, 0.98234375, 0.9553125, 0.98046875, 0.99078125, 0.9753125, 0.9375, 0.9515625, 0.99421875, 0.901875, 0.9984375, 0.980625, 1.0, 0.8834375, 0.86421875, 0.90203125, 0.86640625, 0.95734375, 0.92625, 0.95875, 0.94671875, 0.96015625, 0.88671875, 0.94234375, 0.87078125, 0.98359375, 1.0, 0.96765625, 0.91078125, 0.97953125, 0.9903125, 0.97671875, 0.89578125, 0.988125, 0.9478125, 1.0, 0.94375, 1.0, 0.84078125, 0.94828125, 0.93578125, 1.0, 1.0, 0.8440625, 0.98875, 0.9728125, 0.999375, 0.94578125, 0.8540625, 0.89265625, 0.92765625, 0.93578125, 0.89953125, 0.92515625, 0.97234375, 0.94109375, 0.9746875, 0.891875, 0.97453125, 0.97234375, 0.97140625, 0.945625, 0.90703125, 0.9109375, 0.91671875, 0.976875, 0.858125, 0.971875, 0.820625, 0.90046875, 0.98546875, 1.0, 0.8584375, 0.951875, 1.0, 0.9565625, 0.96328125, 0.9853125, 0.961875, 0.94515625, 0.954375, 0.95828125, 0.97015625, 1.0, 1.0, 0.96453125, 0.95171875, 1.0, 1.0, 1.0, 0.96421875, 0.8378125, 0.98453125, 0.9528125, 0.9878125, 0.871875, 0.94484375, 1.0, 0.895625, 0.99296875, 1.0, 0.97921875, 1.0, 1.0, 0.9503125, 1.0, 0.93109375, 0.94578125, 1.0, 0.9159375, 0.95953125, 0.9553125, 0.94453125, 0.9684375, 0.91296875, 1.0, 0.98578125, 0.9865625, 1.0, 0.8959375, 0.956875, 0.83078125, 0.91796875, 0.9965625, 0.94296875, 0.9740625, 1.0, 0.96796875, 0.9978125, 0.9821875, 0.87296875, 0.99734375, 0.93234375, 0.90859375, 0.893125, 1.0, 0.9940625, 0.99875, 0.90046875, 0.911875, 0.9725, 1.0, 0.98640625, 0.9996875, 0.84890625, 1.0, 0.94578125, 1.0, 0.98640625, 0.99203125, 0.98125, 0.8559375, 1.0, 1.0, 1.0, 0.9840625, 0.92, 0.92453125, 0.9696875, 0.90390625, 0.975, 1.0, 0.9990625, 0.949375, 0.9646875, 0.93578125, 0.95, 0.8928125, 0.9903125, 0.89953125, 0.91875, 0.9221875, 0.7834375, 0.995, 0.9871875, 1.0, 0.995, 1.0, 0.97390625, 0.95375, 1.0, 0.9590625, 1.0, 0.971875, 0.98140625, 1.0, 0.9259375, 0.9546875, 1.0, 0.98328125, 0.94640625, 1.0, 0.98125, 1.0, 0.9078125, 0.98703125, 0.963125, 0.871875, 0.97453125, 0.87265625, 0.9825, 1.0, 0.92921875, 0.95640625, 0.95984375, 1.0, 0.9784375, 0.9346875, 0.97625, 1.0, 0.91765625, 0.91671875, 0.98578125, 0.9909375, 0.97703125, 1.0, 0.90671875, 0.92015625, 0.99625, 0.9434375, 0.92921875, 0.90671875, 0.91578125, 0.98796875, 0.96109375, 0.7975, 0.94953125, 0.8646875, 1.0, 1.0, 0.85484375, 1.0, 0.915, 0.988125, 0.9578125, 0.94578125, 0.99375, 1.0, 0.9078125, 0.975, 0.9075, 0.8753125, 1.0, 0.968125, 0.84984375, 0.9421875, 0.99171875, 0.9571875, 0.95875, 0.968125, 0.9428125, 0.9696875, 0.85296875, 0.92421875, 0.96890625, 0.93984375, 1.0, 0.96703125, 0.955625, 1.0, 0.9328125, 0.89171875, 0.921875, 0.95921875, 0.99859375, 1.0, 0.9240625, 1.0, 0.96765625, 0.95171875, 0.7521875, 0.91828125, 0.92734375, 0.93390625, 0.96703125, 0.8815625, 0.9596875, 0.945, 0.870625, 1.0, 0.99703125, 0.97265625, 0.97421875, 0.93703125, 1.0, 0.99171875, 0.9646875, 0.936875, 0.8721875, 0.9390625, 0.91796875, 0.91203125, 0.953125, 0.98796875, 1.0, 1.0, 1.0, 0.97828125, 0.99921875, 0.9625, 0.91625, 0.97578125, 1.0, 0.991875, 0.9775, 0.95046875, 0.9428125, 0.9978125, 0.97046875, 0.99515625, 0.995625, 0.99328125, 0.9384375, 0.86609375, 0.94640625, 1.0, 0.93390625, 1.0, 1.0, 0.9678125, 0.948125, 0.94921875, 0.94, 0.98, 0.85359375, 0.8925, 0.9421875, 0.9875, 0.978125, 1.0, 1.0, 0.92609375, 0.8821875, 1.0, 0.92890625, 0.96828125, 0.9965625, 1.0, 1.0, 1.0, 0.93296875, 0.9828125, 0.9728125, 0.87640625, 0.9025, 0.9496875, 0.91203125, 0.9128125, 1.0, 1.0, 0.93875, 1.0, 0.96328125, 0.984375, 1.0, 0.96875, 0.97859375, 0.95625, 0.93046875, 0.9346875, 0.92390625, 0.9878125, 0.93765625, 0.9734375, 0.97, 0.95421875, 0.884375, 1.0, 1.0, 0.9615625, 1.0, 0.89328125, 0.9825, 1.0, 0.79609375, 1.0, 0.90640625, 1.0, 0.904375, 0.9909375, 0.96171875, 0.99265625, 0.96171875, 0.9321875, 0.98953125, 0.9434375, 0.86703125, 0.9465625, 0.99875, 0.92703125, 0.91953125, 0.9503125, 0.8996875, 0.9778125, 0.98859375, 0.90203125, 1.0, 0.98859375, 0.96140625, 0.95953125, 0.9515625, 1.0, 1.0, 1.0, 1.0, 0.96203125, 0.99265625, 0.9903125, 0.901875, 1.0, 1.0, 0.9546875, 1.0, 0.98625, 0.9365625, 0.9378125, 0.9509375, 0.88640625, 0.9825, 0.89484375, 0.99828125, 0.99578125, 0.94546875, 0.99890625, 1.0, 0.98609375, 0.943125, 0.97609375, 0.996875, 0.94375, 0.9165625, 0.90546875, 0.95140625, 0.9853125, 0.98328125, 1.0, 1.0, 0.99609375, 1.0, 0.94953125, 0.9509375, 1.0, 0.98546875, 0.95390625, 1.0, 0.99546875, 0.99359375, 0.97375, 0.98453125, 0.99453125, 0.90015625, 0.89734375, 1.0, 0.9671875, 0.95359375, 0.95484375, 0.92984375, 0.91125, 0.961875, 0.98265625, 0.94078125, 1.0, 0.95734375, 0.88328125, 1.0, 1.0, 0.8928125, 0.96578125, 1.0, 0.92921875, 0.9565625, 0.9621875, 1.0, 0.9765625, 1.0, 0.9815625, 0.8759375, 1.0, 0.92453125, 1.0, 1.0, 1.0, 0.9953125, 0.8434375, 1.0, 0.9546875, 0.94546875, 0.97375, 0.97546875, 1.0, 0.89140625, 0.976875, 0.97859375, 1.0, 0.9584375, 0.9928125, 0.92609375, 0.998125, 0.94234375, 1.0, 1.0, 0.9609375, 0.98921875, 1.0, 0.9325, 0.8528125, 0.97515625, 0.9459375, 1.0, 0.93109375, 1.0, 1.0, 0.89171875, 0.94171875, 0.92546875, 0.96546875, 0.98296875, 0.98875, 1.0, 0.91765625, 0.93921875, 0.9940625, 0.96171875, 0.99375, 0.880625, 0.90296875, 1.0, 0.8396875, 1.0, 1.0, 0.99390625, 0.9671875, 1.0, 0.88796875, 0.8025, 0.95375, 0.91703125, 1.0, 0.98015625, 1.0, 0.95078125, 0.91921875, 0.9459375, 0.91046875, 0.98625, 0.9184375, 0.948125, 1.0, 0.9590625, 1.0, 1.0, 0.98609375, 0.925625, 1.0, 1.0, 0.8203125, 1.0, 0.9278125, 0.90125, 0.94796875, 0.9671875, 0.844375, 1.0, 1.0, 0.9315625, 0.975625, 0.91109375, 1.0, 0.85546875, 0.9775, 0.81625, 1.0, 0.9265625, 1.0, 0.87765625, 1.0, 0.89875, 0.96109375, 0.99609375, 1.0, 0.9709375, 0.96375, 0.99265625, 1.0, 0.94296875, 0.92546875, 0.97828125, 0.84578125, 0.8628125, 0.9528125, 0.985625, 0.98703125, 0.80078125, 0.95, 0.944375, 0.82203125, 0.98984375, 0.96078125, 1.0, 0.91109375, 0.883125, 0.8921875, 0.95, 0.95828125, 0.93359375, 0.9840625, 0.97140625, 0.99515625, 1.0, 1.0, 0.78609375, 0.96921875, 1.0, 0.93109375, 1.0, 0.98296875, 0.93359375, 0.97, 0.95703125, 0.92828125, 0.96265625, 0.92703125, 0.9453125, 0.85109375, 0.89171875, 0.8625, 0.9021875, 0.950625, 0.96828125, 0.91828125, 0.98421875, 1.0, 0.930625, 1.0, 0.9640625, 0.96265625, 0.94703125, 0.92703125, 0.97125, 0.97984375, 1.0, 1.0, 0.879375, 0.91640625, 0.81625, 0.9821875, 0.98796875, 0.9990625, 0.94796875, 0.916875, 0.974375, 0.95546875, 0.92953125, 0.84921875, 1.0, 1.0, 0.9921875, 1.0, 0.923125, 0.99609375, 0.95484375, 0.94546875, 0.84578125, 0.96015625, 0.93625, 1.0, 0.93125, 0.956875, 0.9275, 0.9090625, 0.94734375, 0.99375, 0.915625, 0.89625, 0.8696875, 0.99921875, 0.98546875, 0.81421875, 1.0, 0.9634375, 1.0, 0.9628125, 0.88484375, 1.0, 0.85578125, 0.95609375, 0.82859375, 0.99578125, 0.9553125, 0.99859375, 1.0, 0.9775, 0.9328125, 0.9803125, 0.9978125, 0.98765625, 0.9884375, 0.91921875, 0.971875, 1.0, 0.9525, 1.0, 0.865625, 1.0, 0.9503125, 1.0, 0.98234375, 0.97, 0.98359375, 1.0, 0.89765625, 0.925, 0.93671875, 0.96421875, 0.99, 0.93890625, 0.91953125, 0.94796875, 0.90734375, 1.0, 0.8825, 0.935625, 0.914375, 0.95890625, 1.0, 0.9909375, 0.9365625, 1.0, 0.8584375, 0.900625, 0.954375, 0.880625, 0.87890625, 0.8990625, 0.879375, 0.9271875, 0.93984375, 0.94125, 0.9734375, 0.82625, 0.97421875, 0.85828125, 1.0, 0.9275, 0.99703125, 0.99171875, 1.0, 0.88546875, 0.97609375, 0.91109375, 0.97265625, 0.9628125, 0.898125, 0.98140625, 0.91203125, 0.87859375, 0.88546875, 0.9328125, 1.0, 0.98, 0.95515625, 0.9875, 1.0, 0.9884375, 0.90421875, 0.9796875, 0.8553125, 0.94171875, 0.98953125, 0.9840625, 1.0, 0.87484375, 0.92390625, 0.97921875, 0.8153125, 0.9990625, 0.98046875, 0.986875, 0.8284375, 0.8303125, 0.89015625, 0.98296875, 0.95359375, 0.89453125, 0.958125, 0.99203125, 1.0, 0.99625, 0.991875, 0.88375, 1.0, 1.0, 0.94765625, 0.965625, 1.0, 0.9446875, 0.92359375, 0.9059375, 0.915, 0.9828125, 0.9615625, 0.9146875, 0.9409375, 1.0, 0.8440625, 0.99796875, 0.97390625, 0.9990625, 0.95515625, 0.99015625, 0.92515625, 1.0, 0.8453125, 0.98140625, 0.95484375, 1.0, 0.81875, 0.85984375, 1.0, 0.9428125, 0.99046875, 0.99375, 0.81390625, 1.0, 0.92, 0.970625, 0.98015625, 0.97078125, 0.998125, 0.96046875, 0.88421875, 0.901875, 0.96421875, 0.93390625, 0.915625, 0.9296875, 0.97453125, 0.886875, 0.90375, 1.0, 1.0, 1.0, 0.96421875, 0.938125, 1.0, 0.97484375, 0.96546875, 0.91875, 0.98203125, 0.9459375, 0.960625, 0.9715625, 0.95265625, 1.0, 0.96125, 0.92015625, 1.0, 0.79890625, 1.0, 1.0, 1.0, 0.99859375, 0.99734375, 0.93453125, 0.974375, 0.88890625, 0.99421875, 0.99109375, 0.998125, 0.96484375, 1.0, 0.99796875, 0.96796875, 0.995, 1.0, 0.97875, 0.94375, 1.0, 0.84015625, 1.0, 0.9815625, 1.0, 0.98203125, 1.0, 0.98328125, 0.9909375, 1.0, 1.0, 0.99859375, 0.90828125, 1.0, 1.0, 1.0, 0.9728125, 1.0, 0.97625, 0.9840625, 1.0, 0.9521875, 0.82484375, 0.98703125, 0.93546875, 1.0, 0.96953125, 0.996875, 0.91109375, 0.9446875, 1.0, 0.955625, 1.0, 0.97640625, 1.0, 0.86921875, 0.9784375, 1.0, 0.9796875, 0.9534375, 0.9578125, 0.8303125, 0.97875, 0.86484375, 1.0, 0.90796875, 0.8378125, 1.0, 0.975625, 0.97203125, 0.90625, 0.9959375, 0.94546875, 0.9359375, 0.93921875, 0.76640625, 0.8321875, 0.9303125, 1.0, 0.95953125, 0.99359375, 0.88125, 0.8984375, 0.9090625, 0.9603125, 0.88953125, 1.0, 1.0, 0.98046875, 1.0, 0.974375, 0.9634375, 0.97265625, 0.9209375, 1.0, 0.90078125, 0.9871875, 0.88984375, 0.95984375, 0.98359375, 0.87328125, 0.90375, 1.0, 0.88046875, 0.97171875, 0.86078125, 1.0, 0.9171875, 0.975, 0.96546875, 1.0, 1.0, 0.85265625, 0.99515625, 0.99828125, 0.97296875, 0.986875, 0.976875, 0.98875, 1.0, 1.0, 1.0, 0.94484375, 0.9184375, 0.94015625, 0.9940625, 0.9684375, 0.9084375, 1.0, 0.9440625, 1.0, 1.0, 0.9953125, 0.9290625, 0.93421875, 0.96859375, 0.89859375, 0.8996875, 0.89546875, 0.91125, 0.89921875, 0.94640625, 0.85484375, 0.90046875, 0.99828125, 1.0, 0.86796875, 0.94609375, 0.8990625, 0.93546875, 0.8853125, 0.97296875, 0.99609375, 0.98578125, 0.94765625, 0.89, 0.95234375, 0.9575, 0.98703125, 0.9084375, 0.94703125, 0.87546875, 0.9371875, 0.9059375, 1.0, 0.9196875, 0.99328125, 0.9253125, 0.82234375, 0.99109375, 0.97609375, 1.0, 0.98546875, 0.95484375, 0.86890625, 0.9903125, 1.0, 0.97109375, 0.96671875, 0.97234375, 0.83546875, 0.98203125, 0.96015625, 0.9884375, 1.0, 0.95265625, 0.93546875, 0.92, 1.0, 0.9821875, 0.97, 0.82109375, 0.99, 0.941875, 0.8575, 0.94265625, 0.96203125, 0.92765625, 1.0, 0.99875, 0.98328125, 0.93921875, 0.9571875, 1.0, 0.95734375, 0.988125, 0.96015625, 1.0, 0.9875, 0.985, 0.9934375, 1.0, 0.9646875, 0.97859375, 0.99375, 0.97, 0.95890625, 0.914375, 0.941875, 0.9878125, 0.9625, 0.9490625, 0.9140625, 0.97453125, 0.99515625, 1.0, 0.9321875, 0.99875, 0.97953125, 0.85421875, 0.865, 0.9609375, 0.96234375, 0.855, 0.99203125, 1.0, 0.965625, 1.0, 0.96078125, 0.9365625, 0.950625, 1.0, 0.9784375, 0.99265625, 0.90421875, 0.90578125, 0.87953125, 0.89734375, 0.89625, 1.0, 1.0, 0.9103125, 0.96265625, 0.99171875, 0.97765625, 0.889375, 0.96421875, 1.0, 1.0, 0.9459375, 0.98328125, 0.91421875, 0.95078125, 1.0, 0.841875, 0.9978125, 0.94765625, 0.93734375, 0.88875, 0.92390625, 0.90328125, 0.94171875, 1.0, 0.9546875, 1.0, 0.98203125, 0.9803125, 0.94703125, 1.0, 1.0, 0.9546875, 0.9184375, 0.95640625, 0.949375, 0.99015625, 0.96890625, 0.96265625, 0.983125, 0.90234375, 0.9771875, 0.94546875, 0.8884375, 1.0, 0.9946875, 1.0, 1.0, 0.91296875, 0.8978125, 0.9640625, 1.0, 1.0, 0.89796875, 1.0, 0.94828125, 0.94375, 0.93390625, 1.0, 1.0, 1.0, 0.92375, 0.84875, 1.0, 0.880625, 0.97734375, 0.96078125, 0.95453125, 0.89515625, 1.0, 1.0, 0.9625, 0.88140625, 0.99734375, 1.0, 0.9875, 0.96859375, 1.0, 1.0, 0.93890625, 0.98, 1.0, 0.92453125, 0.9709375, 0.9303125, 1.0, 0.92890625, 0.9790625, 0.98734375, 1.0, 1.0, 0.84953125, 0.9521875, 0.9, 0.96109375, 0.9875, 0.9303125, 0.99453125, 0.805625, 1.0, 0.9665625, 0.9171875, 0.96875, 1.0, 1.0, 0.96859375, 0.943125, 0.97171875, 0.99078125, 0.9815625, 0.89015625, 0.94515625, 0.9853125, 1.0, 0.9490625, 1.0, 0.86890625, 1.0, 0.9759375, 0.93046875, 0.889375, 0.93703125, 0.99375, 0.97484375, 0.82359375, 0.95734375, 0.93640625, 1.0, 0.88640625, 0.95484375, 0.95734375, 0.99390625, 0.92625, 0.845625, 0.95703125, 0.8909375, 1.0, 0.91234375, 1.0, 0.9484375, 1.0, 0.94234375, 1.0, 0.9896875, 0.9975, 0.92328125, 1.0, 0.88578125, 0.99296875, 1.0, 0.939375, 0.93265625, 0.99109375, 1.0, 0.7659375, 0.8709375, 0.839375, 1.0, 0.97765625, 0.93546875, 0.95671875, 1.0, 0.9959375, 0.89578125, 0.9259375, 1.0, 1.0, 1.0, 0.95875, 0.86015625, 0.94453125, 0.97296875, 1.0, 0.83125, 0.8753125, 0.786875, 1.0, 0.85546875, 0.95578125, 0.9946875, 0.9990625, 1.0, 0.99265625, 0.97625, 0.99453125, 1.0, 0.969375, 0.99265625, 0.9240625, 1.0, 1.0, 0.95203125, 1.0, 0.95828125, 0.9728125, 1.0, 0.93765625, 1.0, 0.96078125, 0.9603125, 1.0, 0.99140625, 0.9871875, 0.9940625, 1.0, 0.955625, 0.908125, 0.9009375, 1.0, 1.0, 0.8634375, 0.7915625, 0.93796875, 0.89046875, 0.93984375, 0.9825, 0.96734375, 0.97796875, 0.98515625, 1.0, 0.88421875, 0.87, 0.98984375, 0.94671875, 0.99765625, 0.83296875, 0.965625, 0.90375, 0.96, 0.97046875, 0.890625, 0.98859375, 0.931875, 0.99375, 0.89578125, 0.92828125, 0.985625, 0.98328125, 0.92234375, 1.0, 0.88609375, 0.9771875, 1.0, 0.93546875, 0.98125, 1.0, 0.97203125, 1.0, 0.9553125, 1.0, 0.95109375, 1.0, 1.0, 0.9709375, 1.0, 0.9840625, 0.90671875, 0.9859375, 1.0, 1.0, 0.9528125, 0.95390625, 0.9028125, 1.0, 1.0, 0.955, 0.9790625, 0.9234375, 1.0, 1.0, 0.92953125, 0.965, 1.0, 0.9003125, 0.91234375, 1.0, 1.0, 1.0, 0.96546875, 0.96921875, 0.916875, 0.9959375, 0.8228125, 0.9709375, 0.88890625, 0.9465625, 0.8334375, 0.9796875, 0.99421875, 1.0, 0.8803125, 1.0, 0.9653125, 0.924375, 0.9953125, 0.903125, 1.0, 0.99109375, 0.92625, 0.96921875, 1.0, 0.97328125, 0.9009375, 0.9359375, 1.0, 0.96671875, 0.99125, 0.83078125, 1.0, 0.96578125, 0.89453125, 1.0, 0.924375, 0.8671875, 0.94203125, 0.964375, 1.0, 0.9303125, 0.99578125, 0.930625, 0.9303125, 1.0, 0.998125, 0.99046875, 0.94359375, 0.96984375, 0.92875, 0.96328125, 1.0, 0.8909375, 0.979375, 0.935, 0.86234375, 1.0, 0.923125, 0.9853125, 0.96875, 0.978125, 0.96796875, 0.95734375, 0.9659375, 1.0, 0.93859375, 1.0, 0.98375, 0.99234375, 0.94484375, 0.93734375, 0.9559375, 0.96890625, 1.0, 0.9584375, 0.98359375, 0.9709375, 0.974375, 0.8596875, 0.94515625, 1.0, 0.9134375, 1.0, 0.94890625, 0.889375, 0.93859375, 0.955625, 0.9996875, 0.98453125, 0.91875, 0.8878125, 0.99671875, 0.99796875, 0.895, 1.0, 0.918125, 0.9865625, 0.87796875, 1.0, 1.0, 0.9334375, 1.0, 0.97953125, 0.99203125, 0.8875, 0.95953125, 0.990625, 0.93875, 0.83140625, 0.9509375, 0.89921875, 0.9915625, 0.99609375, 0.89265625, 0.84921875, 0.9046875, 1.0, 0.9984375, 0.98390625, 0.9903125, 0.98390625, 0.89546875, 0.91953125, 0.8971875, 0.9684375, 0.98015625, 0.92640625, 0.8896875, 0.866875, 0.9446875, 0.99234375, 1.0, 0.9453125, 0.97078125, 0.9325, 0.95578125, 0.97140625, 0.98171875, 1.0, 0.85546875, 0.945, 1.0, 1.0, 0.893125, 0.9340625, 0.93125, 1.0, 0.99234375, 1.0, 0.934375, 0.95734375, 0.99640625, 0.95828125, 0.971875, 0.905625, 0.95609375, 0.94625, 0.973125, 0.97515625, 0.92203125, 0.92921875, 1.0, 0.90984375, 0.98015625, 0.98890625, 0.94140625, 0.96875, 0.9578125, 1.0, 1.0, 0.96, 1.0, 1.0, 0.97296875, 0.9728125, 0.99640625, 0.95484375, 0.98078125, 1.0, 0.94984375, 0.965, 0.9353125, 0.95703125, 1.0, 1.0, 0.97, 0.94921875, 1.0, 0.97359375, 1.0, 0.799375, 0.95671875, 0.96234375, 0.97578125, 0.9021875, 0.921875, 0.89703125, 1.0, 0.9121875, 0.8271875, 0.993125, 0.99046875, 0.9553125, 0.93890625, 0.8865625, 0.88515625, 0.86484375, 0.97890625, 0.995, 0.9853125, 0.9840625, 0.9753125, 0.88578125, 1.0, 0.98375, 1.0, 1.0, 0.93921875, 0.896875, 0.9703125, 0.92890625, 1.0, 0.97546875, 1.0, 0.89796875, 0.950625, 0.79546875, 0.85375, 0.9865625, 0.86859375, 1.0, 1.0, 0.975625, 0.9934375, 0.895, 0.88203125, 0.8821875, 0.959375, 0.9378125, 1.0, 0.8753125, 0.980625, 0.94234375, 1.0, 0.95984375, 0.99078125, 1.0, 1.0, 0.93015625, 1.0, 0.993125, 1.0, 0.94640625, 0.9621875, 0.96453125, 0.9953125, 1.0, 1.0, 0.99546875, 0.94390625, 0.995625, 0.9325, 0.923125, 0.91734375, 0.999375, 1.0, 0.97453125, 0.9159375, 0.99578125, 0.91171875, 0.99703125, 0.9578125, 0.8828125, 0.8940625, 0.91171875, 1.0, 0.93515625, 0.92671875, 1.0, 0.923125, 0.95796875, 0.96515625, 0.9890625, 0.97765625, 0.9378125, 0.9984375, 0.9109375, 0.87921875, 0.9259375, 0.93671875, 1.0, 0.98453125, 0.88984375, 0.986875, 0.95734375, 1.0, 0.9990625, 0.9875, 0.9778125, 1.0, 0.97984375, 0.9659375, 0.99828125, 0.95859375, 0.98, 0.9946875, 0.98203125, 1.0, 0.89796875, 0.93515625, 0.95296875, 0.9803125, 1.0, 1.0, 0.92296875, 0.94890625, 0.91984375, 0.9771875, 0.92640625, 0.84140625, 0.98203125, 0.9040625, 0.90234375, 0.97546875, 0.98421875, 0.9534375, 0.99140625, 0.98234375, 1.0, 0.9890625, 1.0, 0.92078125, 1.0, 0.9215625, 0.8490625, 1.0, 0.97125, 0.96421875, 1.0, 0.85515625, 0.90421875, 0.9853125, 0.97046875, 0.9959375, 0.995, 0.98234375, 0.99609375, 0.95140625, 0.98578125, 1.0, 0.90109375, 0.994375, 0.8203125, 0.9009375, 0.97234375, 0.88375, 0.99859375, 0.96109375, 0.97765625, 0.99171875, 0.9875, 1.0, 1.0, 0.98984375, 0.94765625, 0.8759375, 0.94453125, 0.9675, 0.985, 1.0, 0.87125, 0.913125, 0.9203125, 0.92921875, 0.8271875, 0.98875, 0.97171875, 1.0, 0.95359375, 0.86828125, 0.9975, 0.98046875, 1.0, 1.0, 1.0, 0.92078125, 0.995, 0.9815625, 0.95296875, 0.9640625, 0.94953125, 0.914375, 0.93453125, 0.99328125, 0.9153125, 0.89265625, 0.99796875, 1.0, 1.0, 0.99375, 0.9359375, 0.993125, 0.95, 0.95234375, 0.8303125, 0.96265625, 1.0, 1.0, 1.0, 0.978125, 0.9034375, 0.95625, 1.0, 0.99921875, 1.0, 0.91203125, 0.98140625, 1.0, 0.89984375, 0.83625, 0.8609375, 1.0, 0.92390625, 0.91046875, 0.95671875, 1.0, 0.885625, 0.88984375, 0.92859375, 0.96703125, 0.9846875, 0.95734375, 0.96203125, 1.0, 0.93078125, 0.96203125, 0.9553125, 0.8596875, 1.0, 0.97984375, 1.0, 1.0, 0.9971875, 0.9515625, 1.0, 1.0, 0.990625, 0.8865625, 0.94078125, 0.98296875, 1.0, 0.97875, 0.95265625, 0.93875, 0.99296875, 0.94171875, 0.99359375, 0.9165625, 0.98796875, 0.96640625, 0.9890625, 0.87453125, 0.96546875, 0.97, 1.0, 1.0, 0.96125, 1.0, 0.96, 0.99625, 0.93828125, 0.89828125, 1.0, 1.0, 1.0, 0.9959375, 0.9140625, 0.9075, 1.0, 0.98328125, 0.98671875, 0.9475, 0.9365625, 0.98109375, 0.9165625, 1.0, 0.905, 0.9790625, 1.0, 0.98640625, 0.9721875, 0.926875, 1.0, 0.970625, 0.84734375, 0.99421875, 0.98640625, 0.920625, 1.0, 0.86234375, 0.97171875, 1.0, 0.7403125, 0.9896875, 0.9825, 0.88328125, 1.0, 0.93984375, 0.94484375, 0.8540625, 0.993125, 0.85984375, 1.0, 1.0, 0.8975, 0.975, 0.9525, 0.90984375, 0.85984375, 0.96953125, 0.966875, 0.85609375, 0.8871875, 0.89109375, 0.98953125, 0.94, 0.88875, 1.0, 0.981875, 1.0, 0.87859375, 0.97890625, 0.9321875, 0.99046875, 1.0, 0.9940625, 0.99078125, 0.920625, 0.966875, 0.94265625, 0.99140625, 0.9215625, 0.90984375, 0.981875, 0.95890625, 0.989375, 0.90125, 0.97828125, 0.984375, 1.0, 1.0, 0.89734375, 0.98421875, 1.0, 0.90265625, 0.988125, 1.0, 0.916875, 0.98953125, 0.958125, 0.97875, 0.9790625, 0.94390625, 0.94328125, 0.93484375, 0.92734375, 1.0, 0.87265625, 0.918125, 1.0, 0.97609375, 1.0, 1.0, 0.9690625, 1.0, 0.99578125, 0.86921875, 0.93578125, 0.98859375, 0.890625, 1.0, 0.88171875, 0.96875, 1.0, 0.9796875, 0.93265625, 0.9828125, 0.82125, 0.94671875, 0.9590625, 0.959375, 0.92390625, 1.0, 0.83671875, 0.99171875, 0.9778125, 0.92375, 0.99375, 0.934375, 0.96046875, 0.9671875, 1.0, 0.95484375, 0.98296875, 1.0, 0.685625, 1.0, 0.88484375, 1.0, 1.0, 0.9378125, 0.985625, 0.9465625, 0.97796875, 0.9178125, 0.86046875, 0.946875, 0.93984375, 1.0, 0.85140625, 0.93265625, 1.0, 0.94140625, 0.84375, 1.0, 0.96359375, 0.9821875, 1.0, 0.99359375, 1.0, 1.0, 1.0, 0.86578125, 1.0, 0.9925, 1.0, 0.904375, 0.92734375, 1.0, 0.9434375, 1.0, 0.976875, 0.97765625, 1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "print(image_accuracies)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sYdMsJAtEPGX",
        "GV6BJOfE0Cck",
        "ZsYz5xMUEAr0",
        "qwVediwtG5aN",
        "IP6BuEZU1I1X",
        "F-avIebJCZqi"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMDBOQujhLL49ppIV5rgtPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}